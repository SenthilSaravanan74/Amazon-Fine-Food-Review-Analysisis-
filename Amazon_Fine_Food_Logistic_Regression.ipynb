{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import sqlite3\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "import string\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from collections import Counter\n",
    "from sklearn import cross_validation\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n",
    "\n",
    "\n",
    "# using the SQLite Table to read data.\n",
    "con = sqlite3.connect('./database.sqlite') \n",
    "\n",
    "\n",
    "\n",
    "#filtering only positive and negative reviews i.e. \n",
    "# not taking into consideration those reviews with Score=3\n",
    "filtered_data = pd.read_sql_query(\"\"\"\n",
    "SELECT *\n",
    "FROM Reviews\n",
    "WHERE Score != 3\n",
    "\"\"\", con) \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Give reviews with Score>3 a positive rating, and reviews with a score<3 a negative rating.\n",
    "def partition(x):\n",
    "    if x < 3:\n",
    "        return 'negative'\n",
    "    return 'positive'\n",
    "\n",
    "#changing reviews with score less than 3 to be positive and vice-versa\n",
    "actualScore = filtered_data['Score']\n",
    "positiveNegative = actualScore.map(partition) \n",
    "filtered_data['Score'] = positiveNegative"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>ProductId</th>\n",
       "      <th>UserId</th>\n",
       "      <th>ProfileName</th>\n",
       "      <th>HelpfulnessNumerator</th>\n",
       "      <th>HelpfulnessDenominator</th>\n",
       "      <th>Score</th>\n",
       "      <th>Time</th>\n",
       "      <th>Summary</th>\n",
       "      <th>Text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>B001E4KFG0</td>\n",
       "      <td>A3SGXH7AUHU8GW</td>\n",
       "      <td>delmartian</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>positive</td>\n",
       "      <td>1303862400</td>\n",
       "      <td>Good Quality Dog Food</td>\n",
       "      <td>I have bought several of the Vitality canned d...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>B00813GRG4</td>\n",
       "      <td>A1D87F6ZCVE5NK</td>\n",
       "      <td>dll pa</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>negative</td>\n",
       "      <td>1346976000</td>\n",
       "      <td>Not as Advertised</td>\n",
       "      <td>Product arrived labeled as Jumbo Salted Peanut...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>B000LQOCH0</td>\n",
       "      <td>ABXLMWJIXXAIN</td>\n",
       "      <td>Natalia Corres \"Natalia Corres\"</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>positive</td>\n",
       "      <td>1219017600</td>\n",
       "      <td>\"Delight\" says it all</td>\n",
       "      <td>This is a confection that has been around a fe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>B000UA0QIQ</td>\n",
       "      <td>A395BORC6FGVXV</td>\n",
       "      <td>Karl</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>negative</td>\n",
       "      <td>1307923200</td>\n",
       "      <td>Cough Medicine</td>\n",
       "      <td>If you are looking for the secret ingredient i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>B006K2ZZ7K</td>\n",
       "      <td>A1UQRSCLF8GW1T</td>\n",
       "      <td>Michael D. Bigham \"M. Wassir\"</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>positive</td>\n",
       "      <td>1350777600</td>\n",
       "      <td>Great taffy</td>\n",
       "      <td>Great taffy at a great price.  There was a wid...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Id   ProductId          UserId                      ProfileName  \\\n",
       "0   1  B001E4KFG0  A3SGXH7AUHU8GW                       delmartian   \n",
       "1   2  B00813GRG4  A1D87F6ZCVE5NK                           dll pa   \n",
       "2   3  B000LQOCH0   ABXLMWJIXXAIN  Natalia Corres \"Natalia Corres\"   \n",
       "3   4  B000UA0QIQ  A395BORC6FGVXV                             Karl   \n",
       "4   5  B006K2ZZ7K  A1UQRSCLF8GW1T    Michael D. Bigham \"M. Wassir\"   \n",
       "\n",
       "   HelpfulnessNumerator  HelpfulnessDenominator     Score        Time  \\\n",
       "0                     1                       1  positive  1303862400   \n",
       "1                     0                       0  negative  1346976000   \n",
       "2                     1                       1  positive  1219017600   \n",
       "3                     3                       3  negative  1307923200   \n",
       "4                     0                       0  positive  1350777600   \n",
       "\n",
       "                 Summary                                               Text  \n",
       "0  Good Quality Dog Food  I have bought several of the Vitality canned d...  \n",
       "1      Not as Advertised  Product arrived labeled as Jumbo Salted Peanut...  \n",
       "2  \"Delight\" says it all  This is a confection that has been around a fe...  \n",
       "3         Cough Medicine  If you are looking for the secret ingredient i...  \n",
       "4            Great taffy  Great taffy at a great price.  There was a wid...  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filtered_data.shape #looking at the number of attributes and size of the data\n",
    "filtered_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data Cleaning: Deduplication\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>ProductId</th>\n",
       "      <th>UserId</th>\n",
       "      <th>ProfileName</th>\n",
       "      <th>HelpfulnessNumerator</th>\n",
       "      <th>HelpfulnessDenominator</th>\n",
       "      <th>Score</th>\n",
       "      <th>Time</th>\n",
       "      <th>Summary</th>\n",
       "      <th>Text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>78445</td>\n",
       "      <td>B000HDL1RQ</td>\n",
       "      <td>AR5J8UI46CURR</td>\n",
       "      <td>Geetha Krishnan</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>1199577600</td>\n",
       "      <td>LOACKER QUADRATINI VANILLA WAFERS</td>\n",
       "      <td>DELICIOUS WAFERS. I FIND THAT EUROPEAN WAFERS ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>138317</td>\n",
       "      <td>B000HDOPYC</td>\n",
       "      <td>AR5J8UI46CURR</td>\n",
       "      <td>Geetha Krishnan</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>1199577600</td>\n",
       "      <td>LOACKER QUADRATINI VANILLA WAFERS</td>\n",
       "      <td>DELICIOUS WAFERS. I FIND THAT EUROPEAN WAFERS ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>138277</td>\n",
       "      <td>B000HDOPYM</td>\n",
       "      <td>AR5J8UI46CURR</td>\n",
       "      <td>Geetha Krishnan</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>1199577600</td>\n",
       "      <td>LOACKER QUADRATINI VANILLA WAFERS</td>\n",
       "      <td>DELICIOUS WAFERS. I FIND THAT EUROPEAN WAFERS ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>73791</td>\n",
       "      <td>B000HDOPZG</td>\n",
       "      <td>AR5J8UI46CURR</td>\n",
       "      <td>Geetha Krishnan</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>1199577600</td>\n",
       "      <td>LOACKER QUADRATINI VANILLA WAFERS</td>\n",
       "      <td>DELICIOUS WAFERS. I FIND THAT EUROPEAN WAFERS ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>155049</td>\n",
       "      <td>B000PAQ75C</td>\n",
       "      <td>AR5J8UI46CURR</td>\n",
       "      <td>Geetha Krishnan</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>1199577600</td>\n",
       "      <td>LOACKER QUADRATINI VANILLA WAFERS</td>\n",
       "      <td>DELICIOUS WAFERS. I FIND THAT EUROPEAN WAFERS ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Id   ProductId         UserId      ProfileName  HelpfulnessNumerator  \\\n",
       "0   78445  B000HDL1RQ  AR5J8UI46CURR  Geetha Krishnan                     2   \n",
       "1  138317  B000HDOPYC  AR5J8UI46CURR  Geetha Krishnan                     2   \n",
       "2  138277  B000HDOPYM  AR5J8UI46CURR  Geetha Krishnan                     2   \n",
       "3   73791  B000HDOPZG  AR5J8UI46CURR  Geetha Krishnan                     2   \n",
       "4  155049  B000PAQ75C  AR5J8UI46CURR  Geetha Krishnan                     2   \n",
       "\n",
       "   HelpfulnessDenominator  Score        Time  \\\n",
       "0                       2      5  1199577600   \n",
       "1                       2      5  1199577600   \n",
       "2                       2      5  1199577600   \n",
       "3                       2      5  1199577600   \n",
       "4                       2      5  1199577600   \n",
       "\n",
       "                             Summary  \\\n",
       "0  LOACKER QUADRATINI VANILLA WAFERS   \n",
       "1  LOACKER QUADRATINI VANILLA WAFERS   \n",
       "2  LOACKER QUADRATINI VANILLA WAFERS   \n",
       "3  LOACKER QUADRATINI VANILLA WAFERS   \n",
       "4  LOACKER QUADRATINI VANILLA WAFERS   \n",
       "\n",
       "                                                Text  \n",
       "0  DELICIOUS WAFERS. I FIND THAT EUROPEAN WAFERS ...  \n",
       "1  DELICIOUS WAFERS. I FIND THAT EUROPEAN WAFERS ...  \n",
       "2  DELICIOUS WAFERS. I FIND THAT EUROPEAN WAFERS ...  \n",
       "3  DELICIOUS WAFERS. I FIND THAT EUROPEAN WAFERS ...  \n",
       "4  DELICIOUS WAFERS. I FIND THAT EUROPEAN WAFERS ...  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "display= pd.read_sql_query(\"\"\"\n",
    "SELECT *\n",
    "FROM Reviews\n",
    "WHERE Score != 3 AND UserId=\"AR5J8UI46CURR\"\n",
    "ORDER BY ProductID\n",
    "\"\"\", con)\n",
    "display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Sorting data according to ProductId in ascending order\n",
    "sorted_data=filtered_data.sort_values('ProductId', axis=0, ascending=True, inplace=False, kind='quicksort', na_position='last')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(364173, 10)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Deduplication of entries\n",
    "final=sorted_data.drop_duplicates(subset={\"UserId\",\"ProfileName\",\"Time\",\"Text\"}, keep='first', inplace=False)\n",
    "#final=final.drop_duplicates(subset={\"UserId\",\"ProductId\",\"Time\"}, keep='first', inplace=False)\n",
    "final.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "69.25890143662969"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Checking to see how much % of data still remains\n",
    "(final['Id'].size*1.0)/(filtered_data['Id'].size*1.0)*100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>ProductId</th>\n",
       "      <th>UserId</th>\n",
       "      <th>ProfileName</th>\n",
       "      <th>HelpfulnessNumerator</th>\n",
       "      <th>HelpfulnessDenominator</th>\n",
       "      <th>Score</th>\n",
       "      <th>Time</th>\n",
       "      <th>Summary</th>\n",
       "      <th>Text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>64422</td>\n",
       "      <td>B000MIDROQ</td>\n",
       "      <td>A161DK06JJMCYF</td>\n",
       "      <td>J. E. Stephens \"Jeanne\"</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>1224892800</td>\n",
       "      <td>Bought This for My Son at College</td>\n",
       "      <td>My son loves spaghetti so I didn't hesitate or...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>44737</td>\n",
       "      <td>B001EQ55RW</td>\n",
       "      <td>A2V0I904FH7ABY</td>\n",
       "      <td>Ram</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>1212883200</td>\n",
       "      <td>Pure cocoa taste with crunchy almonds inside</td>\n",
       "      <td>It was almost a 'love at first bite' - the per...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      Id   ProductId          UserId              ProfileName  \\\n",
       "0  64422  B000MIDROQ  A161DK06JJMCYF  J. E. Stephens \"Jeanne\"   \n",
       "1  44737  B001EQ55RW  A2V0I904FH7ABY                      Ram   \n",
       "\n",
       "   HelpfulnessNumerator  HelpfulnessDenominator  Score        Time  \\\n",
       "0                     3                       1      5  1224892800   \n",
       "1                     3                       2      4  1212883200   \n",
       "\n",
       "                                        Summary  \\\n",
       "0             Bought This for My Son at College   \n",
       "1  Pure cocoa taste with crunchy almonds inside   \n",
       "\n",
       "                                                Text  \n",
       "0  My son loves spaghetti so I didn't hesitate or...  \n",
       "1  It was almost a 'love at first bite' - the per...  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "display= pd.read_sql_query(\"\"\"\n",
    "SELECT *\n",
    "FROM Reviews\n",
    "WHERE Score != 3 AND Id=44737 OR Id=64422\n",
    "ORDER BY ProductID\n",
    "\"\"\", con)\n",
    "display\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "final=final[final.HelpfulnessNumerator<=final.HelpfulnessDenominator]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(364171, 10)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "positive    307061\n",
       "negative     57110\n",
       "Name: Score, dtype: int64"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Before starting the next phase of preprocessing lets see the number of entries left\n",
    "print(final.shape)\n",
    "\n",
    "#How many positive and negative reviews are present in our dataset?\n",
    "final['Score'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Data-preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6\n",
      "I set aside at least an hour each day to read to my son (3 y/o). At this point, I consider myself a connoisseur of children's books and this is one of the best. Santa Clause put this under the tree. Since then, we've read it perpetually and he loves it.<br /><br />First, this book taught him the months of the year.<br /><br />Second, it's a pleasure to read. Well suited to 1.5 y/o old to 4+.<br /><br />Very few children's books are worth owning. Most should be borrowed from the library. This book, however, deserves a permanent spot on your shelf. Sendak's best.\n"
     ]
    }
   ],
   "source": [
    "# find sentences containing HTML tags\n",
    "import re\n",
    "\n",
    "i=0;\n",
    "for sent in final['Text'].values:\n",
    "    if (len(re.findall('<.*?>', sent))):\n",
    "        print(i)\n",
    "        print(sent)\n",
    "        break;\n",
    "    i += 1;    \n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'most', \"isn't\", 'as', 'so', \"weren't\", 'haven', \"shan't\", \"wasn't\", 'we', 'each', \"hasn't\", 'up', 'it', 'just', 'll', 't', \"don't\", 'are', 'does', 'further', 'how', 'down', 'they', 'no', 'which', \"haven't\", 're', 'shan', 'again', 'itself', \"didn't\", 'on', 'other', 'me', 'she', 'being', 'having', 'once', 'any', 'her', 'now', 'those', 'such', \"you're\", 'doing', 'be', \"that'll\", 'both', 'them', 'were', 'didn', 'ain', 'wasn', 'this', 'whom', 'under', 'then', 'too', 'our', 'do', 'there', 'or', 'herself', 've', 'but', 'out', 'own', 'have', 'wouldn', \"wouldn't\", 'my', 'am', 'these', 'an', 'same', 'after', 'weren', 'here', 'than', 'had', 'y', 'his', 'themselves', 'why', 'will', 'and', 'through', \"hadn't\", 'below', 'mustn', 'won', 'yourself', \"it's\", \"you'd\", 'a', 'very', 'couldn', 'ourselves', \"shouldn't\", \"couldn't\", 'ma', 'where', 'should', 'few', 'who', 'himself', 'don', 'while', 's', \"mustn't\", 'not', 'when', \"doesn't\", 'hasn', 'i', 'd', 'aren', 'at', 'about', \"you've\", 'shouldn', 'the', \"should've\", 'can', 'is', 'nor', 'in', 'myself', 'what', 'if', 'mightn', 'was', 'above', 'from', 'by', 'of', 'into', \"she's\", 'between', 'm', 'yours', 'ours', 'been', 'before', 'with', 'some', 'for', 'needn', 'o', 'that', \"mightn't\", 'until', 'off', 'more', 'hadn', 'yourselves', 'him', 'theirs', 'did', 'because', 'during', 'all', \"aren't\", \"won't\", 'you', \"needn't\", 'to', 'isn', 'he', 'against', 'their', 'hers', 'your', 'its', 'has', 'over', 'doesn', 'only', \"you'll\"}\n",
      "************************************\n",
      "tasti\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "import string\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "\n",
    "stop = set(stopwords.words('english')) #set of stopwords\n",
    "sno = nltk.stem.SnowballStemmer('english') #initialising the snowball stemmer\n",
    "\n",
    "def cleanhtml(sentence): #function to clean the word of any html-tags\n",
    "    cleanr = re.compile('<.*?>')\n",
    "    cleantext = re.sub(cleanr, ' ', sentence)\n",
    "    return cleantext\n",
    "def cleanpunc(sentence): #function to clean the word of any punctuation or special characters\n",
    "    cleaned = re.sub(r'[?|!|\\'|\"|#]',r'',sentence)\n",
    "    cleaned = re.sub(r'[.|,|)|(|\\|/]',r' ',cleaned)\n",
    "    return  cleaned\n",
    "print(stop)\n",
    "print('************************************')\n",
    "print(sno.stem('tasty'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Code for implementing step-by-step the checks mentioned in the pre-processing phase\n",
    "i=0\n",
    "str1=' '\n",
    "final_string=[]\n",
    "all_positive_words=[] # store words from +ve reviews here\n",
    "all_negative_words=[] # store words from -ve reviews here.\n",
    "s=''\n",
    "for sent in final['Text'].values:\n",
    "    filtered_sentence=[]\n",
    "    #print(sent);\n",
    "    sent=cleanhtml(sent) # remove HTMl tags\n",
    "    for w in sent.split():\n",
    "        for cleaned_words in cleanpunc(w).split():\n",
    "            if((cleaned_words.isalpha()) & (len(cleaned_words)>2)):    \n",
    "                if(cleaned_words.lower() not in stop):\n",
    "                    s=(sno.stem(cleaned_words.lower())).encode('utf8')\n",
    "                    filtered_sentence.append(s)\n",
    "                    if (final['Score'].values)[i] == 'positive': \n",
    "                        all_positive_words.append(s) #list of all words used to describe positive reviews\n",
    "                    if(final['Score'].values)[i] == 'negative':\n",
    "                        all_negative_words.append(s) #list of all words used to describe negative reviews reviews\n",
    "                else:\n",
    "                    continue\n",
    "            else:\n",
    "                continue \n",
    "    #print(filtered_sentence)\n",
    "    str1 = b\" \".join(filtered_sentence) #final string of cleaned words\n",
    "    #print(\"***********************************************************************\")\n",
    "    \n",
    "    final_string.append(str1)\n",
    "    i+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "final['CleanedText']=final_string #adding a column of CleanedText which displays the data after pre-processing of the review "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# store final table into an SQlLite table for future.\n",
    "conn = sqlite3.connect('final.sqlite')\n",
    "c=conn.cursor()\n",
    "conn.text_factory = str\n",
    "final.to_sql('Reviews', conn, flavor=None, schema=None, if_exists='replace', index=True, index_label=None, chunksize=None, dtype=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Bag of Words (BoW)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df = pd.DataFrame(final) # converting table to a dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>ProductId</th>\n",
       "      <th>UserId</th>\n",
       "      <th>ProfileName</th>\n",
       "      <th>HelpfulnessNumerator</th>\n",
       "      <th>HelpfulnessDenominator</th>\n",
       "      <th>Score</th>\n",
       "      <th>Time</th>\n",
       "      <th>Summary</th>\n",
       "      <th>Text</th>\n",
       "      <th>CleanedText</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>369243</th>\n",
       "      <td>399221</td>\n",
       "      <td>B000VK4K3W</td>\n",
       "      <td>AA157EV59BJGM</td>\n",
       "      <td>domenico luvera</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>negative</td>\n",
       "      <td>1348617600</td>\n",
       "      <td>Formula Changed</td>\n",
       "      <td>The Newman's Own Organics 2nd Generation Turke...</td>\n",
       "      <td>b'newman organ generat turkey veget chang ingr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>358793</th>\n",
       "      <td>388076</td>\n",
       "      <td>B007RTR8UM</td>\n",
       "      <td>A2KUVD844LL7QM</td>\n",
       "      <td>Angela M. Hey</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>positive</td>\n",
       "      <td>1338249600</td>\n",
       "      <td>Creamy conditioner that flows well out of the ...</td>\n",
       "      <td>The conditioner ingredients are unremarkable -...</td>\n",
       "      <td>b'condition ingredi unremark chemic plus oil a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>185818</th>\n",
       "      <td>201549</td>\n",
       "      <td>B000NURB18</td>\n",
       "      <td>A23OH1HHVZDKLI</td>\n",
       "      <td>juliet</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>positive</td>\n",
       "      <td>1284854400</td>\n",
       "      <td>&amp;lt;3 it!!!</td>\n",
       "      <td>a friend gave this to me and I love it!!!  So ...</td>\n",
       "      <td>b'friend gave love good doesnt need anyth'</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>444079</th>\n",
       "      <td>480169</td>\n",
       "      <td>B007GOCR5E</td>\n",
       "      <td>A1Z54EM24Y40LL</td>\n",
       "      <td>c2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>positive</td>\n",
       "      <td>1339372800</td>\n",
       "      <td>One of the boys' favorites</td>\n",
       "      <td>I always have Powerade in the house - for ever...</td>\n",
       "      <td>b'alway powerad hous everyth flu over hot base...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>395554</th>\n",
       "      <td>427721</td>\n",
       "      <td>B003DIIMLK</td>\n",
       "      <td>A14DFCE6FVU07L</td>\n",
       "      <td>FLRoss</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>negative</td>\n",
       "      <td>1303948800</td>\n",
       "      <td>ICK!</td>\n",
       "      <td>What happened Annie?.. There are very few Anni...</td>\n",
       "      <td>b'happen anni anni product say yuck prob one r...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>281055</th>\n",
       "      <td>304509</td>\n",
       "      <td>B0017OE536</td>\n",
       "      <td>A287V946KJP73N</td>\n",
       "      <td>Jason Bourne</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>positive</td>\n",
       "      <td>1285891200</td>\n",
       "      <td>delicious with everything!  alone....well an a...</td>\n",
       "      <td>This product helped a friend lose 50 lbs and I...</td>\n",
       "      <td>b'product help friend lose lbs lost lbs produc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>191005</th>\n",
       "      <td>207094</td>\n",
       "      <td>B0015DQG22</td>\n",
       "      <td>A4UCU29FTOFCF</td>\n",
       "      <td>Janet Krisman</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>negative</td>\n",
       "      <td>1347235200</td>\n",
       "      <td>Lifesavers?</td>\n",
       "      <td>The so-called \"new formula\" has a strong chemi...</td>\n",
       "      <td>b'new formula strong chemic tast dont like hop...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20335</th>\n",
       "      <td>22212</td>\n",
       "      <td>B000KV61FC</td>\n",
       "      <td>A3FV8UWW0KVI82</td>\n",
       "      <td>W. Sprague</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>negative</td>\n",
       "      <td>1281657600</td>\n",
       "      <td>Dog doesn't use it at all</td>\n",
       "      <td>I thought this would be fantastic to keep my p...</td>\n",
       "      <td>b'thought would fantast keep puppi busi chew k...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>411032</th>\n",
       "      <td>444538</td>\n",
       "      <td>B001HTISGQ</td>\n",
       "      <td>ATN552TF5V40Z</td>\n",
       "      <td>Lh</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>positive</td>\n",
       "      <td>1251763200</td>\n",
       "      <td>They really mean sticky</td>\n",
       "      <td>I grew up with sticky rice but this is actuall...</td>\n",
       "      <td>b'grew sticki rice actual hard pull apart cont...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60477</th>\n",
       "      <td>65742</td>\n",
       "      <td>B003KSL1B6</td>\n",
       "      <td>A3RR2P5IS3DGPR</td>\n",
       "      <td>Dr. M. A. Dixon \"hyper-observant\"</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>positive</td>\n",
       "      <td>1287792000</td>\n",
       "      <td>My favorite cinnamon</td>\n",
       "      <td>While not organic, Saigon Cinnamon is the best...</td>\n",
       "      <td>b'organ saigon cinnamon best tast cinnamon opi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55394</th>\n",
       "      <td>60102</td>\n",
       "      <td>B001E5E056</td>\n",
       "      <td>A3SHACREIY6HVX</td>\n",
       "      <td>A. Floyd \"Fun_Accountant\"</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>positive</td>\n",
       "      <td>1347494400</td>\n",
       "      <td>I'm craving this cereal right now :)</td>\n",
       "      <td>This is my all-time favorite cereal.  It taste...</td>\n",
       "      <td>b'favorit cereal tast like healthi blueberri m...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62534</th>\n",
       "      <td>67941</td>\n",
       "      <td>B000G6MBUA</td>\n",
       "      <td>A3M2VGWFJGRUOH</td>\n",
       "      <td>Eco-Friendly Interior Designer</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>positive</td>\n",
       "      <td>1217980800</td>\n",
       "      <td>Best Chip</td>\n",
       "      <td>Kettle lightly slated chips are my favorite on...</td>\n",
       "      <td>b'kettl light slate chip favorit market dont c...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>394785</th>\n",
       "      <td>426901</td>\n",
       "      <td>B003OCCR8Y</td>\n",
       "      <td>AB7LQD5DGNG9U</td>\n",
       "      <td>D. Henry \"Teacher\"</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>negative</td>\n",
       "      <td>1326240000</td>\n",
       "      <td>Review</td>\n",
       "      <td>This mix is very good.  I would add heavy crea...</td>\n",
       "      <td>b'mix good would add heavi cream instead butte...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6328</th>\n",
       "      <td>6925</td>\n",
       "      <td>B001LG945O</td>\n",
       "      <td>A2GF9I8Y6D3FFD</td>\n",
       "      <td>Professional Book Buyer</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>positive</td>\n",
       "      <td>1305417600</td>\n",
       "      <td>My Son's Favorite \"Soda\"</td>\n",
       "      <td>We really have to limit additives and sugar fo...</td>\n",
       "      <td>b'realli limit addit sugar son autism switch o...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>408906</th>\n",
       "      <td>442245</td>\n",
       "      <td>B001L4EME4</td>\n",
       "      <td>A3CI9DSB6Z76JF</td>\n",
       "      <td>LB \"Helfenfreude\"</td>\n",
       "      <td>17</td>\n",
       "      <td>29</td>\n",
       "      <td>positive</td>\n",
       "      <td>1242345600</td>\n",
       "      <td>Several packages for the same yummy product</td>\n",
       "      <td>OK, so I'm used to drinking (and have a prefer...</td>\n",
       "      <td>b'use drink prefer can aluminum top peel teeni...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96518</th>\n",
       "      <td>104882</td>\n",
       "      <td>B000EDK6K2</td>\n",
       "      <td>A9HU2P580OG39</td>\n",
       "      <td>Melissa Soden</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>positive</td>\n",
       "      <td>1220659200</td>\n",
       "      <td>Bob's Red Mill Powder Buttermilk</td>\n",
       "      <td>Great stuff. I always have it on hand for cook...</td>\n",
       "      <td>b'great stuff alway hand cook anyth call butte...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>121006</th>\n",
       "      <td>131181</td>\n",
       "      <td>B001RVCDME</td>\n",
       "      <td>A24WN7YK3UEPYY</td>\n",
       "      <td>Katie D.</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>positive</td>\n",
       "      <td>1335830400</td>\n",
       "      <td>First thought- Bear Claw!</td>\n",
       "      <td>As soon as I smelled the Almond Trail Mix Earn...</td>\n",
       "      <td>b'soon smell almond trail mix earnest eat bar ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>418142</th>\n",
       "      <td>452181</td>\n",
       "      <td>B000HDL1DU</td>\n",
       "      <td>A6QF32V0HO07L</td>\n",
       "      <td>GbreadMan</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>positive</td>\n",
       "      <td>1347321600</td>\n",
       "      <td>Never thought sardines could taste this good!</td>\n",
       "      <td>I've only eaten sardines for about a year.  I ...</td>\n",
       "      <td>b'ive eaten sardin year heard healthi environm...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>294535</th>\n",
       "      <td>319071</td>\n",
       "      <td>B002ZOKNGU</td>\n",
       "      <td>A26NVE6OVMAF4J</td>\n",
       "      <td>Rosie</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>positive</td>\n",
       "      <td>1317686400</td>\n",
       "      <td>dentedinpa</td>\n",
       "      <td>Great coffee at a decent price. Cheaper than m...</td>\n",
       "      <td>b'great coffe decent price cheaper local groce...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>378501</th>\n",
       "      <td>409261</td>\n",
       "      <td>B0030HYAK6</td>\n",
       "      <td>A1FQGQ4Z1SQ01M</td>\n",
       "      <td>T. Freitas \"JijiCat\"</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>positive</td>\n",
       "      <td>1265414400</td>\n",
       "      <td>Favorite Gluten Free Cracker</td>\n",
       "      <td>Crunchy and delicious with anything. We use it...</td>\n",
       "      <td>b'crunchi delici anyth use place bread often y...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>494789</th>\n",
       "      <td>534920</td>\n",
       "      <td>B005PXZ6JM</td>\n",
       "      <td>A2K104U5ND8QAL</td>\n",
       "      <td>Claudia McCowan</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>positive</td>\n",
       "      <td>1325548800</td>\n",
       "      <td>Great alternative to Lansinoh</td>\n",
       "      <td>I got some hand-me-down Lansinoh breastmilk ba...</td>\n",
       "      <td>b'got lansinoh breastmilk bag went back work t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>484642</th>\n",
       "      <td>524048</td>\n",
       "      <td>B004JGQ15E</td>\n",
       "      <td>A3GWFJ48QPA2IR</td>\n",
       "      <td>J. Weiss</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>positive</td>\n",
       "      <td>1305244800</td>\n",
       "      <td>Delicious!</td>\n",
       "      <td>I love this little snack idea!  I have tried a...</td>\n",
       "      <td>b'love littl snack idea tri lot handi low calo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>81299</th>\n",
       "      <td>88404</td>\n",
       "      <td>B001EO5UY2</td>\n",
       "      <td>AKAT811SZJ5H6</td>\n",
       "      <td>D. Taylor</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>positive</td>\n",
       "      <td>1307232000</td>\n",
       "      <td>Delicious!</td>\n",
       "      <td>Hands down this is one of the best prepared so...</td>\n",
       "      <td>b'hand one best prepar soup add cup water cook...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>385010</th>\n",
       "      <td>416322</td>\n",
       "      <td>B000NNCMG4</td>\n",
       "      <td>A2WSQ7FDU1RWDT</td>\n",
       "      <td>J P Simms III/Cross Community Church \"J Simms\"</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>positive</td>\n",
       "      <td>1322006400</td>\n",
       "      <td>Great Tea!</td>\n",
       "      <td>This tea is excellent and is one of our favori...</td>\n",
       "      <td>b'tea excel one favorit flavor absolut wonder ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40820</th>\n",
       "      <td>44354</td>\n",
       "      <td>B0014C3QD6</td>\n",
       "      <td>AWPYPBFJFNBPX</td>\n",
       "      <td>Irish Danny</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>positive</td>\n",
       "      <td>1335225600</td>\n",
       "      <td>My Old Favorite</td>\n",
       "      <td>When I first got out of boot camp, my first du...</td>\n",
       "      <td>b'first got boot camp first duti station quant...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15379</th>\n",
       "      <td>16803</td>\n",
       "      <td>B001LGGH40</td>\n",
       "      <td>A1LA4K5JF78BER</td>\n",
       "      <td>Miyomoto Masaki \"MM\"</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>positive</td>\n",
       "      <td>1239408000</td>\n",
       "      <td>My Family Likes It.</td>\n",
       "      <td>I don't read every label on every drink or foo...</td>\n",
       "      <td>b'dont read everi label everi drink food consu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>509836</th>\n",
       "      <td>551275</td>\n",
       "      <td>B003NC70X2</td>\n",
       "      <td>A1BNIAGZBAGU6R</td>\n",
       "      <td>judys4444 \"judy-m.p.\"</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>positive</td>\n",
       "      <td>1303776000</td>\n",
       "      <td>great</td>\n",
       "      <td>i ordered 3 bags of this for me and it's the b...</td>\n",
       "      <td>b'order bag best popcorn yet almost everi kern...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>282441</th>\n",
       "      <td>305992</td>\n",
       "      <td>B000LKXPD8</td>\n",
       "      <td>A1FM4VU4VIZYMV</td>\n",
       "      <td>J. Miller \"Island Girl\"</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>positive</td>\n",
       "      <td>1252195200</td>\n",
       "      <td>SoLo Bars are the Best!!!</td>\n",
       "      <td>I love the Mint Mania SoLo Bar!  The low glyce...</td>\n",
       "      <td>b'love mint mania solo bar low glycem qualiti ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>314270</th>\n",
       "      <td>340265</td>\n",
       "      <td>B003Q4TVKW</td>\n",
       "      <td>A2I6P6A3YD8JA3</td>\n",
       "      <td>cathy</td>\n",
       "      <td>12</td>\n",
       "      <td>13</td>\n",
       "      <td>positive</td>\n",
       "      <td>1280793600</td>\n",
       "      <td>wish they were in glass bottles</td>\n",
       "      <td>I wish these were in glass bottles, but someti...</td>\n",
       "      <td>b'wish glass bottl sometim up beat box pretti ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>194972</th>\n",
       "      <td>211368</td>\n",
       "      <td>B0002R1HVC</td>\n",
       "      <td>A2H9HZK1OATBW0</td>\n",
       "      <td>J. Schuhmacher</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>positive</td>\n",
       "      <td>1230336000</td>\n",
       "      <td>Definitely a Hit</td>\n",
       "      <td>Stuck for an idea for a \"decade\" gift?  This w...</td>\n",
       "      <td>b'stuck idea decad gift hit recipi guest order...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>117939</th>\n",
       "      <td>127934</td>\n",
       "      <td>B002DPWQWA</td>\n",
       "      <td>AW9ZO67B1CFCA</td>\n",
       "      <td>NOLA Gal</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>positive</td>\n",
       "      <td>1276128000</td>\n",
       "      <td>picky dog didn't like</td>\n",
       "      <td>I'm sure this product is fine, my dog is just ...</td>\n",
       "      <td>b'sure product fine dog incred picki receiv qu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>397404</th>\n",
       "      <td>429694</td>\n",
       "      <td>B000F4D5IU</td>\n",
       "      <td>A1E0NN39IOBNOM</td>\n",
       "      <td>S. Tickle</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>positive</td>\n",
       "      <td>1273795200</td>\n",
       "      <td>wonderful taste</td>\n",
       "      <td>The perfect coconut flakes, unsweetened, has a...</td>\n",
       "      <td>b'perfect coconut flake unsweeten wonder tast ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>456246</th>\n",
       "      <td>493294</td>\n",
       "      <td>B003TC7WN4</td>\n",
       "      <td>A3LZCRW9NU0327</td>\n",
       "      <td>Tinley Park Shopper</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>positive</td>\n",
       "      <td>1333584000</td>\n",
       "      <td>Favorite!!!</td>\n",
       "      <td>For years, I've been completely devoted to Dun...</td>\n",
       "      <td>b'year ive complet devot dunkin donut brew sto...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>215413</th>\n",
       "      <td>233463</td>\n",
       "      <td>B00453M7RM</td>\n",
       "      <td>AY12DBB0U420B</td>\n",
       "      <td>Gary Peterson</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>positive</td>\n",
       "      <td>1350950400</td>\n",
       "      <td>Incredibly Good, If It's The Same.</td>\n",
       "      <td>In shopping at a nearby Costco yesterday, I ca...</td>\n",
       "      <td>b'shop nearbi costco yesterday came across kir...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>224282</th>\n",
       "      <td>243210</td>\n",
       "      <td>B0000DK59A</td>\n",
       "      <td>A6NPVGUI0KXL3</td>\n",
       "      <td>M. Saso</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>positive</td>\n",
       "      <td>1183766400</td>\n",
       "      <td>MMMM</td>\n",
       "      <td>This oil for our candy making is great and it ...</td>\n",
       "      <td>b'oil candi make great ship rite away pleas'</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>175043</th>\n",
       "      <td>189833</td>\n",
       "      <td>B001EO5Q64</td>\n",
       "      <td>A2LCTKLCDRNAQO</td>\n",
       "      <td>sharada</td>\n",
       "      <td>7</td>\n",
       "      <td>13</td>\n",
       "      <td>negative</td>\n",
       "      <td>1328313600</td>\n",
       "      <td>This is not coconut oil</td>\n",
       "      <td>I'm using Nutiva coconutoil since 2008, and I ...</td>\n",
       "      <td>b'use nutiva coconutoil sinc alway bought paca...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96546</th>\n",
       "      <td>104912</td>\n",
       "      <td>B0012OV5PM</td>\n",
       "      <td>A2F8IHET2WY0XP</td>\n",
       "      <td>Western Realm \"Miranda\"</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>positive</td>\n",
       "      <td>1346371200</td>\n",
       "      <td>My family loves this.</td>\n",
       "      <td>We purchased this from Amazon as our local sto...</td>\n",
       "      <td>b'purchas amazon local store carri occasion ex...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>352596</th>\n",
       "      <td>381408</td>\n",
       "      <td>B000F99D2M</td>\n",
       "      <td>A2AMSFMUWXREZZ</td>\n",
       "      <td>chargergrl06</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>positive</td>\n",
       "      <td>1348963200</td>\n",
       "      <td>Tastes great!</td>\n",
       "      <td>The pomegranate flavor in this tea makes it a ...</td>\n",
       "      <td>b'pomegran flavor tea make winner littl bit ex...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>370347</th>\n",
       "      <td>400495</td>\n",
       "      <td>B005A1JX98</td>\n",
       "      <td>A8NKI9II63LXM</td>\n",
       "      <td>Clare Guydish \"Going Places\"</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>positive</td>\n",
       "      <td>1350777600</td>\n",
       "      <td>Great tasting gluten free!</td>\n",
       "      <td>Perfect with cheese or humus! THis crackers ar...</td>\n",
       "      <td>b'perfect chees humus cracker tasti crunchi gl...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5722</th>\n",
       "      <td>6197</td>\n",
       "      <td>B000UOFW9K</td>\n",
       "      <td>A1LARHHDXMS3CH</td>\n",
       "      <td>C. Meyer</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>negative</td>\n",
       "      <td>1269907200</td>\n",
       "      <td>Hard as a rock with about as much flavor</td>\n",
       "      <td>These things are like eating poker chips with ...</td>\n",
       "      <td>b'thing like eat poker chip season salt theyr ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>511148</th>\n",
       "      <td>552680</td>\n",
       "      <td>B002DHMWSG</td>\n",
       "      <td>A3M1TW7SZV6E5M</td>\n",
       "      <td>cleaningoutmycloset</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>positive</td>\n",
       "      <td>1312848000</td>\n",
       "      <td>amazon vs. grocery strore</td>\n",
       "      <td>The cereal is great, and the price is cheaper ...</td>\n",
       "      <td>b'cereal great price cheaper prime ship local ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>362751</th>\n",
       "      <td>392303</td>\n",
       "      <td>B001EQ50U4</td>\n",
       "      <td>A1CFYV8RQ8L6DS</td>\n",
       "      <td>maglean \"magaugust\"</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>positive</td>\n",
       "      <td>1210291200</td>\n",
       "      <td>Kool-Aid excellent</td>\n",
       "      <td>I'm please with this purchase, the kool-aid ta...</td>\n",
       "      <td>b'pleas purchas tast good get lot price excel ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>372146</th>\n",
       "      <td>402439</td>\n",
       "      <td>B0014J5Y3E</td>\n",
       "      <td>AL309NM00MUI4</td>\n",
       "      <td>Mary Corso \"Here's one for the big girl\"</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>positive</td>\n",
       "      <td>1268784000</td>\n",
       "      <td>size matter</td>\n",
       "      <td>It was very hard for me to determine what size...</td>\n",
       "      <td>b'hard determin size gumbal buy descript said ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73264</th>\n",
       "      <td>79737</td>\n",
       "      <td>B000WFKSK0</td>\n",
       "      <td>A3798WV4B49HWV</td>\n",
       "      <td>JJ</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>positive</td>\n",
       "      <td>1350691200</td>\n",
       "      <td>Dinner time</td>\n",
       "      <td>Great wet dog food and worth the cost. My pug ...</td>\n",
       "      <td>b'great wet dog food worth cost pug digest foo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>276690</th>\n",
       "      <td>299840</td>\n",
       "      <td>B0040J01KQ</td>\n",
       "      <td>AKBZD4VMJ1FF4</td>\n",
       "      <td>astroqueen67</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>positive</td>\n",
       "      <td>1316822400</td>\n",
       "      <td>Great Deal</td>\n",
       "      <td>This box was huge and will last a long time! P...</td>\n",
       "      <td>b'box huge last long time product intact broke...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>163339</th>\n",
       "      <td>177097</td>\n",
       "      <td>B004FEJ968</td>\n",
       "      <td>A31YSTICHFYN7O</td>\n",
       "      <td>CPR \"xfilebuffyfan\"</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>positive</td>\n",
       "      <td>1305763200</td>\n",
       "      <td>Tastes Like Real Fruit</td>\n",
       "      <td>This cookie is better than expected.  They are...</td>\n",
       "      <td>b'cooki better expect larger regular cooki thi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>256189</th>\n",
       "      <td>277737</td>\n",
       "      <td>B0048IFVCM</td>\n",
       "      <td>A3BOGELRQJK8OE</td>\n",
       "      <td>a gentle sound</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>positive</td>\n",
       "      <td>1343952000</td>\n",
       "      <td>Dinner in 20 minutes</td>\n",
       "      <td>I saw this product today at the store. I'm a g...</td>\n",
       "      <td>b'saw product today store good cook sure day w...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>188514</th>\n",
       "      <td>204457</td>\n",
       "      <td>B007OWQD0S</td>\n",
       "      <td>A3190EG59DUCVS</td>\n",
       "      <td>janet bonser</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>positive</td>\n",
       "      <td>1342828800</td>\n",
       "      <td>the best ever</td>\n",
       "      <td>i purchased the original biscotti and ate the ...</td>\n",
       "      <td>b'purchas origin biscotti ate whole box sit we...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>270503</th>\n",
       "      <td>293233</td>\n",
       "      <td>B000I6O3B8</td>\n",
       "      <td>A3RYWRIHN8PELF</td>\n",
       "      <td>R. J. Sorretto</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>positive</td>\n",
       "      <td>1281225600</td>\n",
       "      <td>Oh, It's So Good!</td>\n",
       "      <td>This amaretto coffee is wonderful, with a nice...</td>\n",
       "      <td>b'amaretto coffe wonder nice mellow flavor alm...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>153656</th>\n",
       "      <td>166606</td>\n",
       "      <td>B002AMXQNY</td>\n",
       "      <td>A3R41BEJLP6N2W</td>\n",
       "      <td>LOVE TO LEARN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>positive</td>\n",
       "      <td>1292976000</td>\n",
       "      <td>COFFEE</td>\n",
       "      <td>I only drink coffee as a speciality and now I ...</td>\n",
       "      <td>b'drink coffe special drink cup day product gr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1368</th>\n",
       "      <td>1485</td>\n",
       "      <td>B000GULKW6</td>\n",
       "      <td>A2GAXGETOQMLJC</td>\n",
       "      <td>Kelly Kent Johnson</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>positive</td>\n",
       "      <td>1168473600</td>\n",
       "      <td>Can this really be gluten free?</td>\n",
       "      <td>I served this wonderful cake at a Christmas pa...</td>\n",
       "      <td>b'serv wonder cake christma parti hous celiac ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>444056</th>\n",
       "      <td>480145</td>\n",
       "      <td>B000NY510C</td>\n",
       "      <td>A2LYSGYMG61C97</td>\n",
       "      <td>Yvonne D. Lefevre</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>positive</td>\n",
       "      <td>1284422400</td>\n",
       "      <td>Excellent product - Great tin!</td>\n",
       "      <td>This paprika is very fresh &amp; the refillable ti...</td>\n",
       "      <td>b'paprika fresh refil tin packag superb hard f...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>446771</th>\n",
       "      <td>483035</td>\n",
       "      <td>B002AWXELI</td>\n",
       "      <td>A1GWJQLX1LSZF0</td>\n",
       "      <td>Larry G. Rogers</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>positive</td>\n",
       "      <td>1342828800</td>\n",
       "      <td>Do you want almonds with a smoke flavor?</td>\n",
       "      <td>I could not believe how good these were.  Not ...</td>\n",
       "      <td>b'could believ good blue diamond actual tast l...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>399486</th>\n",
       "      <td>431934</td>\n",
       "      <td>B002DHL8DG</td>\n",
       "      <td>A14SVM7UQYHPNX</td>\n",
       "      <td>c.c.</td>\n",
       "      <td>4</td>\n",
       "      <td>8</td>\n",
       "      <td>negative</td>\n",
       "      <td>1328486400</td>\n",
       "      <td>Unit price listed is incorrect</td>\n",
       "      <td>This isn't a review of the product per se, but...</td>\n",
       "      <td>b'isnt review product per warn say come cent o...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39910</th>\n",
       "      <td>43391</td>\n",
       "      <td>B004AG6HDO</td>\n",
       "      <td>A2F48MC03EGLPA</td>\n",
       "      <td>armybrat</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>positive</td>\n",
       "      <td>1293235200</td>\n",
       "      <td>french mustard</td>\n",
       "      <td>incredible flavor !Nothing like our american m...</td>\n",
       "      <td>b'incred flavor noth like american mstard mani...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>412359</th>\n",
       "      <td>445968</td>\n",
       "      <td>B001ECQ5FC</td>\n",
       "      <td>AD1GKE1M16054</td>\n",
       "      <td>minakocat</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>negative</td>\n",
       "      <td>1281484800</td>\n",
       "      <td>Not for all types</td>\n",
       "      <td>This deodorant is for women who sit at home or...</td>\n",
       "      <td>b'deodor women sit home offic day never lift f...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>343262</th>\n",
       "      <td>371347</td>\n",
       "      <td>B000FUYCJ0</td>\n",
       "      <td>A2OBWIVH5O1O4S</td>\n",
       "      <td>Steven M. Holgate \"Steven H\"</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>positive</td>\n",
       "      <td>1278633600</td>\n",
       "      <td>Coffee lover</td>\n",
       "      <td>Very rich but yet mello blend.  If I have to d...</td>\n",
       "      <td>b'rich yet mello blend drink coffe one top cho...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9588</th>\n",
       "      <td>10485</td>\n",
       "      <td>B0016J7SYU</td>\n",
       "      <td>A3V7RQT0IJL9T4</td>\n",
       "      <td>Maggie LaJoie</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>positive</td>\n",
       "      <td>1291507200</td>\n",
       "      <td>Absolutely the BEST Granola!</td>\n",
       "      <td>This granola is full of flavor, delightfully c...</td>\n",
       "      <td>b'granola full flavor delight crunchi without ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>148473</th>\n",
       "      <td>161077</td>\n",
       "      <td>B000EUFAHO</td>\n",
       "      <td>A2R6FX80N3Y2SX</td>\n",
       "      <td>Pushy Mama \"Push Your Child to Excel\"</td>\n",
       "      <td>4</td>\n",
       "      <td>12</td>\n",
       "      <td>negative</td>\n",
       "      <td>1176768000</td>\n",
       "      <td>Way too hard</td>\n",
       "      <td>I tried one bite of this cracker and had to th...</td>\n",
       "      <td>b'tri one bite cracker throw away fear crack t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42253</th>\n",
       "      <td>45975</td>\n",
       "      <td>B000G1KNU0</td>\n",
       "      <td>AXJKRHCRCZ8I6</td>\n",
       "      <td>elizabeth arnold</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>negative</td>\n",
       "      <td>1276473600</td>\n",
       "      <td>crushed cans</td>\n",
       "      <td>my eight cans all arrived full of serious dent...</td>\n",
       "      <td>b'eight can arriv full serious dent sinc ship ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10000 rows × 11 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            Id   ProductId          UserId  \\\n",
       "369243  399221  B000VK4K3W   AA157EV59BJGM   \n",
       "358793  388076  B007RTR8UM  A2KUVD844LL7QM   \n",
       "185818  201549  B000NURB18  A23OH1HHVZDKLI   \n",
       "444079  480169  B007GOCR5E  A1Z54EM24Y40LL   \n",
       "395554  427721  B003DIIMLK  A14DFCE6FVU07L   \n",
       "281055  304509  B0017OE536  A287V946KJP73N   \n",
       "191005  207094  B0015DQG22   A4UCU29FTOFCF   \n",
       "20335    22212  B000KV61FC  A3FV8UWW0KVI82   \n",
       "411032  444538  B001HTISGQ   ATN552TF5V40Z   \n",
       "60477    65742  B003KSL1B6  A3RR2P5IS3DGPR   \n",
       "55394    60102  B001E5E056  A3SHACREIY6HVX   \n",
       "62534    67941  B000G6MBUA  A3M2VGWFJGRUOH   \n",
       "394785  426901  B003OCCR8Y   AB7LQD5DGNG9U   \n",
       "6328      6925  B001LG945O  A2GF9I8Y6D3FFD   \n",
       "408906  442245  B001L4EME4  A3CI9DSB6Z76JF   \n",
       "96518   104882  B000EDK6K2   A9HU2P580OG39   \n",
       "121006  131181  B001RVCDME  A24WN7YK3UEPYY   \n",
       "418142  452181  B000HDL1DU   A6QF32V0HO07L   \n",
       "294535  319071  B002ZOKNGU  A26NVE6OVMAF4J   \n",
       "378501  409261  B0030HYAK6  A1FQGQ4Z1SQ01M   \n",
       "494789  534920  B005PXZ6JM  A2K104U5ND8QAL   \n",
       "484642  524048  B004JGQ15E  A3GWFJ48QPA2IR   \n",
       "81299    88404  B001EO5UY2   AKAT811SZJ5H6   \n",
       "385010  416322  B000NNCMG4  A2WSQ7FDU1RWDT   \n",
       "40820    44354  B0014C3QD6   AWPYPBFJFNBPX   \n",
       "15379    16803  B001LGGH40  A1LA4K5JF78BER   \n",
       "509836  551275  B003NC70X2  A1BNIAGZBAGU6R   \n",
       "282441  305992  B000LKXPD8  A1FM4VU4VIZYMV   \n",
       "314270  340265  B003Q4TVKW  A2I6P6A3YD8JA3   \n",
       "194972  211368  B0002R1HVC  A2H9HZK1OATBW0   \n",
       "...        ...         ...             ...   \n",
       "117939  127934  B002DPWQWA   AW9ZO67B1CFCA   \n",
       "397404  429694  B000F4D5IU  A1E0NN39IOBNOM   \n",
       "456246  493294  B003TC7WN4  A3LZCRW9NU0327   \n",
       "215413  233463  B00453M7RM   AY12DBB0U420B   \n",
       "224282  243210  B0000DK59A   A6NPVGUI0KXL3   \n",
       "175043  189833  B001EO5Q64  A2LCTKLCDRNAQO   \n",
       "96546   104912  B0012OV5PM  A2F8IHET2WY0XP   \n",
       "352596  381408  B000F99D2M  A2AMSFMUWXREZZ   \n",
       "370347  400495  B005A1JX98   A8NKI9II63LXM   \n",
       "5722      6197  B000UOFW9K  A1LARHHDXMS3CH   \n",
       "511148  552680  B002DHMWSG  A3M1TW7SZV6E5M   \n",
       "362751  392303  B001EQ50U4  A1CFYV8RQ8L6DS   \n",
       "372146  402439  B0014J5Y3E   AL309NM00MUI4   \n",
       "73264    79737  B000WFKSK0  A3798WV4B49HWV   \n",
       "276690  299840  B0040J01KQ   AKBZD4VMJ1FF4   \n",
       "163339  177097  B004FEJ968  A31YSTICHFYN7O   \n",
       "256189  277737  B0048IFVCM  A3BOGELRQJK8OE   \n",
       "188514  204457  B007OWQD0S  A3190EG59DUCVS   \n",
       "270503  293233  B000I6O3B8  A3RYWRIHN8PELF   \n",
       "153656  166606  B002AMXQNY  A3R41BEJLP6N2W   \n",
       "1368      1485  B000GULKW6  A2GAXGETOQMLJC   \n",
       "444056  480145  B000NY510C  A2LYSGYMG61C97   \n",
       "446771  483035  B002AWXELI  A1GWJQLX1LSZF0   \n",
       "399486  431934  B002DHL8DG  A14SVM7UQYHPNX   \n",
       "39910    43391  B004AG6HDO  A2F48MC03EGLPA   \n",
       "412359  445968  B001ECQ5FC   AD1GKE1M16054   \n",
       "343262  371347  B000FUYCJ0  A2OBWIVH5O1O4S   \n",
       "9588     10485  B0016J7SYU  A3V7RQT0IJL9T4   \n",
       "148473  161077  B000EUFAHO  A2R6FX80N3Y2SX   \n",
       "42253    45975  B000G1KNU0   AXJKRHCRCZ8I6   \n",
       "\n",
       "                                           ProfileName  HelpfulnessNumerator  \\\n",
       "369243                                 domenico luvera                     0   \n",
       "358793                                   Angela M. Hey                     0   \n",
       "185818                                          juliet                     0   \n",
       "444079                                              c2                     2   \n",
       "395554                                          FLRoss                     1   \n",
       "281055                                    Jason Bourne                     2   \n",
       "191005                                   Janet Krisman                     0   \n",
       "20335                                       W. Sprague                     0   \n",
       "411032                                              Lh                     4   \n",
       "60477                Dr. M. A. Dixon \"hyper-observant\"                     1   \n",
       "55394                        A. Floyd \"Fun_Accountant\"                     0   \n",
       "62534                   Eco-Friendly Interior Designer                     2   \n",
       "394785                              D. Henry \"Teacher\"                     0   \n",
       "6328                           Professional Book Buyer                     0   \n",
       "408906                               LB \"Helfenfreude\"                    17   \n",
       "96518                                    Melissa Soden                     1   \n",
       "121006                                        Katie D.                     0   \n",
       "418142                                       GbreadMan                     0   \n",
       "294535                                           Rosie                     2   \n",
       "378501                            T. Freitas \"JijiCat\"                     1   \n",
       "494789                                 Claudia McCowan                     0   \n",
       "484642                                        J. Weiss                     0   \n",
       "81299                                        D. Taylor                     1   \n",
       "385010  J P Simms III/Cross Community Church \"J Simms\"                     0   \n",
       "40820                                      Irish Danny                     0   \n",
       "15379                             Miyomoto Masaki \"MM\"                     1   \n",
       "509836                           judys4444 \"judy-m.p.\"                     3   \n",
       "282441                         J. Miller \"Island Girl\"                     0   \n",
       "314270                                           cathy                    12   \n",
       "194972                                  J. Schuhmacher                     0   \n",
       "...                                                ...                   ...   \n",
       "117939                                        NOLA Gal                     2   \n",
       "397404                                       S. Tickle                     1   \n",
       "456246                             Tinley Park Shopper                     1   \n",
       "215413                                   Gary Peterson                     0   \n",
       "224282                                         M. Saso                     0   \n",
       "175043                                         sharada                     7   \n",
       "96546                          Western Realm \"Miranda\"                     1   \n",
       "352596                                    chargergrl06                     0   \n",
       "370347                    Clare Guydish \"Going Places\"                     0   \n",
       "5722                                          C. Meyer                     0   \n",
       "511148                             cleaningoutmycloset                     0   \n",
       "362751                             maglean \"magaugust\"                     0   \n",
       "372146        Mary Corso \"Here's one for the big girl\"                     1   \n",
       "73264                                               JJ                     0   \n",
       "276690                                    astroqueen67                     0   \n",
       "163339                             CPR \"xfilebuffyfan\"                     0   \n",
       "256189                                  a gentle sound                     1   \n",
       "188514                                    janet bonser                     0   \n",
       "270503                                  R. J. Sorretto                     0   \n",
       "153656                                   LOVE TO LEARN                     0   \n",
       "1368                                Kelly Kent Johnson                     0   \n",
       "444056                               Yvonne D. Lefevre                     3   \n",
       "446771                                 Larry G. Rogers                     2   \n",
       "399486                                            c.c.                     4   \n",
       "39910                                         armybrat                     0   \n",
       "412359                                       minakocat                     1   \n",
       "343262                    Steven M. Holgate \"Steven H\"                     2   \n",
       "9588                                     Maggie LaJoie                     0   \n",
       "148473           Pushy Mama \"Push Your Child to Excel\"                     4   \n",
       "42253                                 elizabeth arnold                     2   \n",
       "\n",
       "        HelpfulnessDenominator     Score        Time  \\\n",
       "369243                       0  negative  1348617600   \n",
       "358793                       0  positive  1338249600   \n",
       "185818                       0  positive  1284854400   \n",
       "444079                       2  positive  1339372800   \n",
       "395554                       4  negative  1303948800   \n",
       "281055                       2  positive  1285891200   \n",
       "191005                       0  negative  1347235200   \n",
       "20335                        1  negative  1281657600   \n",
       "411032                       4  positive  1251763200   \n",
       "60477                        1  positive  1287792000   \n",
       "55394                        0  positive  1347494400   \n",
       "62534                        3  positive  1217980800   \n",
       "394785                       0  negative  1326240000   \n",
       "6328                         0  positive  1305417600   \n",
       "408906                      29  positive  1242345600   \n",
       "96518                        1  positive  1220659200   \n",
       "121006                       0  positive  1335830400   \n",
       "418142                       0  positive  1347321600   \n",
       "294535                       2  positive  1317686400   \n",
       "378501                       1  positive  1265414400   \n",
       "494789                       0  positive  1325548800   \n",
       "484642                       0  positive  1305244800   \n",
       "81299                        1  positive  1307232000   \n",
       "385010                       0  positive  1322006400   \n",
       "40820                        0  positive  1335225600   \n",
       "15379                        1  positive  1239408000   \n",
       "509836                       3  positive  1303776000   \n",
       "282441                       0  positive  1252195200   \n",
       "314270                      13  positive  1280793600   \n",
       "194972                       1  positive  1230336000   \n",
       "...                        ...       ...         ...   \n",
       "117939                       2  positive  1276128000   \n",
       "397404                       1  positive  1273795200   \n",
       "456246                       1  positive  1333584000   \n",
       "215413                       0  positive  1350950400   \n",
       "224282                       0  positive  1183766400   \n",
       "175043                      13  negative  1328313600   \n",
       "96546                        1  positive  1346371200   \n",
       "352596                       0  positive  1348963200   \n",
       "370347                       0  positive  1350777600   \n",
       "5722                         3  negative  1269907200   \n",
       "511148                       0  positive  1312848000   \n",
       "362751                       0  positive  1210291200   \n",
       "372146                       1  positive  1268784000   \n",
       "73264                        0  positive  1350691200   \n",
       "276690                       0  positive  1316822400   \n",
       "163339                       0  positive  1305763200   \n",
       "256189                       1  positive  1343952000   \n",
       "188514                       0  positive  1342828800   \n",
       "270503                       0  positive  1281225600   \n",
       "153656                       0  positive  1292976000   \n",
       "1368                         0  positive  1168473600   \n",
       "444056                       3  positive  1284422400   \n",
       "446771                       2  positive  1342828800   \n",
       "399486                       8  negative  1328486400   \n",
       "39910                        0  positive  1293235200   \n",
       "412359                       5  negative  1281484800   \n",
       "343262                       3  positive  1278633600   \n",
       "9588                         0  positive  1291507200   \n",
       "148473                      12  negative  1176768000   \n",
       "42253                        3  negative  1276473600   \n",
       "\n",
       "                                                  Summary  \\\n",
       "369243                                    Formula Changed   \n",
       "358793  Creamy conditioner that flows well out of the ...   \n",
       "185818                                        &lt;3 it!!!   \n",
       "444079                         One of the boys' favorites   \n",
       "395554                                               ICK!   \n",
       "281055  delicious with everything!  alone....well an a...   \n",
       "191005                                        Lifesavers?   \n",
       "20335                           Dog doesn't use it at all   \n",
       "411032                            They really mean sticky   \n",
       "60477                                My favorite cinnamon   \n",
       "55394                I'm craving this cereal right now :)   \n",
       "62534                                           Best Chip   \n",
       "394785                                             Review   \n",
       "6328                             My Son's Favorite \"Soda\"   \n",
       "408906        Several packages for the same yummy product   \n",
       "96518                    Bob's Red Mill Powder Buttermilk   \n",
       "121006                          First thought- Bear Claw!   \n",
       "418142      Never thought sardines could taste this good!   \n",
       "294535                                         dentedinpa   \n",
       "378501                       Favorite Gluten Free Cracker   \n",
       "494789                      Great alternative to Lansinoh   \n",
       "484642                                         Delicious!   \n",
       "81299                                          Delicious!   \n",
       "385010                                         Great Tea!   \n",
       "40820                                     My Old Favorite   \n",
       "15379                                 My Family Likes It.   \n",
       "509836                                              great   \n",
       "282441                          SoLo Bars are the Best!!!   \n",
       "314270                    wish they were in glass bottles   \n",
       "194972                                   Definitely a Hit   \n",
       "...                                                   ...   \n",
       "117939                              picky dog didn't like   \n",
       "397404                                    wonderful taste   \n",
       "456246                                        Favorite!!!   \n",
       "215413                 Incredibly Good, If It's The Same.   \n",
       "224282                                               MMMM   \n",
       "175043                            This is not coconut oil   \n",
       "96546                               My family loves this.   \n",
       "352596                                      Tastes great!   \n",
       "370347                         Great tasting gluten free!   \n",
       "5722             Hard as a rock with about as much flavor   \n",
       "511148                          amazon vs. grocery strore   \n",
       "362751                                 Kool-Aid excellent   \n",
       "372146                                        size matter   \n",
       "73264                                         Dinner time   \n",
       "276690                                         Great Deal   \n",
       "163339                             Tastes Like Real Fruit   \n",
       "256189                               Dinner in 20 minutes   \n",
       "188514                                      the best ever   \n",
       "270503                                  Oh, It's So Good!   \n",
       "153656                                             COFFEE   \n",
       "1368                      Can this really be gluten free?   \n",
       "444056                     Excellent product - Great tin!   \n",
       "446771           Do you want almonds with a smoke flavor?   \n",
       "399486                     Unit price listed is incorrect   \n",
       "39910                                      french mustard   \n",
       "412359                                  Not for all types   \n",
       "343262                                       Coffee lover   \n",
       "9588                         Absolutely the BEST Granola!   \n",
       "148473                                       Way too hard   \n",
       "42253                                        crushed cans   \n",
       "\n",
       "                                                     Text  \\\n",
       "369243  The Newman's Own Organics 2nd Generation Turke...   \n",
       "358793  The conditioner ingredients are unremarkable -...   \n",
       "185818  a friend gave this to me and I love it!!!  So ...   \n",
       "444079  I always have Powerade in the house - for ever...   \n",
       "395554  What happened Annie?.. There are very few Anni...   \n",
       "281055  This product helped a friend lose 50 lbs and I...   \n",
       "191005  The so-called \"new formula\" has a strong chemi...   \n",
       "20335   I thought this would be fantastic to keep my p...   \n",
       "411032  I grew up with sticky rice but this is actuall...   \n",
       "60477   While not organic, Saigon Cinnamon is the best...   \n",
       "55394   This is my all-time favorite cereal.  It taste...   \n",
       "62534   Kettle lightly slated chips are my favorite on...   \n",
       "394785  This mix is very good.  I would add heavy crea...   \n",
       "6328    We really have to limit additives and sugar fo...   \n",
       "408906  OK, so I'm used to drinking (and have a prefer...   \n",
       "96518   Great stuff. I always have it on hand for cook...   \n",
       "121006  As soon as I smelled the Almond Trail Mix Earn...   \n",
       "418142  I've only eaten sardines for about a year.  I ...   \n",
       "294535  Great coffee at a decent price. Cheaper than m...   \n",
       "378501  Crunchy and delicious with anything. We use it...   \n",
       "494789  I got some hand-me-down Lansinoh breastmilk ba...   \n",
       "484642  I love this little snack idea!  I have tried a...   \n",
       "81299   Hands down this is one of the best prepared so...   \n",
       "385010  This tea is excellent and is one of our favori...   \n",
       "40820   When I first got out of boot camp, my first du...   \n",
       "15379   I don't read every label on every drink or foo...   \n",
       "509836  i ordered 3 bags of this for me and it's the b...   \n",
       "282441  I love the Mint Mania SoLo Bar!  The low glyce...   \n",
       "314270  I wish these were in glass bottles, but someti...   \n",
       "194972  Stuck for an idea for a \"decade\" gift?  This w...   \n",
       "...                                                   ...   \n",
       "117939  I'm sure this product is fine, my dog is just ...   \n",
       "397404  The perfect coconut flakes, unsweetened, has a...   \n",
       "456246  For years, I've been completely devoted to Dun...   \n",
       "215413  In shopping at a nearby Costco yesterday, I ca...   \n",
       "224282  This oil for our candy making is great and it ...   \n",
       "175043  I'm using Nutiva coconutoil since 2008, and I ...   \n",
       "96546   We purchased this from Amazon as our local sto...   \n",
       "352596  The pomegranate flavor in this tea makes it a ...   \n",
       "370347  Perfect with cheese or humus! THis crackers ar...   \n",
       "5722    These things are like eating poker chips with ...   \n",
       "511148  The cereal is great, and the price is cheaper ...   \n",
       "362751  I'm please with this purchase, the kool-aid ta...   \n",
       "372146  It was very hard for me to determine what size...   \n",
       "73264   Great wet dog food and worth the cost. My pug ...   \n",
       "276690  This box was huge and will last a long time! P...   \n",
       "163339  This cookie is better than expected.  They are...   \n",
       "256189  I saw this product today at the store. I'm a g...   \n",
       "188514  i purchased the original biscotti and ate the ...   \n",
       "270503  This amaretto coffee is wonderful, with a nice...   \n",
       "153656  I only drink coffee as a speciality and now I ...   \n",
       "1368    I served this wonderful cake at a Christmas pa...   \n",
       "444056  This paprika is very fresh & the refillable ti...   \n",
       "446771  I could not believe how good these were.  Not ...   \n",
       "399486  This isn't a review of the product per se, but...   \n",
       "39910   incredible flavor !Nothing like our american m...   \n",
       "412359  This deodorant is for women who sit at home or...   \n",
       "343262  Very rich but yet mello blend.  If I have to d...   \n",
       "9588    This granola is full of flavor, delightfully c...   \n",
       "148473  I tried one bite of this cracker and had to th...   \n",
       "42253   my eight cans all arrived full of serious dent...   \n",
       "\n",
       "                                              CleanedText  \n",
       "369243  b'newman organ generat turkey veget chang ingr...  \n",
       "358793  b'condition ingredi unremark chemic plus oil a...  \n",
       "185818         b'friend gave love good doesnt need anyth'  \n",
       "444079  b'alway powerad hous everyth flu over hot base...  \n",
       "395554  b'happen anni anni product say yuck prob one r...  \n",
       "281055  b'product help friend lose lbs lost lbs produc...  \n",
       "191005  b'new formula strong chemic tast dont like hop...  \n",
       "20335   b'thought would fantast keep puppi busi chew k...  \n",
       "411032  b'grew sticki rice actual hard pull apart cont...  \n",
       "60477   b'organ saigon cinnamon best tast cinnamon opi...  \n",
       "55394   b'favorit cereal tast like healthi blueberri m...  \n",
       "62534   b'kettl light slate chip favorit market dont c...  \n",
       "394785  b'mix good would add heavi cream instead butte...  \n",
       "6328    b'realli limit addit sugar son autism switch o...  \n",
       "408906  b'use drink prefer can aluminum top peel teeni...  \n",
       "96518   b'great stuff alway hand cook anyth call butte...  \n",
       "121006  b'soon smell almond trail mix earnest eat bar ...  \n",
       "418142  b'ive eaten sardin year heard healthi environm...  \n",
       "294535  b'great coffe decent price cheaper local groce...  \n",
       "378501  b'crunchi delici anyth use place bread often y...  \n",
       "494789  b'got lansinoh breastmilk bag went back work t...  \n",
       "484642  b'love littl snack idea tri lot handi low calo...  \n",
       "81299   b'hand one best prepar soup add cup water cook...  \n",
       "385010  b'tea excel one favorit flavor absolut wonder ...  \n",
       "40820   b'first got boot camp first duti station quant...  \n",
       "15379   b'dont read everi label everi drink food consu...  \n",
       "509836  b'order bag best popcorn yet almost everi kern...  \n",
       "282441  b'love mint mania solo bar low glycem qualiti ...  \n",
       "314270  b'wish glass bottl sometim up beat box pretti ...  \n",
       "194972  b'stuck idea decad gift hit recipi guest order...  \n",
       "...                                                   ...  \n",
       "117939  b'sure product fine dog incred picki receiv qu...  \n",
       "397404  b'perfect coconut flake unsweeten wonder tast ...  \n",
       "456246  b'year ive complet devot dunkin donut brew sto...  \n",
       "215413  b'shop nearbi costco yesterday came across kir...  \n",
       "224282       b'oil candi make great ship rite away pleas'  \n",
       "175043  b'use nutiva coconutoil sinc alway bought paca...  \n",
       "96546   b'purchas amazon local store carri occasion ex...  \n",
       "352596  b'pomegran flavor tea make winner littl bit ex...  \n",
       "370347  b'perfect chees humus cracker tasti crunchi gl...  \n",
       "5722    b'thing like eat poker chip season salt theyr ...  \n",
       "511148  b'cereal great price cheaper prime ship local ...  \n",
       "362751  b'pleas purchas tast good get lot price excel ...  \n",
       "372146  b'hard determin size gumbal buy descript said ...  \n",
       "73264   b'great wet dog food worth cost pug digest foo...  \n",
       "276690  b'box huge last long time product intact broke...  \n",
       "163339  b'cooki better expect larger regular cooki thi...  \n",
       "256189  b'saw product today store good cook sure day w...  \n",
       "188514  b'purchas origin biscotti ate whole box sit we...  \n",
       "270503  b'amaretto coffe wonder nice mellow flavor alm...  \n",
       "153656  b'drink coffe special drink cup day product gr...  \n",
       "1368    b'serv wonder cake christma parti hous celiac ...  \n",
       "444056  b'paprika fresh refil tin packag superb hard f...  \n",
       "446771  b'could believ good blue diamond actual tast l...  \n",
       "399486  b'isnt review product per warn say come cent o...  \n",
       "39910   b'incred flavor noth like american mstard mani...  \n",
       "412359  b'deodor women sit home offic day never lift f...  \n",
       "343262  b'rich yet mello blend drink coffe one top cho...  \n",
       "9588    b'granola full flavor delight crunchi without ...  \n",
       "148473  b'tri one bite cracker throw away fear crack t...  \n",
       "42253   b'eight can arriv full serious dent sinc ship ...  \n",
       "\n",
       "[10000 rows x 11 columns]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random_final = final_df.sample(n=10000)# Sampling 10krows \n",
    "random_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_final = random_final.sort_values('Time') # Sorting the dataframe based on Time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>ProductId</th>\n",
       "      <th>UserId</th>\n",
       "      <th>ProfileName</th>\n",
       "      <th>HelpfulnessNumerator</th>\n",
       "      <th>HelpfulnessDenominator</th>\n",
       "      <th>Score</th>\n",
       "      <th>Time</th>\n",
       "      <th>Summary</th>\n",
       "      <th>Text</th>\n",
       "      <th>CleanedText</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>138691</th>\n",
       "      <td>150509</td>\n",
       "      <td>0006641040</td>\n",
       "      <td>A3CMRKGE0P909G</td>\n",
       "      <td>Teresa</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>positive</td>\n",
       "      <td>1018396800</td>\n",
       "      <td>A great way to learn the months</td>\n",
       "      <td>This is a book of poetry about the months of t...</td>\n",
       "      <td>b'book poetri month year goe month cute littl ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>325019</th>\n",
       "      <td>351770</td>\n",
       "      <td>B0000DG4B3</td>\n",
       "      <td>A1IU7S4HCK1XK0</td>\n",
       "      <td>Joanna Daneman</td>\n",
       "      <td>13</td>\n",
       "      <td>15</td>\n",
       "      <td>positive</td>\n",
       "      <td>1072656000</td>\n",
       "      <td>If they must have mac and cheese like the box ...</td>\n",
       "      <td>This is the powdered dried cheese like in &amp;quo...</td>\n",
       "      <td>b'powder dri chees like canadian friend call r...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>422107</th>\n",
       "      <td>456525</td>\n",
       "      <td>B001D6C13O</td>\n",
       "      <td>A281NPSIMI1C2R</td>\n",
       "      <td>Rebecca of Amazon \"The Rebecca Review\"</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>positive</td>\n",
       "      <td>1074988800</td>\n",
       "      <td>Organic Darjeeling best with Organic Sucanat</td>\n",
       "      <td>\"Sitting on the porch of a bungalow on a tea p...</td>\n",
       "      <td>b'sit porch bungalow tea plantat darjeel see p...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>316306</th>\n",
       "      <td>342462</td>\n",
       "      <td>B000084F3O</td>\n",
       "      <td>A3DWUM6SN3N3NR</td>\n",
       "      <td>Author Brian Wallace (Mind Transmission, Inc.)</td>\n",
       "      <td>4</td>\n",
       "      <td>33</td>\n",
       "      <td>negative</td>\n",
       "      <td>1076457600</td>\n",
       "      <td>the most unnatural odor</td>\n",
       "      <td>I really hate to do this (having been a fan of...</td>\n",
       "      <td>b'realli hate fan van patten year feel must sa...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>389412</th>\n",
       "      <td>421062</td>\n",
       "      <td>B0000E227M</td>\n",
       "      <td>A55MRYPUAX4QU</td>\n",
       "      <td>Avid Reader</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>positive</td>\n",
       "      <td>1076803200</td>\n",
       "      <td>Musky, exquisite</td>\n",
       "      <td>The quality of this coffee is the first thing ...</td>\n",
       "      <td>b'qualiti coffe first thing one notic goe with...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>125303</th>\n",
       "      <td>135926</td>\n",
       "      <td>B0001217A4</td>\n",
       "      <td>A20Q6AL0RPC1US</td>\n",
       "      <td>J. Giles</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>positive</td>\n",
       "      <td>1080864000</td>\n",
       "      <td>Yo, Cherry Limeade Recipe</td>\n",
       "      <td>Yes, summer's coming. Rather than drinking tea...</td>\n",
       "      <td>b'yes summer come rather drink tea sweet break...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>222671</th>\n",
       "      <td>241462</td>\n",
       "      <td>B0001E5CJO</td>\n",
       "      <td>A239VY115ZCDFU</td>\n",
       "      <td>S. Johnston \"scott13\"</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>positive</td>\n",
       "      <td>1090368000</td>\n",
       "      <td>you should check this out...</td>\n",
       "      <td>Despite the silly premise and acting, i can wa...</td>\n",
       "      <td>b'despit silli premis act watch fim quit sure ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>413420</th>\n",
       "      <td>447119</td>\n",
       "      <td>B0002PCET8</td>\n",
       "      <td>A2TWEIJVSW1G61</td>\n",
       "      <td>A. Rehnblom</td>\n",
       "      <td>8</td>\n",
       "      <td>8</td>\n",
       "      <td>positive</td>\n",
       "      <td>1095206400</td>\n",
       "      <td>yummm</td>\n",
       "      <td>This stuff tastes just like Sweettarts.  I rec...</td>\n",
       "      <td>b'stuff tast like sweettart recommend anyon ge...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>422255</th>\n",
       "      <td>456689</td>\n",
       "      <td>B00068K7UE</td>\n",
       "      <td>A1W9KQRCZ9ORHB</td>\n",
       "      <td>Stuart Gardner \"www.sdgardner.com\"</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>positive</td>\n",
       "      <td>1097712000</td>\n",
       "      <td>Excellent - Fish Breath No More</td>\n",
       "      <td>If your cat has \"foul\" breath after a chicken ...</td>\n",
       "      <td>b'cat foul breath chicken dinner even fish rea...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>457717</th>\n",
       "      <td>494909</td>\n",
       "      <td>B0002ML9U6</td>\n",
       "      <td>AQ8DU6XVA3USJ</td>\n",
       "      <td>Alejandra Vernon \"artist &amp; illustrator\"</td>\n",
       "      <td>15</td>\n",
       "      <td>17</td>\n",
       "      <td>positive</td>\n",
       "      <td>1098057600</td>\n",
       "      <td>for fretting felines and canines</td>\n",
       "      <td>This 100% natural product for cats and dogs go...</td>\n",
       "      <td>b'natur product cat dog go stress time includ ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>192387</th>\n",
       "      <td>208609</td>\n",
       "      <td>B0000WATVI</td>\n",
       "      <td>APRYPBTSZGT61</td>\n",
       "      <td>Lynn Babylon</td>\n",
       "      <td>6</td>\n",
       "      <td>7</td>\n",
       "      <td>positive</td>\n",
       "      <td>1100649600</td>\n",
       "      <td>Very Good</td>\n",
       "      <td>Rich pumpkin flavor. Can be made with skim mil...</td>\n",
       "      <td>b'rich pumpkin flavor made skim milk low fat d...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>333926</th>\n",
       "      <td>361313</td>\n",
       "      <td>B00005IX96</td>\n",
       "      <td>A2TCI780LLLGT2</td>\n",
       "      <td>aussie 2003</td>\n",
       "      <td>6</td>\n",
       "      <td>7</td>\n",
       "      <td>positive</td>\n",
       "      <td>1101081600</td>\n",
       "      <td>Loves convenience in GA</td>\n",
       "      <td>These pods have got to be the best invention y...</td>\n",
       "      <td>b'pod got best invent yet compact extrem easi ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>288677</th>\n",
       "      <td>312685</td>\n",
       "      <td>B0001R04LM</td>\n",
       "      <td>A2XRN476BGNLHX</td>\n",
       "      <td>Robin Boltz \"Robin RN\"</td>\n",
       "      <td>5</td>\n",
       "      <td>15</td>\n",
       "      <td>positive</td>\n",
       "      <td>1103587200</td>\n",
       "      <td>Fun for Grown Ups old and young</td>\n",
       "      <td>I'm taking this to treat my grad school classm...</td>\n",
       "      <td>b'take treat grad school classmat dont think a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>499872</th>\n",
       "      <td>540455</td>\n",
       "      <td>B0000UBTZA</td>\n",
       "      <td>A2LMGWXI2UCA17</td>\n",
       "      <td>HSU Girl</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>positive</td>\n",
       "      <td>1103673600</td>\n",
       "      <td>This is the best Chai tea</td>\n",
       "      <td>My boyfriend and I both love chai tea, and aft...</td>\n",
       "      <td>b'boyfriend love chai tea tri sever brand flav...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16085</th>\n",
       "      <td>17572</td>\n",
       "      <td>B0000GH6UQ</td>\n",
       "      <td>A2Q2IWWX5MLU9C</td>\n",
       "      <td>T. Butterfield \"lab rat\"</td>\n",
       "      <td>8</td>\n",
       "      <td>9</td>\n",
       "      <td>positive</td>\n",
       "      <td>1104451200</td>\n",
       "      <td>The Best</td>\n",
       "      <td>It is the best hot chocolate I have ever taste...</td>\n",
       "      <td>b'best hot chocol ever tast cinnamon make good'</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>108404</th>\n",
       "      <td>117662</td>\n",
       "      <td>B0004MTZQE</td>\n",
       "      <td>A32Z0PMCKUBKJQ</td>\n",
       "      <td>Guapo Poppo \"Sr. Poppo\"</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>positive</td>\n",
       "      <td>1106092800</td>\n",
       "      <td>Caliente...</td>\n",
       "      <td>...and mustardy, I like mine on French bread w...</td>\n",
       "      <td>b'mustardi like mine french bread big slice pr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>119763</th>\n",
       "      <td>129865</td>\n",
       "      <td>B0002CRZRW</td>\n",
       "      <td>A1EAJX56LZKIAU</td>\n",
       "      <td>Wiver \"JCO\"</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>positive</td>\n",
       "      <td>1110412800</td>\n",
       "      <td>This tea has a fresh and nice look</td>\n",
       "      <td>This tea tastes great, and it gives a nicely c...</td>\n",
       "      <td>b'tea tast great give nice conceiv fresh look ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>155416</th>\n",
       "      <td>168562</td>\n",
       "      <td>B0001ES9F8</td>\n",
       "      <td>A1R9HPMUY9DGVE</td>\n",
       "      <td>B. Alvarez</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>positive</td>\n",
       "      <td>1114905600</td>\n",
       "      <td>Finally some flavored coffee for my senseo</td>\n",
       "      <td>The hazelnut flavor is a little on the light s...</td>\n",
       "      <td>b'hazelnut flavor littl light side that way li...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>348264</th>\n",
       "      <td>376710</td>\n",
       "      <td>B00014E3LM</td>\n",
       "      <td>AFQ3LSZ22S8TP</td>\n",
       "      <td>Tickleberries</td>\n",
       "      <td>4</td>\n",
       "      <td>7</td>\n",
       "      <td>positive</td>\n",
       "      <td>1116201600</td>\n",
       "      <td>Enjoyed them</td>\n",
       "      <td>The sugar-free was very good. I've had the reg...</td>\n",
       "      <td>b'good ive regular realli good especi diabet w...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>399616</th>\n",
       "      <td>432077</td>\n",
       "      <td>B0052OUAOK</td>\n",
       "      <td>A2MNUZHMGVN5G2</td>\n",
       "      <td>ARA</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>positive</td>\n",
       "      <td>1117238400</td>\n",
       "      <td>Excellent!!</td>\n",
       "      <td>These are some of the very best tasting energy...</td>\n",
       "      <td>b'best tast energi bar ive ever best ingredi i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>399603</th>\n",
       "      <td>432064</td>\n",
       "      <td>B0052OUAOK</td>\n",
       "      <td>A2MNUZHMGVN5G2</td>\n",
       "      <td>ARA</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>positive</td>\n",
       "      <td>1117238400</td>\n",
       "      <td>GREENS + Energy Bars are FABULOUS!!!</td>\n",
       "      <td>These are some of the very best tasting energy...</td>\n",
       "      <td>b'best tast energi bar ive ever best ingredi i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>471469</th>\n",
       "      <td>509847</td>\n",
       "      <td>B00023T3C6</td>\n",
       "      <td>A35XJ3GRQ9AZX1</td>\n",
       "      <td>L M S</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>positive</td>\n",
       "      <td>1117411200</td>\n",
       "      <td>Mae Ploy Sweet Chilli Sauce</td>\n",
       "      <td>This is the best.  It tastes just like the sau...</td>\n",
       "      <td>b'best tast like sauc come crab wonton chang'</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>327668</th>\n",
       "      <td>354650</td>\n",
       "      <td>B0000E5JQK</td>\n",
       "      <td>AIJY3RG7PU6XC</td>\n",
       "      <td>Beverly L. Mitchell \"teachercreature\"</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>positive</td>\n",
       "      <td>1117411200</td>\n",
       "      <td>great stuff</td>\n",
       "      <td>these things are really great....they are hot,...</td>\n",
       "      <td>b'thing realli great hot good read make want'</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>102032</th>\n",
       "      <td>110824</td>\n",
       "      <td>B0019421NA</td>\n",
       "      <td>A25YR04TDJA8SP</td>\n",
       "      <td>NuncVideo</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>positive</td>\n",
       "      <td>1120608000</td>\n",
       "      <td>Our Toddler Loves These</td>\n",
       "      <td>I'm no \"health nut\", but my wife and I both fi...</td>\n",
       "      <td>b'health nut wife figur toddler doesnt need co...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>155514</th>\n",
       "      <td>168665</td>\n",
       "      <td>B0001ES9F8</td>\n",
       "      <td>A1TKZYH3PPTN9J</td>\n",
       "      <td>Buyer Beware</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>positive</td>\n",
       "      <td>1120780800</td>\n",
       "      <td>More options for my Senseo</td>\n",
       "      <td>I love my Senseo, and I am thrilled to have fl...</td>\n",
       "      <td>b'love senseo thrill flavor option person much...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>410085</th>\n",
       "      <td>443518</td>\n",
       "      <td>B000634FVG</td>\n",
       "      <td>AX3O3ANDPKDIF</td>\n",
       "      <td>blackholesun1Girl \"blackholesun1\"</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>positive</td>\n",
       "      <td>1123459200</td>\n",
       "      <td>My dog is the one that I listen to...</td>\n",
       "      <td>My dog Annie had surgery about 2 months ago an...</td>\n",
       "      <td>b'dog anni surgeri month ago ever sinc hard ge...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>523288</th>\n",
       "      <td>565711</td>\n",
       "      <td>B0002BG8QC</td>\n",
       "      <td>A32K9W2DFEN65X</td>\n",
       "      <td>KMD \"pleased customer\"</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>positive</td>\n",
       "      <td>1123891200</td>\n",
       "      <td>quick service---good quality</td>\n",
       "      <td>The basket arrived to the recipient in a lot l...</td>\n",
       "      <td>b'basket arriv recipi lot less time nice surpr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>197329</th>\n",
       "      <td>213879</td>\n",
       "      <td>B0000D9MXL</td>\n",
       "      <td>A1S1J26I7MDQ6Z</td>\n",
       "      <td>Robert D. Clark</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>positive</td>\n",
       "      <td>1125878400</td>\n",
       "      <td>A cheesey comment</td>\n",
       "      <td>Excellent taste.  All who partook said it was ...</td>\n",
       "      <td>b'excel tast partook said much better four far...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>155515</th>\n",
       "      <td>168666</td>\n",
       "      <td>B0001ES9F8</td>\n",
       "      <td>AIK391A7XIK3W</td>\n",
       "      <td>Grace Sardes \"busy mom\"</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>positive</td>\n",
       "      <td>1126224000</td>\n",
       "      <td>Coffee house coffee right in your own home!</td>\n",
       "      <td>These coffee pods are wonderful. The hazelnut ...</td>\n",
       "      <td>b'coffe pod wonder hazelnut flavor subtl delic...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>428355</th>\n",
       "      <td>463238</td>\n",
       "      <td>B000GZSCKO</td>\n",
       "      <td>A1J64SRMB9XJH3</td>\n",
       "      <td>Steven S. Guzman \"Professional Geek\"</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>positive</td>\n",
       "      <td>1126742400</td>\n",
       "      <td>WOW!  As good as hamburger, but BETTER!</td>\n",
       "      <td>How could a box of dried stuff come out sooooo...</td>\n",
       "      <td>b'could box dri stuff come sooooo good dont kn...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>488853</th>\n",
       "      <td>528608</td>\n",
       "      <td>B002L80MZ2</td>\n",
       "      <td>A3NTE6U617M3RH</td>\n",
       "      <td>Salome \"Salome\"</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>positive</td>\n",
       "      <td>1351036800</td>\n",
       "      <td>Good Product, Costs too Much...</td>\n",
       "      <td>I look for good stuff for our Great Pyrenees p...</td>\n",
       "      <td>b'look good stuff great pyrene pup tri product...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>379806</th>\n",
       "      <td>410667</td>\n",
       "      <td>B002QM4O3E</td>\n",
       "      <td>AP520XQ9GV1AJ</td>\n",
       "      <td>Bernie</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>positive</td>\n",
       "      <td>1351036800</td>\n",
       "      <td>A winner at our home</td>\n",
       "      <td>My vet recommended Dentahex Chews for our two ...</td>\n",
       "      <td>b'vet recommend dentahex chew two poodl three ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36633</th>\n",
       "      <td>39813</td>\n",
       "      <td>B000E7QYNG</td>\n",
       "      <td>A1W4MGP0ARF714</td>\n",
       "      <td>Tlilms</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>positive</td>\n",
       "      <td>1351036800</td>\n",
       "      <td>Alot of seed</td>\n",
       "      <td>This is very good seed and alot of it . I use ...</td>\n",
       "      <td>b'good seed alot use itfor grow wheatgrass jui...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>229729</th>\n",
       "      <td>249091</td>\n",
       "      <td>B00785V2G6</td>\n",
       "      <td>ABWCUS3HBDZRS</td>\n",
       "      <td>Rachel R</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>positive</td>\n",
       "      <td>1351123200</td>\n",
       "      <td>Excellent product</td>\n",
       "      <td>After scouring every store in town for orange ...</td>\n",
       "      <td>b'scour everi store town orang peel find anyth...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>188285</th>\n",
       "      <td>204217</td>\n",
       "      <td>B001SATTGM</td>\n",
       "      <td>A3780M8J99RDJM</td>\n",
       "      <td>socialfeline</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>positive</td>\n",
       "      <td>1351123200</td>\n",
       "      <td>Finally!!!!</td>\n",
       "      <td>I am so glad that I have finally been able to ...</td>\n",
       "      <td>b'glad final abl get black bean famili love ca...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>517873</th>\n",
       "      <td>559891</td>\n",
       "      <td>B008YA1P9I</td>\n",
       "      <td>A1FP5ZLSKR07SU</td>\n",
       "      <td>G. Burnick</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>positive</td>\n",
       "      <td>1351123200</td>\n",
       "      <td>A longtime favorite</td>\n",
       "      <td>Lake and Lodge has been a favorite non-flavore...</td>\n",
       "      <td>b'lake lodg favorit sinc start use keurig mach...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>289603</th>\n",
       "      <td>313689</td>\n",
       "      <td>B005OVPK9G</td>\n",
       "      <td>A30JYPESIBJ8DZ</td>\n",
       "      <td>TM Goss</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>positive</td>\n",
       "      <td>1351123200</td>\n",
       "      <td>Wait...what?! Bland?!</td>\n",
       "      <td>To everyone giving less stars on here for it b...</td>\n",
       "      <td>b'everyon give less star bland brain attempt s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>134672</th>\n",
       "      <td>146179</td>\n",
       "      <td>B003AP2GKY</td>\n",
       "      <td>A2RSB6FVQ9K9OD</td>\n",
       "      <td>Doc</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>positive</td>\n",
       "      <td>1351123200</td>\n",
       "      <td>Bulk k-Cups</td>\n",
       "      <td>This is the best way to buy coffee for my offi...</td>\n",
       "      <td>b'best way buy coffe offic least expens way bu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20122</th>\n",
       "      <td>21923</td>\n",
       "      <td>B000KV61FC</td>\n",
       "      <td>A281J4UAH70YBA</td>\n",
       "      <td>Sophie</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>positive</td>\n",
       "      <td>1351123200</td>\n",
       "      <td>Fantastic for energetic big pups</td>\n",
       "      <td>My 70 lb Shepard/boxer mix absolutely loves he...</td>\n",
       "      <td>b'shepard boxer mix absolut love take mintu fi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>418992</th>\n",
       "      <td>453092</td>\n",
       "      <td>B0029XLH4Y</td>\n",
       "      <td>A2CSPTFY7QPT78</td>\n",
       "      <td>Mary G. Hershey</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>positive</td>\n",
       "      <td>1351123200</td>\n",
       "      <td>Great bold taste-- compare to Emeril's Bold</td>\n",
       "      <td>I've been drinking Emeril's Bold for a year an...</td>\n",
       "      <td>b'ive drink emeril bold year half want tri som...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>507044</th>\n",
       "      <td>548295</td>\n",
       "      <td>B000FPJ78G</td>\n",
       "      <td>A3F14GVSJMJ0WH</td>\n",
       "      <td>Theresa A. Miller</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>negative</td>\n",
       "      <td>1351123200</td>\n",
       "      <td>dingos</td>\n",
       "      <td>The red part of the dingos was missing from 3 ...</td>\n",
       "      <td>b'red part dingo miss ball one bag two other o...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58809</th>\n",
       "      <td>63886</td>\n",
       "      <td>B004SNMAOO</td>\n",
       "      <td>A3HVA6BTVB1UYP</td>\n",
       "      <td>Christina A. Salemi \"Stinarat\"</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>positive</td>\n",
       "      <td>1351123200</td>\n",
       "      <td>love this bar</td>\n",
       "      <td>Sick of melting chocolate or yogurt icing in y...</td>\n",
       "      <td>b'sick melt chocol yogurt ice bar well good on...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>396219</th>\n",
       "      <td>428428</td>\n",
       "      <td>B000TZ8WEC</td>\n",
       "      <td>A8NIRP9EJMP7J</td>\n",
       "      <td>Marie DeFalco</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>positive</td>\n",
       "      <td>1351123200</td>\n",
       "      <td>Bags on Board waste pickup bags</td>\n",
       "      <td>These are decent bags with an easy tear off st...</td>\n",
       "      <td>b'decent bag easi tear strip hold content well...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>179145</th>\n",
       "      <td>194297</td>\n",
       "      <td>B000VDL7RG</td>\n",
       "      <td>A2QGTXLC93BRV9</td>\n",
       "      <td>Steven S</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>positive</td>\n",
       "      <td>1351123200</td>\n",
       "      <td>Good but not as good as I remember</td>\n",
       "      <td>I could be wrong but it seems like these used ...</td>\n",
       "      <td>b'could wrong seem like use better read review...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>404593</th>\n",
       "      <td>437504</td>\n",
       "      <td>B003C15EUK</td>\n",
       "      <td>A32QQ3P51L0LO4</td>\n",
       "      <td>Someone \"Kate\"</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>positive</td>\n",
       "      <td>1351123200</td>\n",
       "      <td>Always great!</td>\n",
       "      <td>Got these for my sister who has Celiac disease...</td>\n",
       "      <td>b'got sister celiac diseas need one candi bar ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>486800</th>\n",
       "      <td>526403</td>\n",
       "      <td>B001E5E1KA</td>\n",
       "      <td>AZPT0S609H54D</td>\n",
       "      <td>Bob  Berman \"BirderBob\"</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>negative</td>\n",
       "      <td>1351123200</td>\n",
       "      <td>Has trouble dissolving</td>\n",
       "      <td>I tried this product to be used in my cold ice...</td>\n",
       "      <td>b'tri product use cold ice tea limeaid also us...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>187833</th>\n",
       "      <td>203739</td>\n",
       "      <td>B0002DGWO8</td>\n",
       "      <td>A2EADJSLFFT39G</td>\n",
       "      <td>M. Jackson</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>positive</td>\n",
       "      <td>1351123200</td>\n",
       "      <td>Consistent</td>\n",
       "      <td>These chews are a consistent size.  Some of th...</td>\n",
       "      <td>b'chew consist size bag purchas big retail che...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>457924</th>\n",
       "      <td>495127</td>\n",
       "      <td>B001BOE3UW</td>\n",
       "      <td>A18J98YVUMMDOA</td>\n",
       "      <td>andromeda</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>positive</td>\n",
       "      <td>1351209600</td>\n",
       "      <td>You can taste the ginger.</td>\n",
       "      <td>I love these ginger candy, tastes like ginger ...</td>\n",
       "      <td>b'love ginger candi tast like ginger noth ad n...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>459971</th>\n",
       "      <td>497428</td>\n",
       "      <td>B0015A2W32</td>\n",
       "      <td>A1SEHFQQ30AR0E</td>\n",
       "      <td>jmble</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>positive</td>\n",
       "      <td>1351209600</td>\n",
       "      <td>Perfect</td>\n",
       "      <td>I was a little worried when I saw the reviews ...</td>\n",
       "      <td>b'littl worri saw review arriv melti enough go...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>109587</th>\n",
       "      <td>118929</td>\n",
       "      <td>B000G1CG50</td>\n",
       "      <td>A3VBVJ6UG09MFC</td>\n",
       "      <td>Bignose30</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>positive</td>\n",
       "      <td>1351209600</td>\n",
       "      <td>Yummy Chummies</td>\n",
       "      <td>All my dogs love them. Healthy treats. Not gre...</td>\n",
       "      <td>b'dog love healthi treat great train crumbl tr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>108513</th>\n",
       "      <td>117779</td>\n",
       "      <td>B003VN97GQ</td>\n",
       "      <td>A20DC4NK8YMOQT</td>\n",
       "      <td>jan e. \"jan e.\"</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>positive</td>\n",
       "      <td>1351209600</td>\n",
       "      <td>One of the Best Dressings Around</td>\n",
       "      <td>As stated above, a great salad dressing at a g...</td>\n",
       "      <td>b'state great salad dress great price especi p...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>271063</th>\n",
       "      <td>293824</td>\n",
       "      <td>B0040B5K28</td>\n",
       "      <td>A22CAEM146DDH5</td>\n",
       "      <td>Mary Vanecek \"Proud  Mary\"</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>positive</td>\n",
       "      <td>1351209600</td>\n",
       "      <td>Good food</td>\n",
       "      <td>The only dry food my queen cat will eat. Helps...</td>\n",
       "      <td>b'dri food queen cat eat help prevent hair bal...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>156284</th>\n",
       "      <td>169487</td>\n",
       "      <td>B000NM1BHQ</td>\n",
       "      <td>A2JDXKFZ0PFHKU</td>\n",
       "      <td>James W. Shondel</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>negative</td>\n",
       "      <td>1351209600</td>\n",
       "      <td>Received broken</td>\n",
       "      <td>I bought these to use as decorative center pie...</td>\n",
       "      <td>b'bought use decor center piec birthday parti ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16026</th>\n",
       "      <td>17512</td>\n",
       "      <td>B0045Z6K50</td>\n",
       "      <td>A3HM6TNYB7FNDL</td>\n",
       "      <td>C. Furman</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>positive</td>\n",
       "      <td>1351209600</td>\n",
       "      <td>Full- bodied without a bitter after-taste</td>\n",
       "      <td>This is my everyday coffee choice...a good all...</td>\n",
       "      <td>b'everyday coffe choic good around crowd pleas...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>393073</th>\n",
       "      <td>425059</td>\n",
       "      <td>B00317HLQA</td>\n",
       "      <td>A3AOK34N9VZ7HY</td>\n",
       "      <td>college student mom</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>positive</td>\n",
       "      <td>1351209600</td>\n",
       "      <td>special k fruit krisps. Blueberry are great</td>\n",
       "      <td>&lt;a href=\"http://www.amazon.com/gp/product/B003...</td>\n",
       "      <td>b'special fruit crisp blueberri bar pack purch...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>469180</th>\n",
       "      <td>507333</td>\n",
       "      <td>B000EEDJGO</td>\n",
       "      <td>ANH6SKT74AFPL</td>\n",
       "      <td>Dandy</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>negative</td>\n",
       "      <td>1351209600</td>\n",
       "      <td>Ain't what they used to be..</td>\n",
       "      <td>Big let down.  I loved these as a kid- they ar...</td>\n",
       "      <td>b'big let love arent dri sort tasteless compar...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>236904</th>\n",
       "      <td>256994</td>\n",
       "      <td>B004ITWDKO</td>\n",
       "      <td>A3934PPUIWRZVX</td>\n",
       "      <td>Chellie H.</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>positive</td>\n",
       "      <td>1351209600</td>\n",
       "      <td>Much, much too expensive!!</td>\n",
       "      <td>I ordered these for my silky terrier and he ab...</td>\n",
       "      <td>b'order silki terrier absolut love howev see p...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>517176</th>\n",
       "      <td>559158</td>\n",
       "      <td>B004LLGBQG</td>\n",
       "      <td>A4IL0CLL27Q33</td>\n",
       "      <td>D. Brennan</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>negative</td>\n",
       "      <td>1351209600</td>\n",
       "      <td>Buyer beware</td>\n",
       "      <td>Nespresso makes GREAT coffee and GREAT machine...</td>\n",
       "      <td>b'nespresso make great coffe great machin nesp...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>404311</th>\n",
       "      <td>437200</td>\n",
       "      <td>B009NIF7BM</td>\n",
       "      <td>AG4JL2SKX22W5</td>\n",
       "      <td>A. Ramos</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>positive</td>\n",
       "      <td>1351209600</td>\n",
       "      <td>AMAZING FIND!!!</td>\n",
       "      <td>I am a stay at home mom of two small kids and ...</td>\n",
       "      <td>b'stay home mom two small kid alway look good ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>206620</th>\n",
       "      <td>223927</td>\n",
       "      <td>B000EMAZK4</td>\n",
       "      <td>A20DY4XPIV6YKR</td>\n",
       "      <td>Traci Weaver</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>positive</td>\n",
       "      <td>1351209600</td>\n",
       "      <td>SO GOOD!!!</td>\n",
       "      <td>I love this French Vanilla tea and it's not av...</td>\n",
       "      <td>b'love french vanilla tea avail store longer t...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10000 rows × 11 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            Id   ProductId          UserId  \\\n",
       "138691  150509  0006641040  A3CMRKGE0P909G   \n",
       "325019  351770  B0000DG4B3  A1IU7S4HCK1XK0   \n",
       "422107  456525  B001D6C13O  A281NPSIMI1C2R   \n",
       "316306  342462  B000084F3O  A3DWUM6SN3N3NR   \n",
       "389412  421062  B0000E227M   A55MRYPUAX4QU   \n",
       "125303  135926  B0001217A4  A20Q6AL0RPC1US   \n",
       "222671  241462  B0001E5CJO  A239VY115ZCDFU   \n",
       "413420  447119  B0002PCET8  A2TWEIJVSW1G61   \n",
       "422255  456689  B00068K7UE  A1W9KQRCZ9ORHB   \n",
       "457717  494909  B0002ML9U6   AQ8DU6XVA3USJ   \n",
       "192387  208609  B0000WATVI   APRYPBTSZGT61   \n",
       "333926  361313  B00005IX96  A2TCI780LLLGT2   \n",
       "288677  312685  B0001R04LM  A2XRN476BGNLHX   \n",
       "499872  540455  B0000UBTZA  A2LMGWXI2UCA17   \n",
       "16085    17572  B0000GH6UQ  A2Q2IWWX5MLU9C   \n",
       "108404  117662  B0004MTZQE  A32Z0PMCKUBKJQ   \n",
       "119763  129865  B0002CRZRW  A1EAJX56LZKIAU   \n",
       "155416  168562  B0001ES9F8  A1R9HPMUY9DGVE   \n",
       "348264  376710  B00014E3LM   AFQ3LSZ22S8TP   \n",
       "399616  432077  B0052OUAOK  A2MNUZHMGVN5G2   \n",
       "399603  432064  B0052OUAOK  A2MNUZHMGVN5G2   \n",
       "471469  509847  B00023T3C6  A35XJ3GRQ9AZX1   \n",
       "327668  354650  B0000E5JQK   AIJY3RG7PU6XC   \n",
       "102032  110824  B0019421NA  A25YR04TDJA8SP   \n",
       "155514  168665  B0001ES9F8  A1TKZYH3PPTN9J   \n",
       "410085  443518  B000634FVG   AX3O3ANDPKDIF   \n",
       "523288  565711  B0002BG8QC  A32K9W2DFEN65X   \n",
       "197329  213879  B0000D9MXL  A1S1J26I7MDQ6Z   \n",
       "155515  168666  B0001ES9F8   AIK391A7XIK3W   \n",
       "428355  463238  B000GZSCKO  A1J64SRMB9XJH3   \n",
       "...        ...         ...             ...   \n",
       "488853  528608  B002L80MZ2  A3NTE6U617M3RH   \n",
       "379806  410667  B002QM4O3E   AP520XQ9GV1AJ   \n",
       "36633    39813  B000E7QYNG  A1W4MGP0ARF714   \n",
       "229729  249091  B00785V2G6   ABWCUS3HBDZRS   \n",
       "188285  204217  B001SATTGM  A3780M8J99RDJM   \n",
       "517873  559891  B008YA1P9I  A1FP5ZLSKR07SU   \n",
       "289603  313689  B005OVPK9G  A30JYPESIBJ8DZ   \n",
       "134672  146179  B003AP2GKY  A2RSB6FVQ9K9OD   \n",
       "20122    21923  B000KV61FC  A281J4UAH70YBA   \n",
       "418992  453092  B0029XLH4Y  A2CSPTFY7QPT78   \n",
       "507044  548295  B000FPJ78G  A3F14GVSJMJ0WH   \n",
       "58809    63886  B004SNMAOO  A3HVA6BTVB1UYP   \n",
       "396219  428428  B000TZ8WEC   A8NIRP9EJMP7J   \n",
       "179145  194297  B000VDL7RG  A2QGTXLC93BRV9   \n",
       "404593  437504  B003C15EUK  A32QQ3P51L0LO4   \n",
       "486800  526403  B001E5E1KA   AZPT0S609H54D   \n",
       "187833  203739  B0002DGWO8  A2EADJSLFFT39G   \n",
       "457924  495127  B001BOE3UW  A18J98YVUMMDOA   \n",
       "459971  497428  B0015A2W32  A1SEHFQQ30AR0E   \n",
       "109587  118929  B000G1CG50  A3VBVJ6UG09MFC   \n",
       "108513  117779  B003VN97GQ  A20DC4NK8YMOQT   \n",
       "271063  293824  B0040B5K28  A22CAEM146DDH5   \n",
       "156284  169487  B000NM1BHQ  A2JDXKFZ0PFHKU   \n",
       "16026    17512  B0045Z6K50  A3HM6TNYB7FNDL   \n",
       "393073  425059  B00317HLQA  A3AOK34N9VZ7HY   \n",
       "469180  507333  B000EEDJGO   ANH6SKT74AFPL   \n",
       "236904  256994  B004ITWDKO  A3934PPUIWRZVX   \n",
       "517176  559158  B004LLGBQG   A4IL0CLL27Q33   \n",
       "404311  437200  B009NIF7BM   AG4JL2SKX22W5   \n",
       "206620  223927  B000EMAZK4  A20DY4XPIV6YKR   \n",
       "\n",
       "                                           ProfileName  HelpfulnessNumerator  \\\n",
       "138691                                          Teresa                     3   \n",
       "325019                                  Joanna Daneman                    13   \n",
       "422107          Rebecca of Amazon \"The Rebecca Review\"                     2   \n",
       "316306  Author Brian Wallace (Mind Transmission, Inc.)                     4   \n",
       "389412                                     Avid Reader                     3   \n",
       "125303                                        J. Giles                     0   \n",
       "222671                           S. Johnston \"scott13\"                     4   \n",
       "413420                                     A. Rehnblom                     8   \n",
       "422255              Stuart Gardner \"www.sdgardner.com\"                     4   \n",
       "457717         Alejandra Vernon \"artist & illustrator\"                    15   \n",
       "192387                                    Lynn Babylon                     6   \n",
       "333926                                     aussie 2003                     6   \n",
       "288677                          Robin Boltz \"Robin RN\"                     5   \n",
       "499872                                        HSU Girl                     2   \n",
       "16085                         T. Butterfield \"lab rat\"                     8   \n",
       "108404                         Guapo Poppo \"Sr. Poppo\"                     2   \n",
       "119763                                     Wiver \"JCO\"                     5   \n",
       "155416                                      B. Alvarez                     0   \n",
       "348264                                   Tickleberries                     4   \n",
       "399616                                             ARA                     1   \n",
       "399603                                             ARA                     3   \n",
       "471469                                           L M S                     2   \n",
       "327668           Beverly L. Mitchell \"teachercreature\"                     0   \n",
       "102032                                       NuncVideo                     4   \n",
       "155514                                    Buyer Beware                     1   \n",
       "410085               blackholesun1Girl \"blackholesun1\"                     1   \n",
       "523288                          KMD \"pleased customer\"                     4   \n",
       "197329                                 Robert D. Clark                     5   \n",
       "155515                         Grace Sardes \"busy mom\"                     3   \n",
       "428355            Steven S. Guzman \"Professional Geek\"                     0   \n",
       "...                                                ...                   ...   \n",
       "488853                                 Salome \"Salome\"                     0   \n",
       "379806                                          Bernie                     0   \n",
       "36633                                           Tlilms                     0   \n",
       "229729                                        Rachel R                     0   \n",
       "188285                                    socialfeline                     0   \n",
       "517873                                      G. Burnick                     0   \n",
       "289603                                         TM Goss                     0   \n",
       "134672                                             Doc                     0   \n",
       "20122                                           Sophie                     0   \n",
       "418992                                 Mary G. Hershey                     0   \n",
       "507044                               Theresa A. Miller                     0   \n",
       "58809                   Christina A. Salemi \"Stinarat\"                     0   \n",
       "396219                                   Marie DeFalco                     0   \n",
       "179145                                        Steven S                     0   \n",
       "404593                                  Someone \"Kate\"                     0   \n",
       "486800                         Bob  Berman \"BirderBob\"                     0   \n",
       "187833                                      M. Jackson                     0   \n",
       "457924                                       andromeda                     0   \n",
       "459971                                           jmble                     0   \n",
       "109587                                       Bignose30                     0   \n",
       "108513                                 jan e. \"jan e.\"                     0   \n",
       "271063                      Mary Vanecek \"Proud  Mary\"                     0   \n",
       "156284                                James W. Shondel                     0   \n",
       "16026                                        C. Furman                     0   \n",
       "393073                             college student mom                     0   \n",
       "469180                                           Dandy                     0   \n",
       "236904                                      Chellie H.                     0   \n",
       "517176                                      D. Brennan                     0   \n",
       "404311                                        A. Ramos                     0   \n",
       "206620                                    Traci Weaver                     0   \n",
       "\n",
       "        HelpfulnessDenominator     Score        Time  \\\n",
       "138691                       4  positive  1018396800   \n",
       "325019                      15  positive  1072656000   \n",
       "422107                       2  positive  1074988800   \n",
       "316306                      33  negative  1076457600   \n",
       "389412                       3  positive  1076803200   \n",
       "125303                       3  positive  1080864000   \n",
       "222671                       5  positive  1090368000   \n",
       "413420                       8  positive  1095206400   \n",
       "422255                       5  positive  1097712000   \n",
       "457717                      17  positive  1098057600   \n",
       "192387                       7  positive  1100649600   \n",
       "333926                       7  positive  1101081600   \n",
       "288677                      15  positive  1103587200   \n",
       "499872                       2  positive  1103673600   \n",
       "16085                        9  positive  1104451200   \n",
       "108404                       2  positive  1106092800   \n",
       "119763                       5  positive  1110412800   \n",
       "155416                       0  positive  1114905600   \n",
       "348264                       7  positive  1116201600   \n",
       "399616                       1  positive  1117238400   \n",
       "399603                       3  positive  1117238400   \n",
       "471469                       2  positive  1117411200   \n",
       "327668                       0  positive  1117411200   \n",
       "102032                       4  positive  1120608000   \n",
       "155514                       1  positive  1120780800   \n",
       "410085                       1  positive  1123459200   \n",
       "523288                       4  positive  1123891200   \n",
       "197329                       5  positive  1125878400   \n",
       "155515                       4  positive  1126224000   \n",
       "428355                       0  positive  1126742400   \n",
       "...                        ...       ...         ...   \n",
       "488853                       0  positive  1351036800   \n",
       "379806                       0  positive  1351036800   \n",
       "36633                        0  positive  1351036800   \n",
       "229729                       0  positive  1351123200   \n",
       "188285                       0  positive  1351123200   \n",
       "517873                       0  positive  1351123200   \n",
       "289603                       0  positive  1351123200   \n",
       "134672                       0  positive  1351123200   \n",
       "20122                        0  positive  1351123200   \n",
       "418992                       0  positive  1351123200   \n",
       "507044                       0  negative  1351123200   \n",
       "58809                        0  positive  1351123200   \n",
       "396219                       0  positive  1351123200   \n",
       "179145                       0  positive  1351123200   \n",
       "404593                       0  positive  1351123200   \n",
       "486800                       0  negative  1351123200   \n",
       "187833                       0  positive  1351123200   \n",
       "457924                       0  positive  1351209600   \n",
       "459971                       0  positive  1351209600   \n",
       "109587                       0  positive  1351209600   \n",
       "108513                       0  positive  1351209600   \n",
       "271063                       0  positive  1351209600   \n",
       "156284                       0  negative  1351209600   \n",
       "16026                        0  positive  1351209600   \n",
       "393073                       0  positive  1351209600   \n",
       "469180                       0  negative  1351209600   \n",
       "236904                       0  positive  1351209600   \n",
       "517176                       0  negative  1351209600   \n",
       "404311                       0  positive  1351209600   \n",
       "206620                       0  positive  1351209600   \n",
       "\n",
       "                                                  Summary  \\\n",
       "138691                    A great way to learn the months   \n",
       "325019  If they must have mac and cheese like the box ...   \n",
       "422107       Organic Darjeeling best with Organic Sucanat   \n",
       "316306                            the most unnatural odor   \n",
       "389412                                   Musky, exquisite   \n",
       "125303                          Yo, Cherry Limeade Recipe   \n",
       "222671                       you should check this out...   \n",
       "413420                                              yummm   \n",
       "422255                    Excellent - Fish Breath No More   \n",
       "457717                   for fretting felines and canines   \n",
       "192387                                          Very Good   \n",
       "333926                            Loves convenience in GA   \n",
       "288677                    Fun for Grown Ups old and young   \n",
       "499872                          This is the best Chai tea   \n",
       "16085                                            The Best   \n",
       "108404                                        Caliente...   \n",
       "119763                 This tea has a fresh and nice look   \n",
       "155416         Finally some flavored coffee for my senseo   \n",
       "348264                                       Enjoyed them   \n",
       "399616                                        Excellent!!   \n",
       "399603               GREENS + Energy Bars are FABULOUS!!!   \n",
       "471469                        Mae Ploy Sweet Chilli Sauce   \n",
       "327668                                        great stuff   \n",
       "102032                            Our Toddler Loves These   \n",
       "155514                         More options for my Senseo   \n",
       "410085              My dog is the one that I listen to...   \n",
       "523288                       quick service---good quality   \n",
       "197329                                  A cheesey comment   \n",
       "155515        Coffee house coffee right in your own home!   \n",
       "428355            WOW!  As good as hamburger, but BETTER!   \n",
       "...                                                   ...   \n",
       "488853                    Good Product, Costs too Much...   \n",
       "379806                               A winner at our home   \n",
       "36633                                        Alot of seed   \n",
       "229729                                  Excellent product   \n",
       "188285                                        Finally!!!!   \n",
       "517873                                A longtime favorite   \n",
       "289603                              Wait...what?! Bland?!   \n",
       "134672                                        Bulk k-Cups   \n",
       "20122                    Fantastic for energetic big pups   \n",
       "418992        Great bold taste-- compare to Emeril's Bold   \n",
       "507044                                             dingos   \n",
       "58809                                       love this bar   \n",
       "396219                    Bags on Board waste pickup bags   \n",
       "179145                 Good but not as good as I remember   \n",
       "404593                                      Always great!   \n",
       "486800                             Has trouble dissolving   \n",
       "187833                                         Consistent   \n",
       "457924                          You can taste the ginger.   \n",
       "459971                                            Perfect   \n",
       "109587                                     Yummy Chummies   \n",
       "108513                   One of the Best Dressings Around   \n",
       "271063                                          Good food   \n",
       "156284                                    Received broken   \n",
       "16026           Full- bodied without a bitter after-taste   \n",
       "393073        special k fruit krisps. Blueberry are great   \n",
       "469180                       Ain't what they used to be..   \n",
       "236904                         Much, much too expensive!!   \n",
       "517176                                       Buyer beware   \n",
       "404311                                    AMAZING FIND!!!   \n",
       "206620                                         SO GOOD!!!   \n",
       "\n",
       "                                                     Text  \\\n",
       "138691  This is a book of poetry about the months of t...   \n",
       "325019  This is the powdered dried cheese like in &quo...   \n",
       "422107  \"Sitting on the porch of a bungalow on a tea p...   \n",
       "316306  I really hate to do this (having been a fan of...   \n",
       "389412  The quality of this coffee is the first thing ...   \n",
       "125303  Yes, summer's coming. Rather than drinking tea...   \n",
       "222671  Despite the silly premise and acting, i can wa...   \n",
       "413420  This stuff tastes just like Sweettarts.  I rec...   \n",
       "422255  If your cat has \"foul\" breath after a chicken ...   \n",
       "457717  This 100% natural product for cats and dogs go...   \n",
       "192387  Rich pumpkin flavor. Can be made with skim mil...   \n",
       "333926  These pods have got to be the best invention y...   \n",
       "288677  I'm taking this to treat my grad school classm...   \n",
       "499872  My boyfriend and I both love chai tea, and aft...   \n",
       "16085   It is the best hot chocolate I have ever taste...   \n",
       "108404  ...and mustardy, I like mine on French bread w...   \n",
       "119763  This tea tastes great, and it gives a nicely c...   \n",
       "155416  The hazelnut flavor is a little on the light s...   \n",
       "348264  The sugar-free was very good. I've had the reg...   \n",
       "399616  These are some of the very best tasting energy...   \n",
       "399603  These are some of the very best tasting energy...   \n",
       "471469  This is the best.  It tastes just like the sau...   \n",
       "327668  these things are really great....they are hot,...   \n",
       "102032  I'm no \"health nut\", but my wife and I both fi...   \n",
       "155514  I love my Senseo, and I am thrilled to have fl...   \n",
       "410085  My dog Annie had surgery about 2 months ago an...   \n",
       "523288  The basket arrived to the recipient in a lot l...   \n",
       "197329  Excellent taste.  All who partook said it was ...   \n",
       "155515  These coffee pods are wonderful. The hazelnut ...   \n",
       "428355  How could a box of dried stuff come out sooooo...   \n",
       "...                                                   ...   \n",
       "488853  I look for good stuff for our Great Pyrenees p...   \n",
       "379806  My vet recommended Dentahex Chews for our two ...   \n",
       "36633   This is very good seed and alot of it . I use ...   \n",
       "229729  After scouring every store in town for orange ...   \n",
       "188285  I am so glad that I have finally been able to ...   \n",
       "517873  Lake and Lodge has been a favorite non-flavore...   \n",
       "289603  To everyone giving less stars on here for it b...   \n",
       "134672  This is the best way to buy coffee for my offi...   \n",
       "20122   My 70 lb Shepard/boxer mix absolutely loves he...   \n",
       "418992  I've been drinking Emeril's Bold for a year an...   \n",
       "507044  The red part of the dingos was missing from 3 ...   \n",
       "58809   Sick of melting chocolate or yogurt icing in y...   \n",
       "396219  These are decent bags with an easy tear off st...   \n",
       "179145  I could be wrong but it seems like these used ...   \n",
       "404593  Got these for my sister who has Celiac disease...   \n",
       "486800  I tried this product to be used in my cold ice...   \n",
       "187833  These chews are a consistent size.  Some of th...   \n",
       "457924  I love these ginger candy, tastes like ginger ...   \n",
       "459971  I was a little worried when I saw the reviews ...   \n",
       "109587  All my dogs love them. Healthy treats. Not gre...   \n",
       "108513  As stated above, a great salad dressing at a g...   \n",
       "271063  The only dry food my queen cat will eat. Helps...   \n",
       "156284  I bought these to use as decorative center pie...   \n",
       "16026   This is my everyday coffee choice...a good all...   \n",
       "393073  <a href=\"http://www.amazon.com/gp/product/B003...   \n",
       "469180  Big let down.  I loved these as a kid- they ar...   \n",
       "236904  I ordered these for my silky terrier and he ab...   \n",
       "517176  Nespresso makes GREAT coffee and GREAT machine...   \n",
       "404311  I am a stay at home mom of two small kids and ...   \n",
       "206620  I love this French Vanilla tea and it's not av...   \n",
       "\n",
       "                                              CleanedText  \n",
       "138691  b'book poetri month year goe month cute littl ...  \n",
       "325019  b'powder dri chees like canadian friend call r...  \n",
       "422107  b'sit porch bungalow tea plantat darjeel see p...  \n",
       "316306  b'realli hate fan van patten year feel must sa...  \n",
       "389412  b'qualiti coffe first thing one notic goe with...  \n",
       "125303  b'yes summer come rather drink tea sweet break...  \n",
       "222671  b'despit silli premis act watch fim quit sure ...  \n",
       "413420  b'stuff tast like sweettart recommend anyon ge...  \n",
       "422255  b'cat foul breath chicken dinner even fish rea...  \n",
       "457717  b'natur product cat dog go stress time includ ...  \n",
       "192387  b'rich pumpkin flavor made skim milk low fat d...  \n",
       "333926  b'pod got best invent yet compact extrem easi ...  \n",
       "288677  b'take treat grad school classmat dont think a...  \n",
       "499872  b'boyfriend love chai tea tri sever brand flav...  \n",
       "16085     b'best hot chocol ever tast cinnamon make good'  \n",
       "108404  b'mustardi like mine french bread big slice pr...  \n",
       "119763  b'tea tast great give nice conceiv fresh look ...  \n",
       "155416  b'hazelnut flavor littl light side that way li...  \n",
       "348264  b'good ive regular realli good especi diabet w...  \n",
       "399616  b'best tast energi bar ive ever best ingredi i...  \n",
       "399603  b'best tast energi bar ive ever best ingredi i...  \n",
       "471469      b'best tast like sauc come crab wonton chang'  \n",
       "327668      b'thing realli great hot good read make want'  \n",
       "102032  b'health nut wife figur toddler doesnt need co...  \n",
       "155514  b'love senseo thrill flavor option person much...  \n",
       "410085  b'dog anni surgeri month ago ever sinc hard ge...  \n",
       "523288  b'basket arriv recipi lot less time nice surpr...  \n",
       "197329  b'excel tast partook said much better four far...  \n",
       "155515  b'coffe pod wonder hazelnut flavor subtl delic...  \n",
       "428355  b'could box dri stuff come sooooo good dont kn...  \n",
       "...                                                   ...  \n",
       "488853  b'look good stuff great pyrene pup tri product...  \n",
       "379806  b'vet recommend dentahex chew two poodl three ...  \n",
       "36633   b'good seed alot use itfor grow wheatgrass jui...  \n",
       "229729  b'scour everi store town orang peel find anyth...  \n",
       "188285  b'glad final abl get black bean famili love ca...  \n",
       "517873  b'lake lodg favorit sinc start use keurig mach...  \n",
       "289603  b'everyon give less star bland brain attempt s...  \n",
       "134672  b'best way buy coffe offic least expens way bu...  \n",
       "20122   b'shepard boxer mix absolut love take mintu fi...  \n",
       "418992  b'ive drink emeril bold year half want tri som...  \n",
       "507044  b'red part dingo miss ball one bag two other o...  \n",
       "58809   b'sick melt chocol yogurt ice bar well good on...  \n",
       "396219  b'decent bag easi tear strip hold content well...  \n",
       "179145  b'could wrong seem like use better read review...  \n",
       "404593  b'got sister celiac diseas need one candi bar ...  \n",
       "486800  b'tri product use cold ice tea limeaid also us...  \n",
       "187833  b'chew consist size bag purchas big retail che...  \n",
       "457924  b'love ginger candi tast like ginger noth ad n...  \n",
       "459971  b'littl worri saw review arriv melti enough go...  \n",
       "109587  b'dog love healthi treat great train crumbl tr...  \n",
       "108513  b'state great salad dress great price especi p...  \n",
       "271063  b'dri food queen cat eat help prevent hair bal...  \n",
       "156284  b'bought use decor center piec birthday parti ...  \n",
       "16026   b'everyday coffe choic good around crowd pleas...  \n",
       "393073  b'special fruit crisp blueberri bar pack purch...  \n",
       "469180  b'big let love arent dri sort tasteless compar...  \n",
       "236904  b'order silki terrier absolut love howev see p...  \n",
       "517176  b'nespresso make great coffe great machin nesp...  \n",
       "404311  b'stay home mom two small kid alway look good ...  \n",
       "206620  b'love french vanilla tea avail store longer t...  \n",
       "\n",
       "[10000 rows x 11 columns]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#BoW\n",
    "count_vect = CountVectorizer() \n",
    "final_counts = count_vect.fit_transform(random_final['CleanedText'].values)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "scipy.sparse.csr.csr_matrix"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(final_counts) # sparse matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000, 12925)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_counts.get_shape() # shape of matrix after Vectorizing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/shebu/anaconda3/lib/python3.6/site-packages/sklearn/utils/validation.py:475: DataConversionWarning: Data with input dtype int64 was converted to float64 by StandardScaler.\n",
      "  warnings.warn(msg, DataConversionWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10000, 12925)\n"
     ]
    }
   ],
   "source": [
    "# Standardizing the data with mean = 0 and std.dev = 1\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "standardized_data = StandardScaler(with_mean=False).fit_transform(final_counts)\n",
    "print(standardized_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = standardized_data.toarray() # converting the standardized data to dense array\n",
    "y = np.array(random_final['Score']) # converting Review attribute to a numpy array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split the data set into train and test 70% train and 30% test \n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X1, X_test, y1, y_test = train_test_split(X, y, test_size=0.3, shuffle=False, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=TimeSeriesSplit(max_train_size=None, n_splits=10),\n",
       "       error_score='raise',\n",
       "       estimator=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
       "          penalty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
       "          verbose=0, warm_start=False),\n",
       "       fit_params=None, iid=True, n_jobs=1,\n",
       "       param_grid=[{'C': [1e-08, 1e-06, 0.0001, 0.01, 1, 100, 10000, 1000000, 100000000]}],\n",
       "       pre_dispatch='2*n_jobs', refit=True, return_train_score='warn',\n",
       "       scoring=None, verbose=0)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import cross_val_predict\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "                               \n",
    "tss = TimeSeriesSplit(n_splits=10) # TimeSeries Split with number of splits=10\n",
    "\n",
    "parameters = [{'C': [10**-8, 10**-6, 10**-4, 10**-2, 10**0, 10**2, 10**4, 10**6, 10**8]}]# specifying the range of the hyperparameter lambda(1/C) for GridSearch\n",
    "\n",
    "Lr = LogisticRegression()\n",
    "\n",
    "# GridSearch for finding right hyperparameter with with 10 fold CV on TimeSeriesSplit\n",
    "model_BOW = GridSearchCV(Lr, parameters, cv=tss, refit=True)\n",
    "\n",
    "# Fit the training data\n",
    "model_BOW.fit(X1, y1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'C': 0.01}"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_BOW.best_params_  # print the best hyperparameter C"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Accuracy =  0.8883333333333333\n",
      "Confusion Matrix\n",
      " [[ 209  267]\n",
      " [  68 2456]]\n",
      "(tn, fn, fp, tp) = (209, 267, 68, 2456)\n",
      "Recall =  0.9730586370839936\n",
      "f1-Score =  0.9361539927577663\n",
      "Precision =  0.901946382666177\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import recall_score, f1_score, confusion_matrix, accuracy_score, precision_score\n",
    "\n",
    "\n",
    "# instantiate learning model for best C\n",
    "Lr_op = LogisticRegression(penalty='l2', C=0.01)\n",
    "\n",
    "# fitting the model\n",
    "Lr_op.fit(X1, y1)\n",
    "\n",
    "# predict the response\n",
    "pred_Lr_op = Lr_op.predict(X_test)\n",
    "\n",
    "\n",
    "print()\n",
    "\n",
    "# evaluate various performance measures:\n",
    "print(\"Accuracy = \",accuracy_score(y_test, pred_Lr_op)) # accuracy\n",
    "print(\"Confusion Matrix\\n\", confusion_matrix(y_test, pred_Lr_op)) # Confusion Matrix\n",
    "tn, fn, fp, tp = confusion_matrix(y_test, pred_Lr_op).ravel() # tn, fn, fp, tp\n",
    "print(\"(tn, fn, fp, tp) =\",(tn, fn, fp, tp))\n",
    "print(\"Recall = \",recall_score(y_test, pred_Lr_op, pos_label='positive')) # Recall\n",
    "print(\"f1-Score = \"  ,f1_score(y_test, pred_Lr_op, pos_label='positive')) # f1-Score\n",
    "print(\"Precision = \",precision_score(y_test, pred_Lr_op, pos_label='positive'))# Precision\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.01712988,  0.00451658,  0.00214061, ...,  0.00021602,\n",
       "         0.        ,  0.00586088]])"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Lr_op.coef_ # weights of eatures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = np.empty(0)\n",
    "a = Lr_op.coef_ # storing in a numpy array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_m = a + np.random.normal(0, 0.002)  # adding a small noise on weights and testing whether they are collinear or not"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-1.70719825e-02,  4.57447204e-03,  2.19850284e-03, ...,\n",
       "         2.73908585e-04,  5.78927724e-05,  5.91877651e-03]])"
      ]
     },
     "execution_count": 151,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_m # array after adding noise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "# As we can see the difference is very very low in terms of weight matrix and updated weight matrix after adding a noise.\n",
    "# They are not collinear."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 20 Positive Features for BOW and Top 20 Negative Features for BOW \n",
      "\n",
      "\n",
      "\n",
      "\tgreat          \t-0.2171\tpositive       \t\tdisappoint     \t0.3940\tnegative       \n",
      "\tlove           \t-0.1693\tpositive       \t\taw             \t0.3477\tnegative       \n",
      "\tbest           \t-0.1669\tpositive       \t\tstale          \t0.2998\tnegative       \n",
      "\tgood           \t-0.1463\tpositive       \t\treturn         \t0.2298\tnegative       \n",
      "\texcel          \t-0.1444\tpositive       \t\tworst          \t0.2115\tnegative       \n",
      "\tdelici         \t-0.1391\tpositive       \t\tbland          \t0.2087\tnegative       \n",
      "\tnice           \t-0.1361\tpositive       \t\tdidnt          \t0.1947\tnegative       \n",
      "\tfind           \t-0.1335\tpositive       \t\thorribl        \t0.1870\tnegative       \n",
      "\tenjoy          \t-0.1323\tpositive       \t\twont           \t0.1726\tnegative       \n",
      "\teasi           \t-0.1250\tpositive       \t\twast           \t0.1696\tnegative       \n",
      "\tsnack          \t-0.1244\tpositive       \t\tbare           \t0.1596\tnegative       \n",
      "\tperfect        \t-0.1240\tpositive       \t\tbad            \t0.1538\tnegative       \n",
      "\tprice          \t-0.1221\tpositive       \t\trefund         \t0.1532\tnegative       \n",
      "\tquick          \t-0.1194\tpositive       \t\trip            \t0.1486\tnegative       \n",
      "\tfavorit        \t-0.1112\tpositive       \t\tmediocr        \t0.1421\tnegative       \n",
      "\tuse            \t-0.1103\tpositive       \t\tterribl        \t0.1420\tnegative       \n",
      "\twonder         \t-0.1101\tpositive       \t\tmoney          \t0.1375\tnegative       \n",
      "\tright          \t-0.1079\tpositive       \t\tdisgust        \t0.1354\tnegative       \n",
      "\tfresh          \t-0.1009\tpositive       \t\tartifici       \t0.1352\tnegative       \n",
      "\ttasti          \t-0.0989\tpositive       \t\tmayb           \t0.1325\tnegative       \n"
     ]
    }
   ],
   "source": [
    "# print top 20 features for each class for BOW\n",
    "print(\"Top 20 Positive Features for BOW and Top 20 Negative Features for BOW \")\n",
    "print(\"\\n\\n\")\n",
    "\n",
    "# defining a function for finding the top features of each class\n",
    "def top_most_features(vectorizer, clf, n=20):\n",
    "    feature_names = vectorizer.get_feature_names() # getting all the feature names\n",
    "    coefs_with_fns = sorted(zip(clf.coef_[0], feature_names)) # retrieve the coefficient & sort them based on values \n",
    "    top = zip(coefs_with_fns[:n], coefs_with_fns[:-(n + 1):-1]) \n",
    "    for (coef_1, fn_1), (coef_2, fn_2) in top:\n",
    "        print (\"\\t%-15s\\t%.4f\\t%-15s\\t\\t%-15s\\t%.4f\\t%-15s\" % (fn_2, coef_1, 'positive', fn_1, coef_2, 'negative'))\n",
    "\n",
    "        \n",
    "top_most_features(count_vect, Lr_op)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sparsity:  115\n",
      "\n",
      "Accuracy =  0.861\n",
      "Confusion Matrix\n",
      " [[  78  398]\n",
      " [  19 2505]]\n",
      "(tn, fn, fp, tp) = (78, 398, 19, 2505)\n",
      "Recall =  0.992472266244057\n",
      "f1-Score =  0.9231619679380872\n",
      "Precision =  0.8629004478126077\n"
     ]
    }
   ],
   "source": [
    "# instantiate learning model for best C\n",
    "Lr_op = LogisticRegression(penalty = 'l1', C=0.01)\n",
    "\n",
    "# fitting the model\n",
    "Lr_op.fit(X1, y1)\n",
    "\n",
    "# predict the response\n",
    "pred_Lr_op = Lr_op.predict(X_test)\n",
    "\n",
    "# Sparsity of array\n",
    "w1 = Lr_op.coef_\n",
    "print(\"Sparsity: \" ,np.count_nonzero(w1))\n",
    "\n",
    "\n",
    "print()\n",
    "\n",
    "# evaluate various performance measures:\n",
    "print(\"Accuracy = \",accuracy_score(y_test, pred_Lr_op)) # accuracy\n",
    "print(\"Confusion Matrix\\n\", confusion_matrix(y_test, pred_Lr_op)) # Confusion Matrix\n",
    "tn, fn, fp, tp = confusion_matrix(y_test, pred_Lr_op).ravel() # tn, fn, fp, tp\n",
    "print(\"(tn, fn, fp, tp) =\",(tn, fn, fp, tp))\n",
    "print(\"Recall = \",recall_score(y_test, pred_Lr_op, pos_label='positive')) # Recall\n",
    "print(\"f1-Score = \"  ,f1_score(y_test, pred_Lr_op, pos_label='positive')) # f1-Score\n",
    "print(\"Precision = \",precision_score(y_test, pred_Lr_op, pos_label='positive'))# Precision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sparsity:  55\n",
      "\n",
      "Precision =  0.8534599728629579\n"
     ]
    }
   ],
   "source": [
    "# instantiate learning model for best C\n",
    "Lr_op = LogisticRegression(penalty = 'l1', C=0.007)\n",
    "\n",
    "# fitting the model\n",
    "Lr_op.fit(X1, y1)\n",
    "\n",
    "# predict the response\n",
    "pred_Lr_op = Lr_op.predict(X_test)\n",
    "\n",
    "# Checking sparsity of array with reduced C\n",
    "w1 = Lr_op.coef_\n",
    "print(\"Sparsity: \" ,np.count_nonzero(w1))\n",
    "\n",
    "\n",
    "print()\n",
    "print(\"Precision = \",precision_score(y_test, pred_Lr_op, pos_label='positive'))# Precision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sparsity:  13\n",
      "\n",
      "Precision =  0.8434782608695652\n"
     ]
    }
   ],
   "source": [
    "# instantiate learning model for best alpha\n",
    "Lr_op = LogisticRegression(penalty = 'l1', C=0.004)\n",
    "\n",
    "# fitting the model\n",
    "Lr_op.fit(X1, y1)\n",
    "\n",
    "# predict the response\n",
    "pred_Lr_op = Lr_op.predict(X_test)\n",
    "\n",
    "# Checking sparsity of array with reduced C\n",
    "w1 = Lr_op.coef_\n",
    "print(\"Sparsity: \" ,np.count_nonzero(w1))\n",
    "\n",
    "\n",
    "print()\n",
    "print(\"Precision = \",precision_score(y_test, pred_Lr_op, pos_label='positive'))# Precision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sparsity:  2\n",
      "\n",
      "Precision =  0.8413333333333334\n"
     ]
    }
   ],
   "source": [
    "# instantiate learning model for best alpha\n",
    "Lr_op = LogisticRegression(penalty = 'l1', C=0.002)\n",
    "\n",
    "# fitting the model\n",
    "Lr_op.fit(X1, y1)\n",
    "\n",
    "# predict the response\n",
    "pred_Lr_op = Lr_op.predict(X_test)\n",
    "\n",
    "# Checking sparsity of array with reduced C\n",
    "w1 = Lr_op.coef_\n",
    "print(\"Sparsity: \" ,np.count_nonzero(w1))\n",
    "\n",
    "\n",
    "print()\n",
    "print(\"Precision = \",precision_score(y_test, pred_Lr_op, pos_label='positive'))# Precision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sparsity:  0\n",
      "\n",
      "Precision =  0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/shebu/anaconda3/lib/python3.6/site-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "# instantiate learning model for best alpha\n",
    "Lr_op = LogisticRegression(penalty = 'l1', C=0.0002)\n",
    "\n",
    "# fitting the model\n",
    "Lr_op.fit(X1, y1)\n",
    "\n",
    "# predict the response\n",
    "pred_Lr_op = Lr_op.predict(X_test)\n",
    "\n",
    "# Checking sparsity of array with reduced C\n",
    "w1 = Lr_op.coef_\n",
    "print(\"Sparsity: \" ,np.count_nonzero(w1))\n",
    "\n",
    "\n",
    "print()\n",
    "print(\"Precision = \",precision_score(y_test, pred_Lr_op, pos_label='positive'))# Precision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sparsity:  2176\n",
      "\n",
      "Precision =  0.9164430816404753\n"
     ]
    }
   ],
   "source": [
    "# instantiate learning model for best alpha\n",
    "Lr_op = LogisticRegression(penalty = 'l1', C=0.8)\n",
    "\n",
    "# fitting the model\n",
    "Lr_op.fit(X1, y1)\n",
    "\n",
    "# predict the response\n",
    "pred_Lr_op = Lr_op.predict(X_test)\n",
    "\n",
    "# Checking sparsity of array with increased C\n",
    "w1 = Lr_op.coef_\n",
    "print(\"Sparsity: \" ,np.count_nonzero(w1))\n",
    "\n",
    "\n",
    "print()\n",
    "print(\"Precision = \",precision_score(y_test, pred_Lr_op, pos_label='positive'))# Precision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomizedSearchCV(cv=TimeSeriesSplit(max_train_size=None, n_splits=10),\n",
       "          error_score='raise',\n",
       "          estimator=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
       "          penalty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
       "          verbose=0, warm_start=False),\n",
       "          fit_params=None, iid=True, n_iter=10, n_jobs=1,\n",
       "          param_distributions={'C': <scipy.stats._distn_infrastructure.rv_frozen object at 0x7f3535b17908>},\n",
       "          pre_dispatch='2*n_jobs', random_state=None, refit=True,\n",
       "          return_train_score='warn', scoring=None, verbose=0)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import cross_val_predict\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from scipy.stats import uniform as sp_rand\n",
    "                               \n",
    "tss_1 = TimeSeriesSplit(n_splits=10) # TimeSeries Split with number of splits=10\n",
    "\n",
    "parameters_1 = {'C': sp_rand() }# specifying the range of the hyperparameter C for RandomizedSearchCV\n",
    "\n",
    "Lr_ran = LogisticRegression()\n",
    "\n",
    "# RandomizedSearchCV for finding right hyperparameter with with 10 fold CV on TimeSeriesSplit\n",
    "model_BOW_ran = RandomizedSearchCV(Lr_ran, parameters_1, cv=tss_1, refit=True)\n",
    "\n",
    "# Fit the training data\n",
    "model_BOW_ran.fit(X1, y1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'C': 0.02875544258870244}"
      ]
     },
     "execution_count": 155,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_BOW_ran.best_params_ # best C or 1/lambda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Accuracy =  0.8843333333333333\n",
      "Confusion Matrix\n",
      " [[ 217  259]\n",
      " [  88 2436]]\n",
      "(tn, fn, fp, tp) = (217, 259, 88, 2436)\n",
      "Recall =  0.96513470681458\n",
      "f1-Score =  0.9335121670818164\n",
      "Precision =  0.9038961038961039\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import recall_score, f1_score, confusion_matrix, accuracy_score, precision_score\n",
    "\n",
    "\n",
    "# instantiate learning model for best C\n",
    "Lr_op = LogisticRegression(penalty='l2', C = 0.02875544258870244)\n",
    "\n",
    "# fitting the model\n",
    "Lr_op.fit(X1, y1)\n",
    "\n",
    "# predict the response\n",
    "pred_Lr_op = Lr_op.predict(X_test)\n",
    "\n",
    "print()\n",
    "\n",
    "# evaluate various performance measures:\n",
    "print(\"Accuracy = \",accuracy_score(y_test, pred_Lr_op)) # accuracy\n",
    "print(\"Confusion Matrix\\n\", confusion_matrix(y_test, pred_Lr_op)) # Confusion Matrix\n",
    "tn, fn, fp, tp = confusion_matrix(y_test, pred_Lr_op).ravel() # tn, fn, fp, tp\n",
    "print(\"(tn, fn, fp, tp) =\",(tn, fn, fp, tp))\n",
    "print(\"Recall = \",recall_score(y_test, pred_Lr_op, pos_label='positive')) # Recall\n",
    "print(\"f1-Score = \"  ,f1_score(y_test, pred_Lr_op, pos_label='positive')) # f1-Score\n",
    "print(\"Precision = \",precision_score(y_test, pred_Lr_op, pos_label='positive'))# Precision\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sparsity:  931\n",
      "\n",
      "Accuracy =  0.8846666666666667\n",
      "Confusion Matrix\n",
      " [[ 174  302]\n",
      " [  44 2480]]\n",
      "(tn, fn, fp, tp) = (174, 302, 44, 2480)\n",
      "Recall =  0.9825673534072901\n",
      "f1-Score =  0.9347908028646814\n",
      "Precision =  0.8914450035945363\n"
     ]
    }
   ],
   "source": [
    "# instantiate learning model for best C\n",
    "Lr_op = LogisticRegression(penalty='l1', C = 0.02875544258870244)\n",
    "\n",
    "\n",
    "# fitting the model\n",
    "Lr_op.fit(X1, y1)\n",
    "\n",
    "# predict the response\n",
    "pred_Lr_op = Lr_op.predict(X_test)\n",
    "\n",
    "# Checking sparsity of array with C\n",
    "w1 = Lr_op.coef_\n",
    "print(\"Sparsity: \" ,np.count_nonzero(w1))\n",
    "\n",
    "print()\n",
    "\n",
    "# evaluate various performance measures:\n",
    "print(\"Accuracy = \",accuracy_score(y_test, pred_Lr_op)) # accuracy\n",
    "print(\"Confusion Matrix\\n\", confusion_matrix(y_test, pred_Lr_op)) # Confusion Matrix\n",
    "tn, fn, fp, tp = confusion_matrix(y_test, pred_Lr_op).ravel() # tn, fn, fp, tp\n",
    "print(\"(tn, fn, fp, tp) =\",(tn, fn, fp, tp))\n",
    "print(\"Recall = \",recall_score(y_test, pred_Lr_op, pos_label='positive')) # Recall\n",
    "print(\"f1-Score = \"  ,f1_score(y_test, pred_Lr_op, pos_label='positive')) # f1-Score\n",
    "print(\"Precision = \",precision_score(y_test, pred_Lr_op, pos_label='positive'))# Precision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sparsity:  685\n",
      "\n",
      "Precision =  0.882937211777226\n"
     ]
    }
   ],
   "source": [
    "# instantiate learning model for best alpha\n",
    "Lr_op = LogisticRegression(penalty='l1', C = 0.020)\n",
    "\n",
    "\n",
    "# fitting the model\n",
    "Lr_op.fit(X1, y1)\n",
    "\n",
    "# predict the response\n",
    "pred_Lr_op = Lr_op.predict(X_test)\n",
    "\n",
    "# Checking sparsity of array with reduced C\n",
    "w1 = Lr_op.coef_\n",
    "print(\"Sparsity: \" ,np.count_nonzero(w1))\n",
    "\n",
    "print()\n",
    "\n",
    "print(\"Precision = \",precision_score(y_test, pred_Lr_op, pos_label='positive'))# Precision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sparsity:  462\n",
      "\n",
      "Precision =  0.8773717498243149\n"
     ]
    }
   ],
   "source": [
    "# instantiate learning model for best alpha\n",
    "Lr_op = LogisticRegression(penalty='l1', C = 0.015)\n",
    "\n",
    "\n",
    "# fitting the model\n",
    "Lr_op.fit(X1, y1)\n",
    "\n",
    "# predict the response\n",
    "pred_Lr_op = Lr_op.predict(X_test)\n",
    "\n",
    "# Checking sparsity of array with reduced C\n",
    "w1 = Lr_op.coef_\n",
    "print(\"Sparsity: \" ,np.count_nonzero(w1))\n",
    "\n",
    "print()\n",
    "\n",
    "\n",
    "print(\"Precision = \",precision_score(y_test, pred_Lr_op, pos_label='positive'))# Precision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sparsity:  115\n",
      "\n",
      "Precision =  0.8629004478126077\n"
     ]
    }
   ],
   "source": [
    "# instantiate learning model for best alpha\n",
    "Lr_op = LogisticRegression(penalty='l1', C = 0.010)\n",
    "\n",
    "\n",
    "# fitting the model\n",
    "Lr_op.fit(X1, y1)\n",
    "\n",
    "# predict the response\n",
    "pred_Lr_op = Lr_op.predict(X_test)\n",
    "\n",
    "# Checking sparsity of array with reduced C\n",
    "w1 = Lr_op.coef_\n",
    "print(\"Sparsity: \" ,np.count_nonzero(w1))\n",
    "\n",
    "print()\n",
    "\n",
    "print(\"Precision = \",precision_score(y_test, pred_Lr_op, pos_label='positive'))# Precision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sparsity:  55\n",
      "\n",
      "Precision =  0.8534599728629579\n"
     ]
    }
   ],
   "source": [
    "# instantiate learning model for best alpha\n",
    "Lr_op = LogisticRegression(penalty='l1', C = 0.007)\n",
    "\n",
    "\n",
    "# fitting the model\n",
    "Lr_op.fit(X1, y1)\n",
    "\n",
    "# predict the response\n",
    "pred_Lr_op = Lr_op.predict(X_test)\n",
    "\n",
    "# Checking sparsity of array with reduced C\n",
    "w1 = Lr_op.coef_\n",
    "print(\"Sparsity: \" ,np.count_nonzero(w1))\n",
    "\n",
    "print()\n",
    "\n",
    "print(\"Precision = \",precision_score(y_test, pred_Lr_op, pos_label='positive'))# Precision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sparsity:  2\n",
      "\n",
      "Precision =  0.8413333333333334\n"
     ]
    }
   ],
   "source": [
    "# instantiate learning model for best alpha\n",
    "Lr_op = LogisticRegression(penalty='l1', C = 0.002)\n",
    "\n",
    "\n",
    "# fitting the model\n",
    "Lr_op.fit(X1, y1)\n",
    "\n",
    "# predict the response\n",
    "pred_Lr_op = Lr_op.predict(X_test)\n",
    "\n",
    "# Checking sparsity of array with reduced C\n",
    "w1 = Lr_op.coef_\n",
    "print(\"Sparsity: \" ,np.count_nonzero(w1))\n",
    "\n",
    "print()\n",
    "\n",
    "print(\"Precision = \",precision_score(y_test, pred_Lr_op, pos_label='positive'))# Precision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sparsity:  0\n",
      "\n",
      "Precision =  0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/shebu/anaconda3/lib/python3.6/site-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "# instantiate learning model for best alpha\n",
    "Lr_op = LogisticRegression(penalty='l1', C = 0.0004)\n",
    "\n",
    "\n",
    "# fitting the model\n",
    "Lr_op.fit(X1, y1)\n",
    "\n",
    "# predict the response\n",
    "pred_Lr_op = Lr_op.predict(X_test)\n",
    "\n",
    "# Checking sparsity of array with reduced C\n",
    "w1 = Lr_op.coef_\n",
    "print(\"Sparsity: \" ,np.count_nonzero(w1))\n",
    "\n",
    "print()\n",
    "\n",
    "print(\"Precision = \",precision_score(y_test, pred_Lr_op, pos_label='positive'))# Precision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sparsity:  2133\n",
      "\n",
      "Precision =  0.9153318077803204\n"
     ]
    }
   ],
   "source": [
    "# instantiate learning model for best alpha\n",
    "Lr_op = LogisticRegression(penalty='l1', C = 0.5)\n",
    "\n",
    "\n",
    "# fitting the model\n",
    "Lr_op.fit(X1, y1)\n",
    "\n",
    "# predict the response\n",
    "pred_Lr_op = Lr_op.predict(X_test)\n",
    "\n",
    "# Checking sparsity of array with increased C\n",
    "w1 = Lr_op.coef_\n",
    "print(\"Sparsity: \" ,np.count_nonzero(w1))\n",
    "\n",
    "print()\n",
    "\n",
    "print(\"Precision = \",precision_score(y_test, pred_Lr_op, pos_label='positive'))# Precision"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Taking Tf-idf bigram and vectorizing it\n",
    "tf_idf_vect = TfidfVectorizer(ngram_range=(1,2))\n",
    "final_tf_idf = tf_idf_vect.fit_transform(random_final['CleanedText'].values)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000, 235352)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_tf_idf.get_shape() # Shape of tf-idf vectorizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10000, 230972)\n"
     ]
    }
   ],
   "source": [
    "# Standardizing the data with mean=0 and std.dev=1\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "standardized_data_tf = StandardScaler(with_mean=False).fit_transform(final_tf_idf)\n",
    "print(standardized_data_tf.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_tf = standardized_data_tf.toarray() # storing the values after standardization in a numpy array\n",
    "y_tf = np.array(random_final['Score']) # storing the values of Scores in numpy array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split the data set into train and test, 70% train and 30% test \n",
    "X2, X_test2, y2, y_test2 = train_test_split(X_tf, y_tf, test_size=0.3, shuffle=False, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=TimeSeriesSplit(max_train_size=None, n_splits=10),\n",
       "       error_score='raise',\n",
       "       estimator=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
       "          penalty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
       "          verbose=0, warm_start=False),\n",
       "       fit_params=None, iid=True, n_jobs=1,\n",
       "       param_grid=[{'C': [1e-08, 1e-06, 0.0001, 0.01, 1, 100, 10000, 1000000, 100000000]}],\n",
       "       pre_dispatch='2*n_jobs', refit=True, return_train_score='warn',\n",
       "       scoring=None, verbose=0)"
      ]
     },
     "execution_count": 184,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import cross_val_predict\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "                               \n",
    "tss_tf = TimeSeriesSplit(n_splits=10) # TimeSeries Split with number of splits=10\n",
    "\n",
    "parameters_tf = [{'C': [10**-8, 10**-6, 10**-4, 10**-2, 10**0, 10**2, 10**4, 10**6, 10**8]}]# specifying the range of the hyperparameter C for GridSearch\n",
    "\n",
    "Lr_tf = LogisticRegression()\n",
    "\n",
    "# GridSearch for finding right hyperparameter with with 10 fold CV on TimeSeriesSplit\n",
    "model_tf = GridSearchCV(Lr_tf, parameters_tf, cv=tss_tf, refit=True)\n",
    "\n",
    "# Fit the training data\n",
    "model_tf.fit(X2, y2)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'C': 0.01}"
      ]
     },
     "execution_count": 185,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_tf.best_params_  # print the best hyperparameter C"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Accuracy =  0.893\n",
      "Confusion Matrix\n",
      " [[ 241  235]\n",
      " [  86 2438]]\n",
      "(tn, fn, fp, tp) = (241, 235, 86, 2438)\n",
      "Recall =  0.9659270998415214\n",
      "f1-Score =  0.9382335963055608\n",
      "Precision =  0.9120838009726898\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import recall_score, f1_score, confusion_matrix, accuracy_score, precision_score\n",
    "\n",
    "\n",
    "# instantiate learning model for best C\n",
    "Lr_tf = LogisticRegression(penalty='l2', C=0.01)\n",
    "\n",
    "# fitting the model\n",
    "Lr_tf.fit(X2, y2)\n",
    "\n",
    "# predict the response\n",
    "pred_Lr_tf = Lr_tf.predict(X_test2)\n",
    "\n",
    "\n",
    "print()\n",
    "\n",
    "# evaluate various performance measures:\n",
    "print(\"Accuracy = \",accuracy_score(y_test2, pred_Lr_tf)) # accuracy\n",
    "print(\"Confusion Matrix\\n\", confusion_matrix(y_test2, pred_Lr_tf)) # Confusion Matrix\n",
    "tn, fn, fp, tp = confusion_matrix(y_test2, pred_Lr_tf).ravel() # tn, fn, fp, tp\n",
    "print(\"(tn, fn, fp, tp) =\",(tn, fn, fp, tp))\n",
    "print(\"Recall = \",recall_score(y_test2, pred_Lr_tf, pos_label='positive')) # Recall\n",
    "print(\"f1-Score = \"  ,f1_score(y_test2, pred_Lr_tf, pos_label='positive')) # f1-Score\n",
    "print(\"Precision = \",precision_score(y_test2, pred_Lr_tf, pos_label='positive'))# Precision\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 3.44296975e-01, -7.33319413e-02, -5.05756970e-02,\n",
       "        -3.88375215e-02,  2.35701281e-01,  1.21512996e-01,\n",
       "        -3.77018290e-01,  5.68081899e-01,  6.70595783e-01,\n",
       "        -2.15901516e-01,  4.62851848e-02, -2.23349184e-03,\n",
       "        -1.03468668e-01, -4.87580273e-02, -1.29410402e-01,\n",
       "        -1.78656586e-02, -3.14731018e-03, -2.71647002e-01,\n",
       "         2.76581550e-01, -3.02133963e-01,  4.11345397e-04,\n",
       "         3.26997745e-02,  2.41330521e-02, -2.86670188e-01,\n",
       "        -2.56182740e-01, -4.47187236e-02, -2.05758113e-01,\n",
       "         6.44139432e-02, -1.08971534e-02, -7.49406618e-02,\n",
       "         3.12927216e-01, -5.94701965e-02,  4.19310999e-06,\n",
       "        -1.44160040e-01,  5.43187044e-02, -5.84022756e-02,\n",
       "         1.85875981e-01,  1.04249139e-01,  2.82745629e-02,\n",
       "         1.23403580e-01,  2.08463803e-03,  8.59077982e-03,\n",
       "         1.53544439e-02,  3.13736579e-01, -9.57707682e-03,\n",
       "        -1.40603857e-02,  4.16678250e-03, -1.29515909e-01,\n",
       "         3.08293064e-02, -3.85846679e-02, -8.90637435e-02,\n",
       "        -1.09630940e-01, -1.73133449e-02,  1.52489874e-01,\n",
       "         1.47977089e-02, -1.70414243e-01, -3.49428838e-03,\n",
       "        -9.04979353e-02,  1.04137458e-01,  2.22760479e-02,\n",
       "        -5.58518755e-02, -3.98276109e-02,  6.10048974e-02,\n",
       "         5.76259066e-02,  8.33064634e-02, -2.23217562e-01,\n",
       "         1.96664487e-02,  6.89977620e-02,  2.36478281e-02,\n",
       "         6.22810254e-03, -4.42903675e-02,  8.66236066e-03,\n",
       "         2.79020714e-04,  1.88384082e-02,  1.21849933e-01,\n",
       "         2.04290218e-02,  5.51543172e-02, -5.71390019e-02,\n",
       "         1.79057630e-01,  2.00222993e-01, -7.06121041e-02,\n",
       "        -1.14570719e-01,  5.93706040e-02,  1.90922300e-02,\n",
       "        -1.23870911e-01, -2.17199844e-01,  2.72711068e-02,\n",
       "        -1.35426480e-02, -4.69371071e-02,  8.22779767e-02,\n",
       "         6.16324958e-02,  1.33149192e-01,  7.92353462e-03,\n",
       "         5.01053317e-02, -1.71476320e-01,  1.35221796e-01,\n",
       "        -2.18045618e-02, -4.64838531e-02, -2.38125609e-02,\n",
       "        -7.64113652e-02,  1.20360956e-01,  8.04866855e-02,\n",
       "         1.99313053e-02,  1.13577011e-02, -1.40380711e-01,\n",
       "        -8.51914667e-02, -4.99580337e-02,  1.22640879e-01,\n",
       "         1.39638508e-02,  5.75428931e-02,  1.78131006e-02,\n",
       "        -3.92067937e-02, -3.10937537e-02,  1.12126805e-02,\n",
       "        -2.19464358e-02, -1.08595183e-01, -1.53131413e-01,\n",
       "         1.04376379e-01,  1.03894223e-01, -9.64752737e-02,\n",
       "        -7.83809986e-02, -6.49646557e-02,  1.28047078e-01,\n",
       "        -8.15202950e-02,  1.17410978e-02,  4.50544467e-02,\n",
       "        -1.50696731e-01,  2.67564375e-02, -2.41293565e-02,\n",
       "        -5.91215857e-02, -2.32480938e-02,  1.66958826e-02,\n",
       "         6.70923089e-02,  1.80783411e-02, -4.29452283e-02,\n",
       "        -5.91014672e-02,  9.91489591e-02,  1.61544672e-02,\n",
       "         7.47699531e-02,  4.67810089e-02, -2.64481196e-02,\n",
       "        -1.32963131e-02,  3.76757240e-02, -2.28585004e-02,\n",
       "         4.92455076e-02, -7.66066699e-02,  3.43379189e-02,\n",
       "        -3.75218105e-02, -7.22948675e-02,  1.29006031e-02,\n",
       "        -5.44945297e-02, -7.61463894e-02, -2.82849390e-03,\n",
       "        -7.29957696e-02, -1.38891472e-02, -4.18913837e-02,\n",
       "         1.35035627e-02, -4.37587643e-03, -2.24171430e-02,\n",
       "         1.04850603e-01, -2.63667252e-03, -6.00401853e-02,\n",
       "         3.44327786e-02, -5.58040784e-02, -4.32805087e-02,\n",
       "         1.12043111e-01,  5.29174874e-03, -8.88053798e-02,\n",
       "         4.75375700e-02, -5.65576734e-02, -1.32516851e-02,\n",
       "         6.07383053e-02, -1.69030284e-02,  4.62545370e-03,\n",
       "         4.21524237e-02,  3.21562116e-02,  4.48423685e-02,\n",
       "         1.13569762e-02,  2.03982934e-03, -9.89900857e-03,\n",
       "         6.29487368e-02, -3.35402002e-02, -7.79976800e-02,\n",
       "         3.79676492e-02, -4.44210009e-02, -6.58362340e-02,\n",
       "        -4.54088071e-03, -3.61185516e-02,  2.02611099e-02,\n",
       "        -9.64094150e-02,  3.63891229e-02, -2.33223875e-02,\n",
       "         6.21377689e-03, -6.63926763e-03, -4.53421045e-02,\n",
       "         3.06363130e-02, -1.85680194e-02,  3.61867329e-02,\n",
       "         8.81596208e-02,  4.20805032e-02,  4.71132255e-02,\n",
       "         8.93760669e-02,  2.76037449e-02,  2.05960661e-02,\n",
       "         7.79655370e-02, -2.03530390e-02, -9.86573950e-02,\n",
       "        -1.05380847e-01, -8.73622943e-02,  7.66288928e-02,\n",
       "        -8.54238684e-02,  3.27338607e-02, -3.16925938e-02,\n",
       "        -3.52539724e-02, -4.48652166e-02,  1.94402501e-02,\n",
       "        -2.64232978e-02,  6.43128997e-02, -3.55939755e-02,\n",
       "        -1.91597216e-02,  1.70292934e-02,  1.31771289e-01,\n",
       "         1.50186270e-02, -3.20518090e-02,  6.53895278e-02,\n",
       "        -2.17581278e-02,  7.44034258e-04, -5.66966431e-02,\n",
       "        -1.30637516e-02,  5.36043406e-03, -1.01732535e-01,\n",
       "         7.44384197e-02, -3.00407235e-02,  2.53396813e-02,\n",
       "        -3.71303055e-02, -9.36144116e-02, -6.31512375e-02,\n",
       "        -5.74109460e-03, -9.24328851e-03, -1.73292951e-02,\n",
       "         5.74556809e-02, -4.52938968e-02,  4.69138709e-02,\n",
       "        -4.38004224e-02, -2.96654363e-02, -4.50985792e-02,\n",
       "         3.47507344e-02,  3.44867799e-03,  5.84629115e-02,\n",
       "         1.25447826e-02,  7.78095615e-02,  9.32164180e-02,\n",
       "         5.34003552e-02,  3.25216971e-02,  1.18574114e-02,\n",
       "        -2.27008927e-03, -6.03559834e-02,  7.50404593e-02,\n",
       "         8.29548192e-03,  7.61918953e-02,  5.00698856e-02,\n",
       "         5.25956720e-03,  1.86904575e-03,  2.44494553e-02,\n",
       "         9.56853334e-03, -5.51248826e-02, -3.54571251e-02,\n",
       "         9.22388322e-02,  2.73874238e-02, -1.48314263e-03,\n",
       "        -1.16153133e-02, -5.71870004e-02, -7.20185348e-02,\n",
       "         5.58830703e-02,  2.68010799e-02,  7.11513357e-02,\n",
       "        -7.37747198e-02, -2.60437363e-02,  9.19319664e-02,\n",
       "        -2.44088339e-03, -5.61082500e-02, -6.89277377e-02,\n",
       "        -2.68267593e-02,  3.06704906e-02, -3.20342738e-02,\n",
       "         7.31010651e-02,  3.59591347e-02, -2.21126778e-02,\n",
       "         1.47900922e-02,  1.59962880e-02,  5.44418455e-03,\n",
       "         7.35572677e-02, -3.48337498e-02,  8.97823364e-03,\n",
       "         7.74628337e-03, -1.04961652e-02,  6.02448704e-02,\n",
       "        -7.48883592e-03, -3.97993326e-02,  5.02028862e-02,\n",
       "         9.18386557e-03,  2.01192555e-02,  4.20436890e-02,\n",
       "         2.90482217e-02, -5.50955936e-02,  1.28665046e-02,\n",
       "         8.99030088e-03,  7.15004251e-02, -2.28498804e-02,\n",
       "         3.11832357e-02,  4.55087331e-02,  1.66441151e-02,\n",
       "         4.85735471e-02,  2.25016017e-02, -5.59722184e-02,\n",
       "         1.18983497e-02, -4.78219853e-02,  1.19364390e-02,\n",
       "         1.14531979e-02,  8.65775893e-03, -1.67995963e-02,\n",
       "         1.74371438e-02, -2.08200002e-02,  2.14861478e-02,\n",
       "         2.76635392e-02,  2.83405378e-02, -1.32237930e-02,\n",
       "         2.65118545e-03, -8.12377056e-03,  1.93115726e-02,\n",
       "         2.06024857e-02,  5.64608254e-02,  3.36573538e-02,\n",
       "        -6.65155507e-02,  8.42909653e-02,  2.16545060e-02,\n",
       "         5.58246634e-02,  3.22220752e-02, -9.84673679e-02,\n",
       "        -5.46737576e-02, -1.71500518e-02,  8.87126598e-03,\n",
       "        -1.21600198e-02,  7.97106399e-02,  2.20352289e-02,\n",
       "         5.93205381e-02, -5.93005799e-03, -1.42494437e-02,\n",
       "        -2.46511371e-02,  4.28089666e-02,  1.11792179e-02,\n",
       "        -7.22572457e-02, -3.66457452e-02, -8.13608369e-02,\n",
       "         1.36717658e-03,  1.61066614e-02,  8.22418735e-02,\n",
       "         1.38948179e-02, -1.31701517e-02,  3.47683424e-02,\n",
       "         4.01645183e-02, -3.70324277e-02,  1.04424563e-02,\n",
       "        -7.37279622e-02, -3.54714756e-03,  7.13352653e-02,\n",
       "         8.59208997e-04,  2.69081804e-02,  6.42953853e-02,\n",
       "         2.23346499e-03,  2.22404514e-02,  3.36315116e-02,\n",
       "        -1.24762361e-02, -6.45235394e-02,  5.59668318e-02,\n",
       "        -2.04611085e-02, -6.11955181e-02, -1.45909269e-02,\n",
       "         5.27115778e-02, -3.82235555e-02, -2.28485122e-02,\n",
       "         1.54271976e-03, -3.59135763e-03,  5.29550345e-02,\n",
       "        -2.79150848e-02, -3.45426271e-02,  5.63728132e-03,\n",
       "         4.66223009e-02,  2.98382149e-02, -1.09735446e-02,\n",
       "        -6.16829928e-04,  4.17029084e-04, -7.28197818e-02,\n",
       "         1.21469062e-02,  1.24856243e-02, -9.39794052e-03,\n",
       "        -4.80748007e-02,  7.81901108e-03, -4.01385027e-02,\n",
       "        -3.51330172e-02]])"
      ]
     },
     "execution_count": 187,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Lr_tf.coef_ # weights of the features of tf-idf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [],
   "source": [
    "c = np.empty(0)\n",
    "c = Lr_tf.coef_ # put in a numpy array i.e weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_tf_idf = c + np.random.normal(0, 0.002) # adding noise i.e pertubation test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 3.45201315e-01, -7.24276006e-02, -4.96713563e-02,\n",
       "        -3.79331808e-02,  2.36605622e-01,  1.22417336e-01,\n",
       "        -3.76113949e-01,  5.68986239e-01,  6.71500124e-01,\n",
       "        -2.14997175e-01,  4.71895255e-02, -1.32915113e-03,\n",
       "        -1.02564327e-01, -4.78536866e-02, -1.28506061e-01,\n",
       "        -1.69613179e-02, -2.24296947e-03, -2.70742661e-01,\n",
       "         2.77485891e-01, -3.01229623e-01,  1.31568611e-03,\n",
       "         3.36041153e-02,  2.50373928e-02, -2.85765848e-01,\n",
       "        -2.55278399e-01, -4.38143829e-02, -2.04853773e-01,\n",
       "         6.53182839e-02, -9.99281273e-03, -7.40363211e-02,\n",
       "         3.13831557e-01, -5.85658558e-02,  9.08533819e-04,\n",
       "        -1.43255699e-01,  5.52230451e-02, -5.74979349e-02,\n",
       "         1.86780322e-01,  1.05153479e-01,  2.91789036e-02,\n",
       "         1.24307920e-01,  2.98897874e-03,  9.49512053e-03,\n",
       "         1.62587846e-02,  3.14640919e-01, -8.67273611e-03,\n",
       "        -1.31560450e-02,  5.07112321e-03, -1.28611568e-01,\n",
       "         3.17336471e-02, -3.76803272e-02, -8.81594028e-02,\n",
       "        -1.08726599e-01, -1.64090042e-02,  1.53394214e-01,\n",
       "         1.57020496e-02, -1.69509902e-01, -2.58994767e-03,\n",
       "        -8.95935946e-02,  1.05041798e-01,  2.31803886e-02,\n",
       "        -5.49475348e-02, -3.89232702e-02,  6.19092381e-02,\n",
       "         5.85302473e-02,  8.42108041e-02, -2.22313222e-01,\n",
       "         2.05707895e-02,  6.99021027e-02,  2.45521688e-02,\n",
       "         7.13244325e-03, -4.33860268e-02,  9.56670137e-03,\n",
       "         1.18336142e-03,  1.97427489e-02,  1.22754273e-01,\n",
       "         2.13333625e-02,  5.60586579e-02, -5.62346612e-02,\n",
       "         1.79961971e-01,  2.01127333e-01, -6.97077634e-02,\n",
       "        -1.13666378e-01,  6.02749447e-02,  1.99965707e-02,\n",
       "        -1.22966570e-01, -2.16295503e-01,  2.81754475e-02,\n",
       "        -1.26383073e-02, -4.60327664e-02,  8.31823174e-02,\n",
       "         6.25368365e-02,  1.34053533e-01,  8.82787533e-03,\n",
       "         5.10096724e-02, -1.70571979e-01,  1.36126136e-01,\n",
       "        -2.09002211e-02, -4.55795124e-02, -2.29082201e-02,\n",
       "        -7.55070245e-02,  1.21265296e-01,  8.13910263e-02,\n",
       "         2.08356460e-02,  1.22620418e-02, -1.39476371e-01,\n",
       "        -8.42871260e-02, -4.90536930e-02,  1.23545220e-01,\n",
       "         1.48681915e-02,  5.84472338e-02,  1.87174413e-02,\n",
       "        -3.83024530e-02, -3.01894130e-02,  1.21170212e-02,\n",
       "        -2.10420950e-02, -1.07690842e-01, -1.52227073e-01,\n",
       "         1.05280720e-01,  1.04798563e-01, -9.55709330e-02,\n",
       "        -7.74766579e-02, -6.40603150e-02,  1.28951418e-01,\n",
       "        -8.06159543e-02,  1.26454385e-02,  4.59587874e-02,\n",
       "        -1.49792390e-01,  2.76607782e-02, -2.32250158e-02,\n",
       "        -5.82172449e-02, -2.23437531e-02,  1.76002233e-02,\n",
       "         6.79966496e-02,  1.89826819e-02, -4.20408875e-02,\n",
       "        -5.81971264e-02,  1.00053300e-01,  1.70588079e-02,\n",
       "         7.56742938e-02,  4.76853496e-02, -2.55437788e-02,\n",
       "        -1.23919724e-02,  3.85800647e-02, -2.19541596e-02,\n",
       "         5.01498483e-02, -7.57023292e-02,  3.52422596e-02,\n",
       "        -3.66174698e-02, -7.13905267e-02,  1.38049438e-02,\n",
       "        -5.35901890e-02, -7.52420487e-02, -1.92415319e-03,\n",
       "        -7.20914288e-02, -1.29848065e-02, -4.09870430e-02,\n",
       "         1.44079034e-02, -3.47153572e-03, -2.15128023e-02,\n",
       "         1.05754943e-01, -1.73233182e-03, -5.91358446e-02,\n",
       "         3.53371193e-02, -5.48997377e-02, -4.23761680e-02,\n",
       "         1.12947451e-01,  6.19608945e-03, -8.79010391e-02,\n",
       "         4.84419107e-02, -5.56533327e-02, -1.23473444e-02,\n",
       "         6.16426460e-02, -1.59986877e-02,  5.52979441e-03,\n",
       "         4.30567644e-02,  3.30605523e-02,  4.57467092e-02,\n",
       "         1.22613169e-02,  2.94417005e-03, -8.99466786e-03,\n",
       "         6.38530775e-02, -3.26358595e-02, -7.70933393e-02,\n",
       "         3.88719899e-02, -4.35166602e-02, -6.49318933e-02,\n",
       "        -3.63654000e-03, -3.52142108e-02,  2.11654506e-02,\n",
       "        -9.55050742e-02,  3.72934637e-02, -2.24180468e-02,\n",
       "         7.11811760e-03, -5.73492693e-03, -4.44377638e-02,\n",
       "         3.15406537e-02, -1.76636787e-02,  3.70910736e-02,\n",
       "         8.90639615e-02,  4.29848439e-02,  4.80175662e-02,\n",
       "         9.02804076e-02,  2.85080856e-02,  2.15004068e-02,\n",
       "         7.88698777e-02, -1.94486983e-02, -9.77530542e-02,\n",
       "        -1.04476506e-01, -8.64579536e-02,  7.75332335e-02,\n",
       "        -8.45195277e-02,  3.36382014e-02, -3.07882531e-02,\n",
       "        -3.43496317e-02, -4.39608759e-02,  2.03445908e-02,\n",
       "        -2.55189571e-02,  6.52172404e-02, -3.46896348e-02,\n",
       "        -1.82553809e-02,  1.79336342e-02,  1.32675630e-01,\n",
       "         1.59229677e-02, -3.11474683e-02,  6.62938685e-02,\n",
       "        -2.08537871e-02,  1.64837497e-03, -5.57923024e-02,\n",
       "        -1.21594109e-02,  6.26477477e-03, -1.00828194e-01,\n",
       "         7.53427604e-02, -2.91363828e-02,  2.62440220e-02,\n",
       "        -3.62259647e-02, -9.27100709e-02, -6.22468968e-02,\n",
       "        -4.83675389e-03, -8.33894780e-03, -1.64249544e-02,\n",
       "         5.83600216e-02, -4.43895561e-02,  4.78182116e-02,\n",
       "        -4.28960817e-02, -2.87610955e-02, -4.41942385e-02,\n",
       "         3.56550751e-02,  4.35301870e-03,  5.93672523e-02,\n",
       "         1.34491233e-02,  7.87139022e-02,  9.41207587e-02,\n",
       "         5.43046959e-02,  3.34260378e-02,  1.27617521e-02,\n",
       "        -1.36574857e-03, -5.94516427e-02,  7.59448000e-02,\n",
       "         9.19982263e-03,  7.70962360e-02,  5.09742263e-02,\n",
       "         6.16390791e-03,  2.77338646e-03,  2.53537960e-02,\n",
       "         1.04728741e-02, -5.42205419e-02, -3.45527844e-02,\n",
       "         9.31431729e-02,  2.82917645e-02, -5.78801925e-04,\n",
       "        -1.07109726e-02, -5.62826597e-02, -7.11141941e-02,\n",
       "         5.67874110e-02,  2.77054206e-02,  7.20556765e-02,\n",
       "        -7.28703791e-02, -2.51393956e-02,  9.28363071e-02,\n",
       "        -1.53654268e-03, -5.52039093e-02, -6.80233970e-02,\n",
       "        -2.59224186e-02,  3.15748313e-02, -3.11299331e-02,\n",
       "         7.40054059e-02,  3.68634754e-02, -2.12083371e-02,\n",
       "         1.56944329e-02,  1.69006287e-02,  6.34852526e-03,\n",
       "         7.44616084e-02, -3.39294091e-02,  9.88257435e-03,\n",
       "         8.65062408e-03, -9.59182447e-03,  6.11492111e-02,\n",
       "        -6.58449521e-03, -3.88949918e-02,  5.11072269e-02,\n",
       "         1.00882063e-02,  2.10235962e-02,  4.29480297e-02,\n",
       "         2.99525625e-02, -5.41912529e-02,  1.37708453e-02,\n",
       "         9.89464159e-03,  7.24047658e-02, -2.19455397e-02,\n",
       "         3.20875764e-02,  4.64130738e-02,  1.75484558e-02,\n",
       "         4.94778878e-02,  2.34059424e-02, -5.50678777e-02,\n",
       "         1.28026904e-02, -4.69176445e-02,  1.28407797e-02,\n",
       "         1.23575386e-02,  9.56209964e-03, -1.58952556e-02,\n",
       "         1.83414845e-02, -1.99156595e-02,  2.23904885e-02,\n",
       "         2.85678799e-02,  2.92448785e-02, -1.23194522e-02,\n",
       "         3.55552616e-03, -7.21942985e-03,  2.02159133e-02,\n",
       "         2.15068264e-02,  5.73651661e-02,  3.45616945e-02,\n",
       "        -6.56112100e-02,  8.51953061e-02,  2.25588467e-02,\n",
       "         5.67290041e-02,  3.31264159e-02, -9.75630272e-02,\n",
       "        -5.37694169e-02, -1.62457111e-02,  9.77560669e-03,\n",
       "        -1.12556791e-02,  8.06149806e-02,  2.29395696e-02,\n",
       "         6.02248789e-02, -5.02571728e-03, -1.33451030e-02,\n",
       "        -2.37467964e-02,  4.37133073e-02,  1.20835586e-02,\n",
       "        -7.13529050e-02, -3.57414045e-02, -8.04564962e-02,\n",
       "         2.27151729e-03,  1.70110021e-02,  8.31462142e-02,\n",
       "         1.47991586e-02, -1.22658110e-02,  3.56726831e-02,\n",
       "         4.10688590e-02, -3.61280870e-02,  1.13467970e-02,\n",
       "        -7.28236215e-02, -2.64280685e-03,  7.22396060e-02,\n",
       "         1.76354971e-03,  2.78125211e-02,  6.51997261e-02,\n",
       "         3.13780570e-03,  2.31447921e-02,  3.45358523e-02,\n",
       "        -1.15718954e-02, -6.36191987e-02,  5.68711725e-02,\n",
       "        -1.95567678e-02, -6.02911774e-02, -1.36865862e-02,\n",
       "         5.36159185e-02, -3.73192148e-02, -2.19441715e-02,\n",
       "         2.44706047e-03, -2.68701693e-03,  5.38593752e-02,\n",
       "        -2.70107441e-02, -3.36382864e-02,  6.54162203e-03,\n",
       "         4.75266416e-02,  3.07425556e-02, -1.00692038e-02,\n",
       "         2.87510782e-04,  1.32136979e-03, -7.19154410e-02,\n",
       "         1.30512469e-02,  1.33899650e-02, -8.49359981e-03,\n",
       "        -4.71704600e-02,  8.72335179e-03, -3.92341620e-02,\n",
       "        -3.42286765e-02]])"
      ]
     },
     "execution_count": 194,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_tf_idf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# As we can see the difference is very very low in terms of weight matrix and updated weight matrix after adding a noise.\n",
    "# They are not collinear."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 20 Positive Features for BOW and Top 20 Negative Features for tf-idf \n",
      "\n",
      "\n",
      "\n",
      "\tabandon eat    \t-0.3770\tpositive       \t\tabandon        \t0.6706\tnegative       \n",
      "\tabandon contain\t-0.3021\tpositive       \t\tabc six        \t0.5681\tnegative       \n",
      "\taaah           \t-0.2867\tpositive       \t\tabid           \t0.3443\tnegative       \n",
      "\tabil synthes   \t-0.2716\tpositive       \t\tabc            \t0.3137\tnegative       \n",
      "\tabil compar    \t-0.2562\tpositive       \t\tabid kid       \t0.3129\tnegative       \n",
      "\tabc news       \t-0.2232\tpositive       \t\tabl coffe      \t0.2766\tnegative       \n",
      "\taback          \t-0.2172\tpositive       \t\tabl follow     \t0.2357\tnegative       \n",
      "\tabl ever       \t-0.2159\tpositive       \t\tabandon idea   \t0.2002\tnegative       \n",
      "\tabil hydrat    \t-0.2058\tpositive       \t\tabil brew      \t0.1859\tnegative       \n",
      "\tabl enjoy      \t-0.1715\tpositive       \t\tabl itll       \t0.1791\tnegative       \n",
      "\tabl bear       \t-0.1704\tpositive       \t\tabl brew       \t0.1525\tnegative       \n",
      "\tabl keep       \t-0.1531\tpositive       \t\tabl recommend  \t0.1352\tnegative       \n",
      "\tabl hold       \t-0.1507\tpositive       \t\tabl ship       \t0.1331\tnegative       \n",
      "\tabsolut granola\t-0.1442\tpositive       \t\tabil find      \t0.1318\tnegative       \n",
      "\tabl sausag     \t-0.1404\tpositive       \t\tabl open       \t0.1280\tnegative       \n",
      "\tabil keep      \t-0.1295\tpositive       \t\tabl            \t0.1234\tnegative       \n",
      "\tabl peanut     \t-0.1294\tpositive       \t\tabbey trappist \t0.1226\tnegative       \n",
      "\tabl detect     \t-0.1239\tpositive       \t\tabl finish     \t0.1218\tnegative       \n",
      "\taback first    \t-0.1146\tpositive       \t\tabl expect     \t0.1215\tnegative       \n",
      "\tabl maintain   \t-0.1096\tpositive       \t\tabl amazon     \t0.1204\tnegative       \n"
     ]
    }
   ],
   "source": [
    "# print top 20 features for each class for tf-idf\n",
    "print(\"Top 20 Positive Features for BOW and Top 20 Negative Features for tf-idf \")\n",
    "print(\"\\n\\n\")\n",
    "\n",
    "# defining a function for finding the top features of each class\n",
    "def top_most_features(vectorizer, clf, n=20):\n",
    "    feature_names = vectorizer.get_feature_names() # getting all the feature names\n",
    "    coefs_with_fns = sorted(zip(clf.coef_[0], feature_names)) # retrieve the coefficient & sort them based on values \n",
    "    top = zip(coefs_with_fns[:n], coefs_with_fns[:-(n + 1):-1]) \n",
    "    for (coef_1, fn_1), (coef_2, fn_2) in top:\n",
    "        print (\"\\t%-15s\\t%.4f\\t%-15s\\t\\t%-15s\\t%.4f\\t%-15s\" % (fn_2, coef_1, 'positive', fn_1, coef_2, 'negative'))\n",
    "\n",
    "        \n",
    "top_most_features(tf_idf_vect, Lr_tf)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sparsity:  38\n",
      "\n",
      "Accuracy =  0.8603333333333333\n",
      "Confusion Matrix\n",
      " [[  78  398]\n",
      " [  21 2503]]\n",
      "(tn, fn, fp, tp) = (78, 398, 21, 2503)\n",
      "Recall =  0.9916798732171157\n",
      "f1-Score =  0.9227649769585253\n",
      "Precision =  0.8628059289900034\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import recall_score, f1_score, confusion_matrix, accuracy_score, precision_score\n",
    "\n",
    "\n",
    "# instantiate learning model for best C\n",
    "Lr_tf = LogisticRegression(penalty='l1', C=0.01)\n",
    "\n",
    "# fitting the model\n",
    "Lr_tf.fit(X2, y2)\n",
    "\n",
    "# predict the response\n",
    "pred_Lr_tf = Lr_tf.predict(X_test2)\n",
    "\n",
    "# Checking sparsity of array with C\n",
    "w2 = Lr_tf.coef_\n",
    "print(\"Sparsity: \" ,np.count_nonzero(w2))\n",
    "\n",
    "print()\n",
    "\n",
    "# evaluate various performance measures:\n",
    "print(\"Accuracy = \",accuracy_score(y_test2, pred_Lr_tf)) # accuracy\n",
    "print(\"Confusion Matrix\\n\", confusion_matrix(y_test2, pred_Lr_tf)) # Confusion Matrix\n",
    "tn, fn, fp, tp = confusion_matrix(y_test2, pred_Lr_tf).ravel() # tn, fn, fp, tp\n",
    "print(\"(tn, fn, fp, tp) =\",(tn, fn, fp, tp))\n",
    "print(\"Recall = \",recall_score(y_test2, pred_Lr_tf, pos_label='positive')) # Recall\n",
    "print(\"f1-Score = \"  ,f1_score(y_test2, pred_Lr_tf, pos_label='positive')) # f1-Score\n",
    "print(\"Precision = \",precision_score(y_test2, pred_Lr_tf, pos_label='positive'))# Precision\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sparsity:  22\n",
      "\n",
      "Precision =  0.8523421588594705\n"
     ]
    }
   ],
   "source": [
    "# instantiate learning model for best alpha\n",
    "Lr_tf = LogisticRegression(penalty='l1', C = 0.007)\n",
    "\n",
    "\n",
    "# fitting the model\n",
    "Lr_tf.fit(X2, y2)\n",
    "\n",
    "# predict the response\n",
    "pred_Lr_tf = Lr_tf.predict(X_test2)\n",
    "\n",
    "# Checking sparsity of array with reduced C\n",
    "w2 = Lr_tf.coef_\n",
    "print(\"Sparsity: \" ,np.count_nonzero(w2))\n",
    "\n",
    "print()\n",
    "\n",
    "print(\"Precision = \",precision_score(y_test2, pred_Lr_tf, pos_label='positive'))# Precision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sparsity:  7\n",
      "\n",
      "Precision =  0.8418945963975984\n"
     ]
    }
   ],
   "source": [
    "# instantiate learning model for best alpha\n",
    "Lr_tf = LogisticRegression(penalty='l1', C = 0.004)\n",
    "\n",
    "\n",
    "# fitting the model\n",
    "Lr_tf.fit(X2, y2)\n",
    "\n",
    "# predict the response\n",
    "pred_Lr_tf = Lr_tf.predict(X_test2)\n",
    "\n",
    "# Checking sparsity of array with reduced C\n",
    "w2 = Lr_tf.coef_\n",
    "print(\"Sparsity: \" ,np.count_nonzero(w2))\n",
    "\n",
    "print()\n",
    "\n",
    "print(\"Precision = \",precision_score(y_test2, pred_Lr_tf, pos_label='positive'))# Precision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sparsity:  0\n",
      "\n",
      "Precision =  0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/shebu/anaconda3/lib/python3.6/site-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "# instantiate learning model for best alpha\n",
    "Lr_tf = LogisticRegression(penalty='l1', C = 0.0004)\n",
    "\n",
    "\n",
    "# fitting the model\n",
    "Lr_tf.fit(X2, y2)\n",
    "\n",
    "# predict the response\n",
    "pred_Lr_tf = Lr_tf.predict(X_test2)\n",
    "\n",
    "# Checking sparsity of array with reduced C\n",
    "w2 = Lr_tf.coef_\n",
    "print(\"Sparsity: \" ,np.count_nonzero(w2))\n",
    "\n",
    "print()\n",
    "\n",
    "print(\"Precision = \",precision_score(y_test2, pred_Lr_tf, pos_label='positive'))# Precision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sparsity:  373\n",
      "\n",
      "Precision =  0.9246653919694072\n"
     ]
    }
   ],
   "source": [
    "# instantiate learning model for best alpha\n",
    "Lr_tf = LogisticRegression(penalty='l1', C = 0.5)\n",
    "\n",
    "\n",
    "# fitting the model\n",
    "Lr_tf.fit(X2, y2)\n",
    "\n",
    "# predict the response\n",
    "pred_Lr_tf = Lr_tf.predict(X_test2)\n",
    "\n",
    "# Checking sparsity of array with increased C\n",
    "w2 = Lr_tf.coef_\n",
    "print(\"Sparsity: \" ,np.count_nonzero(w2))\n",
    "\n",
    "print()\n",
    "\n",
    "print(\"Precision = \",precision_score(y_test2, pred_Lr_tf, pos_label='positive'))# Precision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomizedSearchCV(cv=TimeSeriesSplit(max_train_size=None, n_splits=10),\n",
       "          error_score='raise',\n",
       "          estimator=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
       "          penalty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
       "          verbose=0, warm_start=False),\n",
       "          fit_params=None, iid=True, n_iter=10, n_jobs=1,\n",
       "          param_distributions={'C': <scipy.stats._distn_infrastructure.rv_frozen object at 0x7f352e45c4a8>},\n",
       "          pre_dispatch='2*n_jobs', random_state=None, refit=True,\n",
       "          return_train_score='warn', scoring=None, verbose=0)"
      ]
     },
     "execution_count": 212,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import cross_val_predict\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from scipy.stats import uniform as sp_rand\n",
    "                               \n",
    "tss_tf_r = TimeSeriesSplit(n_splits=10) # TimeSeries Split with number of splits=10\n",
    "\n",
    "parameters_tf_r = {'C': sp_rand()} # specifying the range of the hyperparameter C for RandomizedSearchCV\n",
    "\n",
    "Lr_ran_tf = LogisticRegression()\n",
    "\n",
    "# RandomizedSearchCV for finding right hyperparameter with with 10 fold CV on TimeSeriesSplit\n",
    "model_tf_r = RandomizedSearchCV(Lr_ran_tf, parameters_tf_r, cv=tss_tf_r, refit=True)\n",
    "\n",
    "# Fit the training data\n",
    "model_tf_r.fit(X2, y2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'C': 0.06988426976771223}"
      ]
     },
     "execution_count": 213,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_tf_r.best_params_ # best C"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Accuracy =  0.8936666666666667\n",
      "Confusion Matrix\n",
      " [[ 267  209]\n",
      " [ 110 2414]]\n",
      "(tn, fn, fp, tp) = (267, 209, 110, 2414)\n",
      "Recall =  0.9564183835182251\n",
      "f1-Score =  0.9380221488245579\n",
      "Precision =  0.9203202439954251\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import recall_score, f1_score, confusion_matrix, accuracy_score, precision_score\n",
    "\n",
    "\n",
    "# instantiate learning model for best C\n",
    "Lr_tf_r = LogisticRegression(penalty='l2', C = 0.06988426976771223)\n",
    "\n",
    "# fitting the model\n",
    "Lr_tf_r.fit(X2, y2)\n",
    "\n",
    "# predict the response\n",
    "pred_Lr_tf_r = Lr_tf_r.predict(X_test2)\n",
    "\n",
    "print()\n",
    "\n",
    "# evaluate various performance measures:\n",
    "print(\"Accuracy = \",accuracy_score(y_test2, pred_Lr_tf_r)) # accuracy\n",
    "print(\"Confusion Matrix\\n\", confusion_matrix(y_test2, pred_Lr_tf_r)) # Confusion Matrix\n",
    "tn, fn, fp, tp = confusion_matrix(y_test2, pred_Lr_tf_r).ravel() # tn, fn, fp, tp\n",
    "print(\"(tn, fn, fp, tp) =\",(tn, fn, fp, tp))\n",
    "print(\"Recall = \",recall_score(y_test2, pred_Lr_tf_r, pos_label='positive')) # Recall\n",
    "print(\"f1-Score = \"  ,f1_score(y_test2, pred_Lr_tf_r, pos_label='positive')) # f1-Score\n",
    "print(\"Precision = \",precision_score(y_test2, pred_Lr_tf_r, pos_label='positive'))# Precision\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sparsity:  283\n",
      "\n",
      "Accuracy =  0.8976666666666666\n",
      "Confusion Matrix\n",
      " [[ 243  233]\n",
      " [  74 2450]]\n",
      "(tn, fn, fp, tp) = (243, 233, 74, 2450)\n",
      "Recall =  0.9706814580031695\n",
      "f1-Score =  0.9410409064720568\n",
      "Precision =  0.9131569139023481\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import recall_score, f1_score, confusion_matrix, accuracy_score, precision_score\n",
    "\n",
    "\n",
    "# instantiate learning model for best C\n",
    "Lr_tf_r = LogisticRegression(penalty='l1', C = 0.06988426976771223)\n",
    "\n",
    "# fitting the model\n",
    "Lr_tf_r.fit(X2, y2)\n",
    "\n",
    "# predict the response\n",
    "pred_Lr_tf_r = Lr_tf_r.predict(X_test2)\n",
    "\n",
    "# Checking sparsity of array with C\n",
    "w3 = Lr_tf_r.coef_\n",
    "print(\"Sparsity: \" ,np.count_nonzero(w3))\n",
    "\n",
    "print()\n",
    "\n",
    "# evaluate various performance measures:\n",
    "print(\"Accuracy = \",accuracy_score(y_test2, pred_Lr_tf_r)) # accuracy\n",
    "print(\"Confusion Matrix\\n\", confusion_matrix(y_test2, pred_Lr_tf_r)) # Confusion Matrix\n",
    "tn, fn, fp, tp = confusion_matrix(y_test2, pred_Lr_tf_r).ravel() # tn, fn, fp, tp\n",
    "print(\"(tn, fn, fp, tp) =\",(tn, fn, fp, tp))\n",
    "print(\"Recall = \",recall_score(y_test2, pred_Lr_tf_r, pos_label='positive')) # Recall\n",
    "print(\"f1-Score = \"  ,f1_score(y_test2, pred_Lr_tf_r, pos_label='positive')) # f1-Score\n",
    "print(\"Precision = \",precision_score(y_test2, pred_Lr_tf_r, pos_label='positive'))# Precision\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sparsity:  228\n",
      "\n",
      "Precision =  0.907448377581121\n"
     ]
    }
   ],
   "source": [
    "# instantiate learning model for best alpha\n",
    "Lr_tf_r = LogisticRegression(penalty='l1', C = 0.0408)\n",
    "\n",
    "\n",
    "# fitting the model\n",
    "Lr_tf_r.fit(X2, y2)\n",
    "\n",
    "# predict the response\n",
    "pred_Lr_tf_r = Lr_tf_r.predict(X_test2)\n",
    "\n",
    "# Checking sparsity of array with reduced C\n",
    "w3 = Lr_tf_r.coef_\n",
    "print(\"Sparsity: \" ,np.count_nonzero(w3))\n",
    "\n",
    "print()\n",
    "\n",
    "print(\"Precision = \",precision_score(y_test2, pred_Lr_tf_r, pos_label='positive'))# Precision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sparsity:  189\n",
      "\n",
      "Precision =  0.8998178506375227\n"
     ]
    }
   ],
   "source": [
    "# instantiate learning model for best alpha\n",
    "Lr_tf_r = LogisticRegression(penalty='l1', C = 0.0308)\n",
    "\n",
    "\n",
    "# fitting the model\n",
    "Lr_tf_r.fit(X2, y2)\n",
    "\n",
    "# predict the response\n",
    "pred_Lr_tf_r = Lr_tf_r.predict(X_test2)\n",
    "\n",
    "# Checking sparsity of array with reduced C\n",
    "w3 = Lr_tf_r.coef_\n",
    "print(\"Sparsity: \" ,np.count_nonzero(w3))\n",
    "\n",
    "print()\n",
    "\n",
    "print(\"Precision = \",precision_score(y_test2, pred_Lr_tf_r, pos_label='positive'))# Precision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sparsity:  45\n",
      "\n",
      "Precision =  0.8644536652835408\n"
     ]
    }
   ],
   "source": [
    "# instantiate learning model for best alpha\n",
    "Lr_tf_r = LogisticRegression(penalty='l1', C = 0.0108)\n",
    "\n",
    "\n",
    "# fitting the model\n",
    "Lr_tf_r.fit(X2, y2)\n",
    "\n",
    "# predict the response\n",
    "pred_Lr_tf_r = Lr_tf_r.predict(X_test2)\n",
    "\n",
    "# Checking sparsity of array with reduced C\n",
    "w3 = Lr_tf_r.coef_\n",
    "print(\"Sparsity: \" ,np.count_nonzero(w3))\n",
    "\n",
    "print()\n",
    "\n",
    "print(\"Precision = \",precision_score(y_test2, pred_Lr_tf_r, pos_label='positive'))# Precision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sparsity:  1\n",
      "\n",
      "Precision =  0.8413333333333334\n"
     ]
    }
   ],
   "source": [
    "# instantiate learning model for best alpha\n",
    "Lr_tf_r = LogisticRegression(penalty='l1', C = 0.0018)\n",
    "\n",
    "\n",
    "# fitting the model\n",
    "Lr_tf_r.fit(X2, y2)\n",
    "\n",
    "# predict the response\n",
    "pred_Lr_tf_r = Lr_tf_r.predict(X_test2)\n",
    "\n",
    "# Checking sparsity of array with reduced C\n",
    "w3 = Lr_tf_r.coef_\n",
    "print(\"Sparsity: \" ,np.count_nonzero(w3))\n",
    "\n",
    "print()\n",
    "\n",
    "print(\"Precision = \",precision_score(y_test2, pred_Lr_tf_r, pos_label='positive'))# Precision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sparsity:  0\n",
      "\n",
      "Precision =  0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/shebu/anaconda3/lib/python3.6/site-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "# instantiate learning model for best alpha\n",
    "Lr_tf_r = LogisticRegression(penalty='l1', C = 0.0004)\n",
    "\n",
    "\n",
    "# fitting the model\n",
    "Lr_tf_r.fit(X2, y2)\n",
    "\n",
    "# predict the response\n",
    "pred_Lr_tf_r = Lr_tf_r.predict(X_test2)\n",
    "\n",
    "# Checking sparsity of array with reduced C\n",
    "w3 = Lr_tf_r.coef_\n",
    "print(\"Sparsity: \" ,np.count_nonzero(w3))\n",
    "\n",
    "print()\n",
    "\n",
    "print(\"Precision = \",precision_score(y_test2, pred_Lr_tf_r, pos_label='positive'))# Precision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sparsity:  384\n",
      "\n",
      "Precision =  0.9253159708923784\n"
     ]
    }
   ],
   "source": [
    "# instantiate learning model for best alpha\n",
    "Lr_tf_r = LogisticRegression(penalty='l1', C = 0.9)\n",
    "\n",
    "\n",
    "# fitting the model\n",
    "Lr_tf_r.fit(X2, y2)\n",
    "\n",
    "# predict the response\n",
    "pred_Lr_tf_r = Lr_tf_r.predict(X_test2)\n",
    "\n",
    "# Checking sparsity of array with increased C\n",
    "w3 = Lr_tf_r.coef_\n",
    "print(\"Sparsity: \" ,np.count_nonzero(w3))\n",
    "\n",
    "print()\n",
    "\n",
    "print(\"Precision = \",precision_score(y_test2, pred_Lr_tf_r, pos_label='positive'))# Precision"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training Word2Vec model using our own text corpus\n",
    "\n",
    "import gensim\n",
    "i=0\n",
    "list_of_sent=[]\n",
    "for sent in random_final['Text'].values:\n",
    "    filtered_sentence=[]\n",
    "    sent=cleanhtml(sent)\n",
    "    for w in sent.split():\n",
    "        for cleaned_words in cleanpunc(w).split():\n",
    "            if(cleaned_words.isalpha()):    \n",
    "                filtered_sentence.append(cleaned_words.lower())\n",
    "            else:\n",
    "                continue \n",
    "    list_of_sent.append(filtered_sentence)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v_model=gensim.models.Word2Vec(list_of_sent,min_count=5,size=50, workers=6)  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Avg W2V"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000\n",
      "50\n"
     ]
    }
   ],
   "source": [
    "# average Word2Vec\n",
    "# compute average word2vec for each review.\n",
    "sent_vectors = []; # the avg-w2v for each sentence/review is stored in this list\n",
    "for sent in list_of_sent: # for each review/sentence\n",
    "    sent_vec = np.zeros(50) # as word vectors are of zero length\n",
    "    cnt_words =0; # num of words with a valid vector in the sentence/review\n",
    "    for word in sent: # for each word in a review/sentence\n",
    "        try:\n",
    "            vec = w2v_model.wv[word]\n",
    "            sent_vec += vec\n",
    "            cnt_words += 1\n",
    "        except:\n",
    "            pass\n",
    "    sent_vec /= cnt_words\n",
    "    sent_vectors.append(sent_vec)\n",
    "print(len(sent_vectors)) # length(rows) of average word2vec model\n",
    "print(len(sent_vectors[0])) # dimensionality of table i.e. number of attributes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10000, 50)\n"
     ]
    }
   ],
   "source": [
    "# Standardizing the data with mean=0 and std.dev=1\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "standardized_data_av = StandardScaler().fit_transform(sent_vectors)\n",
    "print(standardized_data_av.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_av = np.array(standardized_data_av) # storing the values after standardization in a numpy array\n",
    "y_av = np.array(random_final['Score']) # storing the values of Scores in numpy array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split the data set into train and test, 70% train and 30% test \n",
    "X3, X_test3, y3, y_test3 = train_test_split(X_av, y_av, test_size=0.3, shuffle=False, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=TimeSeriesSplit(max_train_size=None, n_splits=10),\n",
       "       error_score='raise',\n",
       "       estimator=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
       "          penalty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
       "          verbose=0, warm_start=False),\n",
       "       fit_params=None, iid=True, n_jobs=1,\n",
       "       param_grid=[{'C': [1e-08, 1e-06, 0.0001, 0.01, 1, 100, 10000, 1000000, 100000000]}],\n",
       "       pre_dispatch='2*n_jobs', refit=True, return_train_score='warn',\n",
       "       scoring=None, verbose=0)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import cross_val_predict\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "                               \n",
    "tss_av = TimeSeriesSplit(n_splits=10) # TimeSeries Split with number of splits=10\n",
    "\n",
    "parameters_av = [{'C': [10**-8, 10**-6, 10**-4, 10**-2, 10**0, 10**2, 10**4, 10**6, 10**8]}]# specifying the range of the hyperparameter C for GridSearch\n",
    "\n",
    "Lr_av = LogisticRegression()\n",
    "\n",
    "# GridSearch for finding right hyperparameter with with 10 fold CV on TimeSeriesSplit\n",
    "model_av = GridSearchCV(Lr_av, parameters_av, cv=tss_av, refit=True)\n",
    "\n",
    "# Fit the training data\n",
    "model_av.fit(X3, y3)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'C': 1}"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_av.best_params_ # print the best hyperparameter C"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Accuracy =  0.869\n",
      "Confusion Matrix\n",
      " [[ 148  312]\n",
      " [  81 2459]]\n",
      "(tn, fn, fp, tp) = (148, 312, 81, 2459)\n",
      "Recall =  0.9681102362204724\n",
      "f1-Score =  0.9260026360384108\n",
      "Precision =  0.8874052688560087\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import recall_score, f1_score, confusion_matrix, accuracy_score, precision_score\n",
    "\n",
    "\n",
    "# instantiate learning model for best C\n",
    "Lr_av = LogisticRegression(penalty='l2', C = 1)\n",
    "\n",
    "# fitting the model\n",
    "Lr_av.fit(X3, y3)\n",
    "\n",
    "# predict the response\n",
    "pred_Lr_av = Lr_av.predict(X_test3)\n",
    "\n",
    "print()\n",
    "\n",
    "# evaluate various performance measures:\n",
    "print(\"Accuracy = \",accuracy_score(y_test3, pred_Lr_av)) # accuracy\n",
    "print(\"Confusion Matrix\\n\", confusion_matrix(y_test3, pred_Lr_av)) # Confusion Matrix\n",
    "tn, fn, fp, tp = confusion_matrix(y_test3, pred_Lr_av).ravel() # tn, fn, fp, tp\n",
    "print(\"(tn, fn, fp, tp) =\",(tn, fn, fp, tp))\n",
    "print(\"Recall = \",recall_score(y_test3, pred_Lr_av, pos_label='positive')) # Recall\n",
    "print(\"f1-Score = \"  ,f1_score(y_test3, pred_Lr_av, pos_label='positive')) # f1-Score\n",
    "print(\"Precision = \",precision_score(y_test3, pred_Lr_av, pos_label='positive'))# Precision\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.08005884,  0.08280384,  0.33503268,  0.10609063, -0.51906066,\n",
       "         0.37434723,  0.6006179 ,  0.04445225, -0.27361939,  0.65260072,\n",
       "        -0.59205295, -0.00717538, -0.39121655,  0.04496764, -0.31790092,\n",
       "         0.05801493,  0.65085597,  0.52352541,  0.21409915, -0.21894988,\n",
       "        -0.0274133 ,  0.02651862,  0.56263945, -0.43338965, -0.05337297,\n",
       "        -0.42315488, -0.11941252, -0.65448379, -0.81239727,  1.0579726 ,\n",
       "        -0.29500718, -0.4507781 , -0.2468308 , -0.51595347,  1.33254397,\n",
       "        -0.41452788, -0.03099506,  0.3503463 ,  0.03604411, -0.03547868,\n",
       "        -0.76691235, -0.06297229,  0.11322419, -0.694697  ,  0.08773148,\n",
       "         0.29493774,  0.58331405, -0.67724479,  0.42696756,  0.55977858]])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Lr_av.coef_ # weights of aver-w2v features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "a_av = np.empty(0)\n",
    "a_av = Lr_av.coef_ # storing in anumpy array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_av = a_av + np.random.normal(0, 0.02) # adding a small noise in weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.09323852,  0.09598352,  0.34821236,  0.11927032, -0.50588098,\n",
       "         0.38752691,  0.61379758,  0.05763193, -0.2604397 ,  0.66578041,\n",
       "        -0.57887326,  0.00600431, -0.37803687,  0.05814733, -0.30472124,\n",
       "         0.07119461,  0.66403565,  0.53670509,  0.22727884, -0.2057702 ,\n",
       "        -0.01423362,  0.03969831,  0.57581914, -0.42020997, -0.04019329,\n",
       "        -0.4099752 , -0.10623284, -0.64130411, -0.79921758,  1.07115229,\n",
       "        -0.2818275 , -0.43759841, -0.23365112, -0.50277379,  1.34572365,\n",
       "        -0.40134819, -0.01781538,  0.36352598,  0.0492238 , -0.022299  ,\n",
       "        -0.75373267, -0.04979261,  0.12640388, -0.68151732,  0.10091116,\n",
       "         0.30811742,  0.59649373, -0.66406511,  0.44014724,  0.57295826]])"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_av"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# As the weights don't differ significantly hence they are not collinear and we can use the weights as feature importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 10 Positive Features for Average Word2Vec\n",
      "\n",
      "[('wonderful', 0.8924843072891235), ('fantastic', 0.883131742477417), ('fabulous', 0.8691504597663879), ('awesome', 0.8667495250701904), ('truly', 0.8283336162567139), ('terrific', 0.8055059909820557), ('awful', 0.8021843433380127), ('excellent', 0.799284815788269), ('unbeatable', 0.7787255048751831), ('smooth', 0.7784639596939087)]\n",
      "\n",
      "\n",
      "Top 10 Negative Features for Average Word2Vec\n",
      "\n",
      "[('keep', 0.1298961490392685), ('went', 0.08189871907234192), ('were', 0.06614072620868683), ('days', 0.045755207538604736), ('are', 0.043285585939884186), ('am', 0.03632929548621178), ('started', 0.017165493220090866), ('couple', 0.007540944963693619), ('im', 0.006798788905143738), ('came', 0.0007816553115844727)]\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# print top 20 positive features for positive class\n",
    "print(\"Top 10 Positive Features for Average Word2Vec\")\n",
    "print()\n",
    "print(w2v_model.most_similar(positive=['amazing'], topn=10))\n",
    "print()\n",
    "print()\n",
    "\n",
    "# print top 20 positive features for negative class\n",
    "print(\"Top 10 Negative Features for Average Word2Vec\")\n",
    "print()\n",
    "print(w2v_model.most_similar(negative=['awful'], topn=10))\n",
    "print()\n",
    "print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sparsity:  47\n",
      "\n",
      "Accuracy =  0.8683333333333333\n",
      "Confusion Matrix\n",
      " [[ 146  314]\n",
      " [  81 2459]]\n",
      "(tn, fn, fp, tp) = (146, 314, 81, 2459)\n",
      "Recall =  0.9681102362204724\n",
      "f1-Score =  0.9256540560888387\n",
      "Precision =  0.8867652362062748\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import recall_score, f1_score, confusion_matrix, accuracy_score, precision_score\n",
    "\n",
    "\n",
    "# instantiate learning model for best C\n",
    "Lr_av = LogisticRegression(penalty='l1', C = 1)\n",
    "\n",
    "# fitting the model\n",
    "Lr_av.fit(X3, y3)\n",
    "\n",
    "# predict the response\n",
    "pred_Lr_av = Lr_av.predict(X_test3)\n",
    "\n",
    "# Sparsity of av-w2v features with C\n",
    "w4 = Lr_av.coef_\n",
    "print(\"Sparsity: \" ,np.count_nonzero(w4))\n",
    "\n",
    "print()\n",
    "\n",
    "# evaluate various performance measures:\n",
    "print(\"Accuracy = \",accuracy_score(y_test3, pred_Lr_av)) # accuracy\n",
    "print(\"Confusion Matrix\\n\", confusion_matrix(y_test3, pred_Lr_av)) # Confusion Matrix\n",
    "tn, fn, fp, tp = confusion_matrix(y_test3, pred_Lr_av).ravel() # tn, fn, fp, tp\n",
    "print(\"(tn, fn, fp, tp) =\",(tn, fn, fp, tp))\n",
    "print(\"Recall = \",recall_score(y_test3, pred_Lr_av, pos_label='positive')) # Recall\n",
    "print(\"f1-Score = \"  ,f1_score(y_test3, pred_Lr_av, pos_label='positive')) # f1-Score\n",
    "print(\"Precision = \",precision_score(y_test3, pred_Lr_av, pos_label='positive'))# Precision\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sparsity:  36\n",
      "\n",
      "Precision =  0.8772919605077574\n"
     ]
    }
   ],
   "source": [
    "# instantiate learning model for best alpha\n",
    "Lr_av = LogisticRegression(penalty='l1', C = 0.05)\n",
    "\n",
    "\n",
    "# fitting the model\n",
    "Lr_av.fit(X3, y3)\n",
    "\n",
    "# predict the response\n",
    "pred_Lr_av = Lr_av.predict(X_test3)\n",
    "\n",
    "# Sparsity of av-w2v features with reduced C\n",
    "w4 = Lr_av.coef_\n",
    "print(\"Sparsity: \" ,np.count_nonzero(w4))\n",
    "\n",
    "print()\n",
    "\n",
    "print(\"Precision = \",precision_score(y_test3, pred_Lr_av, pos_label='positive'))# Precision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sparsity:  13\n",
      "\n",
      "Precision =  0.8565587734241908\n"
     ]
    }
   ],
   "source": [
    "# instantiate learning model for best alpha\n",
    "Lr_av = LogisticRegression(penalty='l1', C = 0.01)\n",
    "\n",
    "\n",
    "# fitting the model\n",
    "Lr_av.fit(X3, y3)\n",
    "\n",
    "# predict the response\n",
    "pred_Lr_av = Lr_av.predict(X_test3)\n",
    "\n",
    "# Sparsity of av-w2v features with reduced C\n",
    "w4 = Lr_av.coef_\n",
    "print(\"Sparsity: \" ,np.count_nonzero(w4))\n",
    "\n",
    "print()\n",
    "\n",
    "print(\"Precision = \",precision_score(y_test3, pred_Lr_av, pos_label='positive'))# Precision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sparsity:  7\n",
      "\n",
      "Precision =  0.8490123870103783\n"
     ]
    }
   ],
   "source": [
    "# instantiate learning model for best alpha\n",
    "Lr_av = LogisticRegression(penalty='l1', C = 0.005)\n",
    "\n",
    "\n",
    "# fitting the model\n",
    "Lr_av.fit(X3, y3)\n",
    "\n",
    "# predict the response\n",
    "pred_Lr_av = Lr_av.predict(X_test3)\n",
    "\n",
    "# Sparsity of av-w2v features with reduced C\n",
    "w4 = Lr_av.coef_\n",
    "print(\"Sparsity: \" ,np.count_nonzero(w4))\n",
    "\n",
    "print()\n",
    "\n",
    "print(\"Precision = \",precision_score(y_test3, pred_Lr_av, pos_label='positive'))# Precision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sparsity:  0\n",
      "\n",
      "Precision =  0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/shebu/anaconda3/lib/python3.6/site-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "# instantiate learning model for best alpha\n",
    "Lr_av = LogisticRegression(penalty='l1', C = 0.0004)\n",
    "\n",
    "\n",
    "# fitting the model\n",
    "Lr_av.fit(X3, y3)\n",
    "\n",
    "# predict the response\n",
    "pred_Lr_av = Lr_av.predict(X_test3)\n",
    "\n",
    "# Sparsity of av-w2v features with reduced C\n",
    "w4 = Lr_av.coef_\n",
    "print(\"Sparsity: \" ,np.count_nonzero(w4))\n",
    "\n",
    "print()\n",
    "\n",
    "print(\"Precision = \",precision_score(y_test3, pred_Lr_av, pos_label='positive'))# Precision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sparsity:  50\n",
      "\n",
      "Precision =  0.8882863340563991\n"
     ]
    }
   ],
   "source": [
    "# instantiate learning model for best alpha\n",
    "Lr_av = LogisticRegression(penalty='l1', C = 10)\n",
    "\n",
    "\n",
    "# fitting the model\n",
    "Lr_av.fit(X3, y3)\n",
    "\n",
    "# predict the response\n",
    "pred_Lr_av = Lr_av.predict(X_test3)\n",
    "\n",
    "# Sparsity of av-w2v features with increased C\n",
    "w4 = Lr_av.coef_\n",
    "print(\"Sparsity: \" ,np.count_nonzero(w4))\n",
    "\n",
    "print()\n",
    "\n",
    "print(\"Precision = \",precision_score(y_test3, pred_Lr_av, pos_label='positive'))# Precision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomizedSearchCV(cv=TimeSeriesSplit(max_train_size=None, n_splits=10),\n",
       "          error_score='raise',\n",
       "          estimator=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
       "          penalty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
       "          verbose=0, warm_start=False),\n",
       "          fit_params=None, iid=True, n_iter=10, n_jobs=1,\n",
       "          param_distributions={'C': <scipy.stats._distn_infrastructure.rv_frozen object at 0x7f926454e550>},\n",
       "          pre_dispatch='2*n_jobs', random_state=None, refit=True,\n",
       "          return_train_score='warn', scoring=None, verbose=0)"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import cross_val_predict\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from scipy.stats import uniform as sp_rand\n",
    "                               \n",
    "                              \n",
    "tss_av_r = TimeSeriesSplit(n_splits=10) # TimeSeries Split with number of splits=10\n",
    "\n",
    "parameters_av_r = {'C': sp_rand()}# specifying the range of the hyperparameter C for RandomizedSearchCV\n",
    "\n",
    "Lr_av_r = LogisticRegression()\n",
    "\n",
    "# RandomizedSearchCV for finding right hyperparameter with with 10 fold CV on TimeSeriesSplit\n",
    "model_av_r = RandomizedSearchCV(Lr_av_r, parameters_av_r, cv=tss_av_r, refit=True)\n",
    "\n",
    "# Fit the training data\n",
    "model_av_r.fit(X3, y3)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'C': 0.40039032299062904}"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_av_r.best_params_ # best hyperaparameter C"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Accuracy =  0.8683333333333333\n",
      "Confusion Matrix\n",
      " [[ 145  315]\n",
      " [  80 2460]]\n",
      "(tn, fn, fp, tp) = (145, 315, 80, 2460)\n",
      "Recall =  0.968503937007874\n",
      "f1-Score =  0.9256820319849484\n",
      "Precision =  0.8864864864864865\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import recall_score, f1_score, confusion_matrix, accuracy_score, precision_score\n",
    "\n",
    "\n",
    "# instantiate learning model for best C\n",
    "Lr_av_r = LogisticRegression(penalty='l2', C = 0.40039032299062904)\n",
    "\n",
    "# fitting the model\n",
    "Lr_av_r.fit(X3, y3)\n",
    "\n",
    "# predict the response\n",
    "pred_Lr_av_r = Lr_av_r.predict(X_test3)\n",
    "\n",
    "print()\n",
    "\n",
    "# evaluate various performance measures:\n",
    "print(\"Accuracy = \",accuracy_score(y_test3, pred_Lr_av_r)) # accuracy\n",
    "print(\"Confusion Matrix\\n\", confusion_matrix(y_test3, pred_Lr_av_r)) # Confusion Matrix\n",
    "tn, fn, fp, tp = confusion_matrix(y_test3, pred_Lr_av_r).ravel() # tn, fn, fp, tp\n",
    "print(\"(tn, fn, fp, tp) =\",(tn, fn, fp, tp))\n",
    "print(\"Recall = \",recall_score(y_test3, pred_Lr_av_r, pos_label='positive')) # Recall\n",
    "print(\"f1-Score = \"  ,f1_score(y_test3, pred_Lr_av_r, pos_label='positive')) # f1-Score\n",
    "print(\"Precision = \",precision_score(y_test3, pred_Lr_av_r, pos_label='positive'))# Precision\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sparsity:  43\n",
      "\n",
      "Accuracy =  0.8676666666666667\n",
      "Confusion Matrix\n",
      " [[ 144  316]\n",
      " [  81 2459]]\n",
      "(tn, fn, fp, tp) = (144, 316, 81, 2459)\n",
      "Recall =  0.9681102362204724\n",
      "f1-Score =  0.9253057384760112\n",
      "Precision =  0.8861261261261262\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import recall_score, f1_score, confusion_matrix, accuracy_score, precision_score\n",
    "\n",
    "\n",
    "# instantiate learning model for best C\n",
    "Lr_av_r = LogisticRegression(penalty='l1', C = 0.40039032299062904)\n",
    "\n",
    "# fitting the model\n",
    "Lr_av_r.fit(X3, y3)\n",
    "\n",
    "# predict the response\n",
    "pred_Lr_av_r = Lr_av_r.predict(X_test3)\n",
    "\n",
    "# Sparsity of av-w2v features with C\n",
    "w5 = Lr_av_r.coef_\n",
    "print(\"Sparsity: \" ,np.count_nonzero(w5))\n",
    "\n",
    "print()\n",
    "\n",
    "# evaluate various performance measures:\n",
    "print(\"Accuracy = \",accuracy_score(y_test3, pred_Lr_av_r)) # accuracy\n",
    "print(\"Confusion Matrix\\n\", confusion_matrix(y_test3, pred_Lr_av_r)) # Confusion Matrix\n",
    "tn, fn, fp, tp = confusion_matrix(y_test3, pred_Lr_av_r).ravel() # tn, fn, fp, tp\n",
    "print(\"(tn, fn, fp, tp) =\",(tn, fn, fp, tp))\n",
    "print(\"Recall = \",recall_score(y_test3, pred_Lr_av_r, pos_label='positive')) # Recall\n",
    "print(\"f1-Score = \"  ,f1_score(y_test3, pred_Lr_av_r, pos_label='positive')) # f1-Score\n",
    "print(\"Precision = \",precision_score(y_test3, pred_Lr_av_r, pos_label='positive'))# Precision\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sparsity:  36\n",
      "\n",
      "Precision =  0.8772919605077574\n"
     ]
    }
   ],
   "source": [
    "# instantiate learning model for best alpha\n",
    "Lr_av_r = LogisticRegression(penalty='l1', C = 0.05)\n",
    "\n",
    "\n",
    "# fitting the model\n",
    "Lr_av_r.fit(X3, y3)\n",
    "\n",
    "# predict the response\n",
    "pred_Lr_av_r = Lr_av_r.predict(X_test3)\n",
    "\n",
    "\n",
    "# Sparsity of av-w2v features with reduced C\n",
    "w5 = Lr_av_r.coef_\n",
    "print(\"Sparsity: \" ,np.count_nonzero(w5))\n",
    "\n",
    "print()\n",
    "\n",
    "print(\"Precision = \",precision_score(y_test3, pred_Lr_av_r, pos_label='positive'))# Precision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sparsity:  13\n",
      "\n",
      "Precision =  0.8565587734241908\n"
     ]
    }
   ],
   "source": [
    "# instantiate learning model for best alpha\n",
    "Lr_av_r = LogisticRegression(penalty='l1', C = 0.01)\n",
    "\n",
    "\n",
    "# fitting the model\n",
    "Lr_av_r.fit(X3, y3)\n",
    "\n",
    "# predict the response\n",
    "pred_Lr_av_r = Lr_av_r.predict(X_test3)\n",
    "\n",
    "# Sparsity of av-w2v features with reduced C\n",
    "w5 = Lr_av_r.coef_\n",
    "print(\"Sparsity: \" ,np.count_nonzero(w5))\n",
    "\n",
    "print()\n",
    "\n",
    "print(\"Precision = \",precision_score(y_test3, pred_Lr_av_r, pos_label='positive'))# Precision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sparsity:  7\n",
      "\n",
      "Precision =  0.8468468468468469\n"
     ]
    }
   ],
   "source": [
    "# instantiate learning model for best alpha\n",
    "Lr_av_r = LogisticRegression(penalty='l1', C = 0.004)\n",
    "\n",
    "\n",
    "# fitting the model\n",
    "Lr_av_r.fit(X3, y3)\n",
    "\n",
    "# predict the response\n",
    "pred_Lr_av_r = Lr_av_r.predict(X_test3)\n",
    "\n",
    "# Sparsity of av-w2v features with reduced C\n",
    "w5 = Lr_av_r.coef_\n",
    "print(\"Sparsity: \" ,np.count_nonzero(w5))\n",
    "\n",
    "print()\n",
    "\n",
    "print(\"Precision = \",precision_score(y_test3, pred_Lr_av_r, pos_label='positive'))# Precision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sparsity:  0\n",
      "\n",
      "Precision =  0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/shebu/anaconda3/lib/python3.6/site-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "# instantiate learning model for best alpha\n",
    "Lr_av_r = LogisticRegression(penalty='l1', C = 0.0001)\n",
    "\n",
    "\n",
    "# fitting the model\n",
    "Lr_av_r.fit(X3, y3)\n",
    "\n",
    "# predict the response\n",
    "pred_Lr_av_r = Lr_av_r.predict(X_test3)\n",
    "\n",
    "# Sparsity of av-w2v features with reduced C\n",
    "w5 = Lr_av_r.coef_\n",
    "print(\"Sparsity: \" ,np.count_nonzero(w5))\n",
    "\n",
    "print()\n",
    "\n",
    "print(\"Precision = \",precision_score(y_test3, pred_Lr_av_r, pos_label='positive'))# Precision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sparsity:  48\n",
      "\n",
      "Precision =  0.8880057803468208\n"
     ]
    }
   ],
   "source": [
    "# instantiate learning model for best alpha\n",
    "Lr_av_r = LogisticRegression(penalty='l1', C = 2.5)\n",
    "\n",
    "\n",
    "# fitting the model\n",
    "Lr_av_r.fit(X3, y3)\n",
    "\n",
    "# predict the response\n",
    "pred_Lr_av_r = Lr_av_r.predict(X_test3)\n",
    "\n",
    "# Sparsity of av-w2v features with increased C\n",
    "w5 = Lr_av_r.coef_\n",
    "print(\"Sparsity: \" ,np.count_nonzero(w5))\n",
    "\n",
    "print()\n",
    "\n",
    "print(\"Precision = \",precision_score(y_test3, pred_Lr_av_r, pos_label='positive'))# Precision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TF_IDF-w2v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/shebu/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:19: RuntimeWarning: invalid value encountered in true_divide\n"
     ]
    }
   ],
   "source": [
    "# TF-IDF weighted Word2Vec\n",
    "tfidf_feat = tf_idf_vect.get_feature_names() # tfidf words/col-names\n",
    "# final_tf_idf is the sparse matrix with row= sentence, col=word and cell_val = tfidf\n",
    "\n",
    "tfidf_sent_vectors = []; # the tfidf-w2v for each sentence/review is stored in this list\n",
    "row=0;\n",
    "for sent in list_of_sent: # for each review/sentence\n",
    "    sent_vec = np.zeros(50) # as word vectors are of zero length\n",
    "    weight_sum =0; # num of words with a valid vector in the sentence/review\n",
    "    for word in sent: # for each word in a review/sentence\n",
    "        try:\n",
    "            vec = w2v_model.wv[word]\n",
    "            # obtain the tf_idfidf of a word in a sentence/review\n",
    "            tfidf = final_tf_idf[row, tfidf_feat.index(word)]\n",
    "            sent_vec += (vec * tf_idf)\n",
    "            weight_sum += tf_idf\n",
    "        except:\n",
    "            pass\n",
    "    sent_vec /= weight_sum\n",
    "    tfidf_sent_vectors.append(sent_vec)\n",
    "    row += 1\n",
    "    \n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000\n",
      "50\n"
     ]
    }
   ],
   "source": [
    "print(len(tfidf_sent_vectors)) # number of rows in tf-idf-w2v\n",
    "print(len(tfidf_sent_vectors[0])) # number of features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(tfidf_sent_vectors) # converting to dataframe "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0     True\n",
       "1     True\n",
       "2     True\n",
       "3     True\n",
       "4     True\n",
       "5     True\n",
       "6     True\n",
       "7     True\n",
       "8     True\n",
       "9     True\n",
       "10    True\n",
       "11    True\n",
       "12    True\n",
       "13    True\n",
       "14    True\n",
       "15    True\n",
       "16    True\n",
       "17    True\n",
       "18    True\n",
       "19    True\n",
       "20    True\n",
       "21    True\n",
       "22    True\n",
       "23    True\n",
       "24    True\n",
       "25    True\n",
       "26    True\n",
       "27    True\n",
       "28    True\n",
       "29    True\n",
       "30    True\n",
       "31    True\n",
       "32    True\n",
       "33    True\n",
       "34    True\n",
       "35    True\n",
       "36    True\n",
       "37    True\n",
       "38    True\n",
       "39    True\n",
       "40    True\n",
       "41    True\n",
       "42    True\n",
       "43    True\n",
       "44    True\n",
       "45    True\n",
       "46    True\n",
       "47    True\n",
       "48    True\n",
       "49    True\n",
       "dtype: bool"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.isnull().any() # check if there is any Nan entry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.fillna(0) # fill with 0 inplace of NaN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10000, 50)\n"
     ]
    }
   ],
   "source": [
    "# Standardizing the data with mean=0 and std.dev=1\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "standardized_data_av_tf = StandardScaler().fit_transform(df)\n",
    "print(standardized_data_av_tf.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_av_tf = np.array(standardized_data_av_tf) # storing the values after standardization in a numpy array\n",
    "y_av_tf = np.array(random_final['Score']) # storing the values of Scores in numpy array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split the data set into train and test, 70% train and 30% test \n",
    "X4, X_test4, y4, y_test4 = train_test_split(X_av_tf, y_av_tf, test_size=0.3, shuffle=False, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=TimeSeriesSplit(max_train_size=None, n_splits=10),\n",
       "       error_score='raise',\n",
       "       estimator=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
       "          penalty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
       "          verbose=0, warm_start=False),\n",
       "       fit_params=None, iid=True, n_jobs=1,\n",
       "       param_grid=[{'C': [1e-08, 1e-06, 0.0001, 0.01, 1, 100, 10000, 1000000, 100000000]}],\n",
       "       pre_dispatch='2*n_jobs', refit=True, return_train_score='warn',\n",
       "       scoring=None, verbose=0)"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import cross_val_predict\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "                               \n",
    "tss_av_tf = TimeSeriesSplit(n_splits=10) # TimeSeries Split with number of splits=10\n",
    "\n",
    "parameters_av_tf = [{'C': [10**-8, 10**-6, 10**-4, 10**-2, 10**0, 10**2, 10**4, 10**6, 10**8]}]# specifying the range of the hyperparameter C for GridSearch\n",
    "\n",
    "Lr_av_tf = LogisticRegression()\n",
    "\n",
    "# GridSearch for finding right hyperparameter with with 10 fold CV on TimeSeriesSplit\n",
    "model_av_tf = GridSearchCV(Lr_av_tf, parameters_av_tf, cv=tss_av_tf, refit=True)\n",
    "\n",
    "# Fit the training data\n",
    "model_av_tf.fit(X4, y4)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'C': 1e-08}"
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_av_tf.best_params_ # best hyperaparamter C"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Accuracy =  0.8466666666666667\n",
      "Confusion Matrix\n",
      " [[   0  460]\n",
      " [   0 2540]]\n",
      "(tn, fn, fp, tp) = (0, 460, 0, 2540)\n",
      "Recall =  1.0\n",
      "f1-Score =  0.9169675090252708\n",
      "Precision =  0.8466666666666667\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import recall_score, f1_score, confusion_matrix, accuracy_score, precision_score\n",
    "\n",
    "\n",
    "# instantiate learning model for best C\n",
    "Lr_av_tf = LogisticRegression(penalty='l2', C = 1e-08)\n",
    "\n",
    "# fitting the model\n",
    "Lr_av_tf.fit(X4, y4)\n",
    "\n",
    "# predict the response\n",
    "pred_Lr_av_tf = Lr_av_tf.predict(X_test4)\n",
    "\n",
    "print()\n",
    "\n",
    "# evaluate various performance measures:\n",
    "print(\"Accuracy = \",accuracy_score(y_test4, pred_Lr_av_tf)) # accuracy\n",
    "print(\"Confusion Matrix\\n\", confusion_matrix(y_test4, pred_Lr_av_tf)) # Confusion Matrix\n",
    "tn, fn, fp, tp = confusion_matrix(y_test4, pred_Lr_av_tf).ravel() # tn, fn, fp, tp\n",
    "print(\"(tn, fn, fp, tp) =\",(tn, fn, fp, tp))\n",
    "print(\"Recall = \",recall_score(y_test4, pred_Lr_av_tf, pos_label='positive')) # Recall\n",
    "print(\"f1-Score = \"  ,f1_score(y_test4, pred_Lr_av_tf, pos_label='positive')) # f1-Score\n",
    "print(\"Precision = \",precision_score(y_test4, pred_Lr_av_tf, pos_label='positive'))# Precision\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "a_av_tf = np.empty(0)\n",
    "a_av_tf = Lr_av_tf.coef_ # put the weights in a numpy array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0.]])"
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a_av_tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_av_tf = a_av_tf + np.random.normal(0, 0.02) # adding a small noise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.0094857, 0.0094857, 0.0094857, 0.0094857, 0.0094857, 0.0094857,\n",
       "        0.0094857, 0.0094857, 0.0094857, 0.0094857, 0.0094857, 0.0094857,\n",
       "        0.0094857, 0.0094857, 0.0094857, 0.0094857, 0.0094857, 0.0094857,\n",
       "        0.0094857, 0.0094857, 0.0094857, 0.0094857, 0.0094857, 0.0094857,\n",
       "        0.0094857, 0.0094857, 0.0094857, 0.0094857, 0.0094857, 0.0094857,\n",
       "        0.0094857, 0.0094857, 0.0094857, 0.0094857, 0.0094857, 0.0094857,\n",
       "        0.0094857, 0.0094857, 0.0094857, 0.0094857, 0.0094857, 0.0094857,\n",
       "        0.0094857, 0.0094857, 0.0094857, 0.0094857, 0.0094857, 0.0094857,\n",
       "        0.0094857, 0.0094857]])"
      ]
     },
     "execution_count": 195,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_av_tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [],
   "source": [
    "# As seen that they are changing i.e. thay are not collinear. So, we can take edge weights as feature importance "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 10 Positive Features for Average Word2Vec\n",
      "\n",
      "[('wonderful', 0.8924843072891235), ('fantastic', 0.883131742477417), ('fabulous', 0.8691504597663879), ('awesome', 0.8667495250701904), ('truly', 0.8283336162567139), ('terrific', 0.8055059909820557), ('awful', 0.8021843433380127), ('excellent', 0.799284815788269), ('unbeatable', 0.7787255048751831), ('smooth', 0.7784639596939087)]\n",
      "\n",
      "\n",
      "Top 10 Negative Features for Average Word2Vec\n",
      "\n",
      "[('keep', 0.1298961490392685), ('went', 0.08189871907234192), ('were', 0.06614072620868683), ('days', 0.045755207538604736), ('are', 0.043285585939884186), ('am', 0.03632929548621178), ('started', 0.017165493220090866), ('couple', 0.007540944963693619), ('im', 0.006798788905143738), ('came', 0.0007816553115844727)]\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# print top 20 positive features for positive class\n",
    "print(\"Top 10 Positive Features for Average Word2Vec\")\n",
    "print()\n",
    "print( w2v_model.most_similar(positive=['amazing'], topn=10))\n",
    "print()\n",
    "print()\n",
    "\n",
    "# print top 20 positive features for negative class\n",
    "print(\"Top 10 Negative Features for Average Word2Vec\")\n",
    "print()\n",
    "print(w2v_model.most_similar(negative=['awful'], topn=10))\n",
    "print()\n",
    "print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sparsity:  0\n",
      "\n",
      "Accuracy =  0.15333333333333332\n",
      "Confusion Matrix\n",
      " [[ 460    0]\n",
      " [2540    0]]\n",
      "(tn, fn, fp, tp) = (460, 0, 2540, 0)\n",
      "Recall =  0.0\n",
      "f1-Score =  0.0\n",
      "Precision =  0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/shebu/anaconda3/lib/python3.6/site-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/home/shebu/anaconda3/lib/python3.6/site-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import recall_score, f1_score, confusion_matrix, accuracy_score, precision_score\n",
    "\n",
    "\n",
    "# instantiate learning model for best C\n",
    "Lr_av_tf = LogisticRegression(penalty='l1', C = 1e-08)\n",
    "\n",
    "# fitting the model\n",
    "Lr_av_tf.fit(X4, y4)\n",
    "\n",
    "# predict the response\n",
    "pred_Lr_av_tf = Lr_av_tf.predict(X_test4)\n",
    "\n",
    "# Sparsity of av-w2v features with C\n",
    "w6 = Lr_av_tf.coef_\n",
    "print(\"Sparsity: \" ,np.count_nonzero(w6))\n",
    "\n",
    "print()\n",
    "\n",
    "# evaluate various performance measures:\n",
    "print(\"Accuracy = \",accuracy_score(y_test4, pred_Lr_av_tf)) # accuracy\n",
    "print(\"Confusion Matrix\\n\", confusion_matrix(y_test4, pred_Lr_av_tf)) # Confusion Matrix\n",
    "tn, fn, fp, tp = confusion_matrix(y_test4, pred_Lr_av_tf).ravel() # tn, fn, fp, tp\n",
    "print(\"(tn, fn, fp, tp) =\",(tn, fn, fp, tp))\n",
    "print(\"Recall = \",recall_score(y_test4, pred_Lr_av_tf, pos_label='positive')) # Recall\n",
    "print(\"f1-Score = \"  ,f1_score(y_test4, pred_Lr_av_tf, pos_label='positive')) # f1-Score\n",
    "print(\"Precision = \",precision_score(y_test4, pred_Lr_av_tf, pos_label='positive'))# Precision\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sparsity:  0\n",
      "\n",
      "Precision =  0.8466666666666667\n"
     ]
    }
   ],
   "source": [
    "# instantiate learning model for best alpha\n",
    "Lr_av_tf = LogisticRegression(penalty='l1', C = 1e-01)\n",
    "\n",
    "\n",
    "# fitting the model\n",
    "Lr_av_tf.fit(X4, y4)\n",
    "\n",
    "# predict the response\n",
    "pred_Lr_av_tf = Lr_av_tf.predict(X_test3)\n",
    "\n",
    "# Sparsity of av-w2v features with reduced C\n",
    "w6 = Lr_av_tf.coef_\n",
    "print(\"Sparsity: \" ,np.count_nonzero(w6))\n",
    "\n",
    "print()\n",
    "\n",
    "print(\"Precision = \",precision_score(y_test4, pred_Lr_av_tf, pos_label='positive'))# Precision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomizedSearchCV(cv=TimeSeriesSplit(max_train_size=None, n_splits=10),\n",
       "          error_score='raise',\n",
       "          estimator=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
       "          penalty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
       "          verbose=0, warm_start=False),\n",
       "          fit_params=None, iid=True, n_iter=10, n_jobs=1,\n",
       "          param_distributions={'C': <scipy.stats._distn_infrastructure.rv_frozen object at 0x7f9257e3bb38>},\n",
       "          pre_dispatch='2*n_jobs', random_state=None, refit=True,\n",
       "          return_train_score='warn', scoring=None, verbose=0)"
      ]
     },
     "execution_count": 169,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import cross_val_predict\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from scipy.stats import uniform as sp_rand\n",
    "                               \n",
    "                              \n",
    "tss_av_tf_r = TimeSeriesSplit(n_splits=10) # TimeSeries Split with number of splits=10\n",
    "\n",
    "parameters_av_tf_r = {'C': sp_rand()}# specifying the range of the hyperparameter C for RandomizedSearchCV\n",
    "\n",
    "Lr_av_tf_r = LogisticRegression()\n",
    "\n",
    "# RandomizedSearchCV for finding right hyperparameter with with 10 fold CV on TimeSeriesSplit\n",
    "model_av_tf_r = RandomizedSearchCV(Lr_av_tf_r, parameters_av_tf_r, cv=tss_av_tf_r, refit=True)\n",
    "\n",
    "# Fit the training data\n",
    "model_av_tf_r.fit(X4, y4)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'C': 0.41034631322795734}"
      ]
     },
     "execution_count": 170,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_av_tf_r.best_params_ # best hyperparameter C"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Accuracy =  0.8466666666666667\n",
      "Confusion Matrix\n",
      " [[   0  460]\n",
      " [   0 2540]]\n",
      "(tn, fn, fp, tp) = (0, 460, 0, 2540)\n",
      "Recall =  1.0\n",
      "f1-Score =  0.9169675090252708\n",
      "Precision =  0.8466666666666667\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import recall_score, f1_score, confusion_matrix, accuracy_score, precision_score\n",
    "\n",
    "\n",
    "# instantiate learning model for best C\n",
    "Lr_av_tf_r = LogisticRegression(penalty='l2', C = 0.41034631322795734)\n",
    "\n",
    "# fitting the model\n",
    "Lr_av_tf_r.fit(X4, y4)\n",
    "\n",
    "# predict the response\n",
    "pred_Lr_av_tf_r = Lr_av_tf_r.predict(X_test4)\n",
    "\n",
    "print()\n",
    "\n",
    "# evaluate various performance measures:\n",
    "print(\"Accuracy = \",accuracy_score(y_test4, pred_Lr_av_tf_r)) # accuracy\n",
    "print(\"Confusion Matrix\\n\", confusion_matrix(y_test4, pred_Lr_av_tf_r)) # Confusion Matrix\n",
    "tn, fn, fp, tp = confusion_matrix(y_test4, pred_Lr_av_tf_r).ravel() # tn, fn, fp, tp\n",
    "print(\"(tn, fn, fp, tp) =\",(tn, fn, fp, tp))\n",
    "print(\"Recall = \",recall_score(y_test4, pred_Lr_av_tf_r, pos_label='positive')) # Recall\n",
    "print(\"f1-Score = \"  ,f1_score(y_test4, pred_Lr_av_tf_r, pos_label='positive')) # f1-Score\n",
    "print(\"Precision = \",precision_score(y_test4, pred_Lr_av_tf_r, pos_label='positive'))# Precision\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sparsity:  0\n",
      "\n",
      "Accuracy =  0.8466666666666667\n",
      "Confusion Matrix\n",
      " [[   0  460]\n",
      " [   0 2540]]\n",
      "(tn, fn, fp, tp) = (0, 460, 0, 2540)\n",
      "Recall =  1.0\n",
      "f1-Score =  0.9169675090252708\n",
      "Precision =  0.8466666666666667\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import recall_score, f1_score, confusion_matrix, accuracy_score, precision_score\n",
    "\n",
    "\n",
    "# instantiate learning model for best alpha\n",
    "Lr_av_tf_r = LogisticRegression(penalty='l1', C = 0.41034631322795734)\n",
    "\n",
    "# fitting the model\n",
    "Lr_av_tf_r.fit(X4, y4)\n",
    "\n",
    "# predict the response\n",
    "pred_Lr_av_tf_r = Lr_av_tf_r.predict(X_test4)\n",
    "\n",
    "# Sparsity of av-w2v features with C\n",
    "w7 = Lr_av_tf.coef_\n",
    "print(\"Sparsity: \" ,np.count_nonzero(w7))\n",
    "\n",
    "print()\n",
    "\n",
    "# evaluate various performance measures:\n",
    "print(\"Accuracy = \",accuracy_score(y_test4, pred_Lr_av_tf_r)) # accuracy\n",
    "print(\"Confusion Matrix\\n\", confusion_matrix(y_test4, pred_Lr_av_tf_r)) # Confusion Matrix\n",
    "tn, fn, fp, tp = confusion_matrix(y_test4, pred_Lr_av_tf_r).ravel() # tn, fn, fp, tp\n",
    "print(\"(tn, fn, fp, tp) =\",(tn, fn, fp, tp))\n",
    "print(\"Recall = \",recall_score(y_test4, pred_Lr_av_tf_r, pos_label='positive')) # Recall\n",
    "print(\"f1-Score = \"  ,f1_score(y_test4, pred_Lr_av_tf_r, pos_label='positive')) # f1-Score\n",
    "print(\"Precision = \",precision_score(y_test4, pred_Lr_av_tf_r, pos_label='positive'))# Precision\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sparsity:  0\n",
      "\n",
      "Precision =  0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/shebu/anaconda3/lib/python3.6/site-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "# instantiate learning model for best alpha\n",
    "Lr_av_tf_r = LogisticRegression(penalty='l1', C = 0.01)\n",
    "\n",
    "\n",
    "# fitting the model\n",
    "Lr_av_tf_r.fit(X4, y4)\n",
    "\n",
    "# predict the response\n",
    "pred_Lr_av_tf_r = Lr_av_tf.predict(X_test4)\n",
    "\n",
    "# Sparsity of av-w2v features with reduced C\n",
    "w7 = Lr_av_tf_r.coef_\n",
    "print(\"Sparsity: \" ,np.count_nonzero(w7))\n",
    "\n",
    "print()\n",
    "\n",
    "print(\"Precision = \",precision_score(y_test4, pred_Lr_av_tf_r, pos_label='positive'))# Precision"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
